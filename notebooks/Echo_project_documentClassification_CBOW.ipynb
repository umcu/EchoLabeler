{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import regex\n",
    "import clinlp\n",
    "import medcat as mc\n",
    "import gensim as gs\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, multilabel_confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import echo_models, echo_utils, deabber\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aortic_regurgitation.jsonl',\n",
       " 'aortic_stenosis.jsonl',\n",
       " 'diastolic_dysfunction.jsonl',\n",
       " 'lv_dil.jsonl',\n",
       " 'lv_syst_func.jsonl',\n",
       " 'merged_labels.jsonl',\n",
       " 'mitral_regurgitation.jsonl',\n",
       " 'old',\n",
       " 'pe.jsonl',\n",
       " 'rv_dil.jsonl',\n",
       " 'rv_syst_func.jsonl',\n",
       " 'tricuspid_regurgitation.jsonl',\n",
       " 'wma.jsonl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./echo_doc_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = ['de', 'het', 'een', 'is', 'bij', 'van', 'met', 'en', 'in', 'voor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes = ['lv_dil', 'pe', 'rv_dil', 'aortic_regurgitation', \n",
    "           'lv_syst_func', 'rv_syst_func', 'aortic_stenosis', \n",
    "           'diastolic_dysfunction', 'mitral_regurgitation',\n",
    "           'tricuspid_regurgitation', 'wma']\n",
    "\n",
    "Class = 'aortic_regurgitation'\n",
    "lemmatize = True\n",
    "lowercase = False\n",
    "deabbreviate = False\n",
    "filter_reports = True\n",
    "reduce_labels = True\n",
    "num_topics = 5\n",
    "num_words_in_vocab = 5_000\n",
    "weight_matrix = 'count' # count, tfidf, etm\n",
    "\n",
    "model_TDIDF = True\n",
    "model_ETM = False\n",
    "model_SALT = False\n",
    "SALT_METHOD = 'kmean'\n",
    "model_embeddings_CUI = False\n",
    "model_embeddings_W2V = False\n",
    "model_embeddings_SBERT = False\n",
    "EMBEDDING_AGGREGATOR = 'mean'\n",
    "model_CLF = 'svm'\n",
    "model_normaliser = 'standardize'\n",
    "\n",
    "\n",
    "FLAG_TERMS = ['uitslag zie medische status', 'zie status', 'zie verslag status', 'slecht echovenster', 'echo overwegen', 'ge echo',\n",
    "              'geen echovenster', 'geen beoordeelbaar echo', 'tee 190', 'hdf 36mnd', 'geen beoordeelbare echo', 'verslag op ic']\n",
    "SAVE_TERMS = ['goed', 'geen', 'normaal', 'normale']\n",
    "\n",
    "reduce_labels = True\n",
    "REDUCED_LABELMAP = {\n",
    "    'Present': 'Present',\n",
    "    'No label': 'No label',\n",
    "    'Normal': 'Normal',\n",
    "    'Moderate': 'Present',\n",
    "    'Severe': 'Present',\n",
    "    'Mild': 'Present'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deabbreviate:\n",
    "    ABBREVIATIONS = benedict.benedict(\"../assets/abbreviations.yml\")\n",
    "    \n",
    "if lemmatize:\n",
    "    nlp = spacy.load(\"nl_core_news_lg\", disable = ['parser','ner'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary with labeled_documents\n",
    "labeled_documents = pd.read_json(f\"./echo_doc_labels/{Class}.jsonl\", lines=True)\n",
    "label_col = 'label' if Class!='merged_labels' else 'labels'\n",
    "\n",
    "train_ids = pd.read_csv('./train_echoid.csv', sep=',').input_hash.unique()\n",
    "test_ids = pd.read_csv('./test_echoid.csv', sep=',').input_hash.unique()\n",
    "\n",
    "labeled_documents['_hash'] = labeled_documents.text.str.strip().apply(lambda x: hash(x))\n",
    "labeled_documents = labeled_documents.drop_duplicates(subset=['_hash']).reset_index(drop=True)\n",
    "\n",
    "if reduce_labels:\n",
    "    labeled_documents['label'] = labeled_documents['label'].map(REDUCED_LABELMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordLists = labeled_documents.text.str.split(\" \").tolist()\n",
    "WordCount = defaultdict(int)\n",
    "for d in WordLists:\n",
    "    for t in d:\n",
    "        WordCount[t.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('geen', 4886),\n",
       " ('goede', 4465),\n",
       " ('en', 3754),\n",
       " ('normale', 3703),\n",
       " ('met', 3170),\n",
       " ('lv', 2548),\n",
       " ('functie.', 2441),\n",
       " ('globaal', 2131),\n",
       " ('systolische', 1905),\n",
       " ('niet', 1832)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(WordCount.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand with label columns\n",
    "if Class == 'merged_labels':\n",
    "    target_df = pd.DataFrame.from_records(labeled_documents[label_col])\n",
    "    Target_maps = {\n",
    "        _Class: {Label:i for i,Label in enumerate(target_df[Class].unique())}\n",
    "        for _Class in target_df.columns\n",
    "    }\n",
    "else:\n",
    "    Target_maps = {\n",
    "        Class: {Label: i for i,Label in enumerate(labeled_documents['label'].unique())} \n",
    "    }\n",
    "    \n",
    "if Class == 'merged_labels':\n",
    "    DF = labeled_documents[['text', '_input_hash']].join(target_df[Class])\n",
    "else:\n",
    "    DF = labeled_documents[['text', '_input_hash', 'label']]\n",
    "\n",
    "DF.columns = ['sentence', '_input_hash', 'labels']\n",
    "\n",
    "label2id = Target_maps[Class]\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "DF= DF.assign(label=DF['labels'].map(label2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = DF.assign(sentence=DF.sentence.str.replace(r'[\\r\\n]', '', regex=True)) \n",
    "\n",
    "if lemmatize:\n",
    "    print(\"Lemmatizing...\")\n",
    "    docs = nlp.pipe(DF.sentence.values)\n",
    "    new_texts = [\" \".join([token.lemma_ for token in doc]) for doc in docs] \n",
    "    DF = DF.assign(sentence = new_texts)\n",
    "\n",
    "if lowercase:\n",
    "    print(\"Lowercasing...\")\n",
    "    DF = DF.assign(sentence = DF.sentence.str.lower())\n",
    "    \n",
    "if filter_reports:\n",
    "    print(\"Filtering...\")\n",
    "    DF = DF.assign(sentence = echo_utils.report_filter(DF.sentence, \n",
    "                                            flag_terms=FLAG_TERMS, \n",
    "                                            save_terms=SAVE_TERMS)[0])\n",
    "    DF = DF.loc[DF.sentence.notna()]\n",
    "\n",
    "if deabbreviate:\n",
    "    print(\"Deabbreviate...\")\n",
    "    DeAbber = deabber.deabber(model_type='sbert', abbreviations=ABBREVIATIONS['nl']['echocardiogram'], min_sim=0.5, top_k=10)\n",
    "    DF = DF.assign(sentence=DeAbber.deabb(DF.sentence.values, TokenRadius=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified cross-validation\n",
    "\n",
    "def fold_indices(targets: pd.Series=None, stratified: bool=True, seed: int=42, numfolds: int=10)->Tuple[List,List]:\n",
    "    if stratified:\n",
    "        splitter = StratifiedKFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = targets\n",
    "    else:\n",
    "        splitter = KFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = None\n",
    "\n",
    "    train_indcs, test_indcs = [], []\n",
    "    for train_index, test_index in splitter.split(X=targets, y=_Targets):\n",
    "        train_indcs.append(train_index)\n",
    "        test_indcs.append(test_index)\n",
    "\n",
    "    return zip(train_indcs, test_indcs)\n",
    "\n",
    "def make_folds(targets: pd.Series=None, \n",
    "               train_test: tuple=None, \n",
    "               n_folds: int=10, \n",
    "               stratified: bool=True,\n",
    "               splitting: str='CV',\n",
    "               label_col: str='labels',\n",
    "               text_col: str='sentence'):\n",
    "\n",
    "    TTDict = defaultdict(dict)\n",
    "    if splitting == 'CV':\n",
    "        for k,(train_index, test_index) in enumerate(fold_indices(targets=targets[label_col], \n",
    "                                                                  stratified=stratified,\n",
    "                                                                  numfolds=n_folds)):\n",
    "            TTDict[k]['Xtrain'] = targets.iloc[train_index][text_col]\n",
    "            TTDict[k]['Xtest'] = targets.iloc[test_index][text_col]\n",
    "            \n",
    "            TTDict[k]['ytrain'] = targets.iloc[train_index][label_col]\n",
    "            TTDict[k]['ytest'] = targets.iloc[test_index][label_col]\n",
    "    else:\n",
    "        train_ids, test_ids = train_test\n",
    "        TTDict[0]['Xtrain'] = targets.loc[targets._input_hash.isin(train_ids)][text_col]\n",
    "        TTDict[0]['Xtest'] = targets.loc[targets._input_hash.isin(test_ids)][text_col]\n",
    "        \n",
    "        TTDict[0]['ytrain'] = targets.loc[targets._input_hash.isin(train_ids)][label_col]\n",
    "        TTDict[0]['ytest'] = targets.loc[targets._input_hash.isin(test_ids)][label_col]\n",
    "    \n",
    "    return TTDict\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(probs, labels, threshold=0.5):\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    # finally, compute metrics\n",
    "    \n",
    "    y_true = labels\n",
    "    #y_true = tf.keras.backend.eval(y_true)\n",
    "    #y_pred = tf.keras.backend.eval(y_pred)\n",
    "    \n",
    "    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    prec_macro = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    prec_weighted = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    prec_micro = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    recall_macro = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_micro = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    try:\n",
    "        roc_auc_weighted = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "        roc_auc_macro = roc_auc_score(y_true, probs, average = 'macro')\n",
    "        roc_auc_micro = roc_auc_score(y_true, probs, average = 'micro')\n",
    "    except ValueError:\n",
    "        roc_auc_weighted = None\n",
    "        roc_auc_macro = None\n",
    "        roc_auc_micro = None\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_macro': f1_macro,\n",
    "               'f1_weighted': f1_weighted,\n",
    "               'prec_macro': prec_macro,\n",
    "               'prec_weighted': prec_weighted,\n",
    "               'recall_macro': recall_macro,\n",
    "               'recall_weighted': recall_weighted,\n",
    "               'roc_auc_macro': roc_auc_macro,\n",
    "               'roc_auc_weighted': roc_auc_weighted,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTestDict = make_folds(DF, \n",
    "                           (train_ids, test_ids), \n",
    "                           n_folds=10, \n",
    "                           stratified=True, \n",
    "                           splitting='from_file',\n",
    "                           label_col='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topic models using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETM(tfidf_matrix, lda_theta, lda_beta):\n",
    "    # beta_{k,j} : probability of word j, given topic k\n",
    "    # theta_{i,k}: probability of topic k, given document i\n",
    "    # tfidf_matrix\n",
    "    \n",
    "    # Get Gamma_i for all documents\n",
    "    num_topics = lda_theta.shape[1]\n",
    "    AverageSentenceLength = np.mean(np.sum(tfidf_matrix>0, axis=1))\n",
    "    gammas=AverageSentenceLength/np.array(np.sum(tfidf_matrix>0, axis=1))[:,0]\n",
    "    # replace inf-gammas with 0\n",
    "    gammas[np.where(np.isinf(gammas))]=0\n",
    "    \n",
    "    # prep omega\n",
    "    indices = tfidf_matrix.indices\n",
    "    indptr = tfidf_matrix.indptr\n",
    "    z = []\n",
    "    \n",
    "    for d,row in enumerate(tfidf_matrix):\n",
    "        idcs = row.indices\n",
    "        for w in idcs:\n",
    "            omega = 0\n",
    "            for k in range(num_topics):\n",
    "                omega = omega + gammas[d]*lda_theta[d,k]*lda_beta[k, w]\n",
    "            z.append(omega)\n",
    "    omega_sparse = sparse.csr_matrix((z, indices, indptr), shape=tfidf_matrix.shape)\n",
    "    return tfidf_matrix.multiply(omega_sparse)\n",
    "    \n",
    "def SALT(tfidf_matrix, cluster_object=None):\n",
    "    # https://github.com/bagheria/saltclass/blob/master/saltclass/saltclass.py\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do for each TrainTestDict[k]['Xtrain]\n",
    "\n",
    "xtrain = TrainTestDict[0]['Xtrain']\n",
    "xtest = TrainTestDict[0]['Xtest']\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=num_topics)\n",
    "TVEC =  CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=num_words_in_vocab, stop_words=STOPWORDS\n",
    ")\n",
    "TFVEC = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=num_words_in_vocab, stop_words=STOPWORDS\n",
    ")\n",
    "TVEC.fit(xtrain)\n",
    "train_tf = TVEC.transform(xtrain)\n",
    "test_tf = TVEC.transform(xtest)\n",
    "LDA.fit(train_tf)\n",
    "lda_theta = LDA.transform(train_tf)\n",
    "lda_theta_test = LDA.transform(test_tf)\n",
    "\n",
    "lda_beta = LDA.components_ / LDA.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "TFVEC.fit(xtrain)\n",
    "train_tfidf = TFVEC.transform(xtrain)\n",
    "test_tfidf = TFVEC.transform(xtest)\n",
    "\n",
    "train_weight_matrix = ETM(train_tfidf, lda_theta, lda_beta)\n",
    "test_weight_matrix = ETM(test_tfidf, lda_theta_test, lda_beta)\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_macro': 0.7461156477320502,\n",
      " 'f1_weighted': 0.7950138809283359,\n",
      " 'prec_macro': 0.781427910359399,\n",
      " 'prec_weighted': 0.7999731899968268,\n",
      " 'recall_macro': 0.7236181839488484,\n",
      " 'recall_weighted': 0.8019981834695731,\n",
      " 'roc_auc_macro': None,\n",
      " 'roc_auc_weighted': None,\n",
      " 'accuracy': 0.8019981834695731}\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(probability=True)\n",
    "clf.fit(train_weight_matrix, TrainTestDict[0]['ytrain'])\n",
    "\n",
    "preds = clf.predict_proba(test_weight_matrix)\n",
    "ytest = TrainTestDict[0]['ytest']\n",
    "\n",
    "pprint.pp(multi_label_metrics(preds, ytest, threshold=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_macro': 0.8872860396372437,\n",
      " 'f1_weighted': 0.9182822657103805,\n",
      " 'prec_macro': 0.8930873183677671,\n",
      " 'prec_weighted': 0.9190109349459678,\n",
      " 'recall_macro': 0.8830569393634798,\n",
      " 'recall_weighted': 0.9191643960036331,\n",
      " 'roc_auc_macro': None,\n",
      " 'roc_auc_weighted': None,\n",
      " 'accuracy': 0.9191643960036331}\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(probability=True)\n",
    "clf.fit(train_weight_matrix, TrainTestDict[0]['ytrain'])\n",
    "\n",
    "preds = clf.predict_proba(test_weight_matrix)\n",
    "ytest = TrainTestDict[0]['ytest']\n",
    "\n",
    "pprint.pp(multi_label_metrics(preds, ytest, threshold=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_macro': 0.8875059593849306,\n",
      " 'f1_weighted': 0.9200610193347314,\n",
      " 'prec_macro': 0.8946266306253579,\n",
      " 'prec_weighted': 0.9201617897118376,\n",
      " 'recall_macro': 0.8813470294181535,\n",
      " 'recall_weighted': 0.9209809264305178,\n",
      " 'roc_auc_macro': None,\n",
      " 'roc_auc_weighted': None,\n",
      " 'accuracy': 0.9209809264305178}\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(probability=True)\n",
    "clf.fit(train_tfidf, TrainTestDict[0]['ytrain'])\n",
    "\n",
    "preds = clf.predict_proba(test_tfidf)\n",
    "ytest = TrainTestDict[0]['ytest']\n",
    "\n",
    "pprint.pp(multi_label_metrics(preds, ytest, threshold=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Cui-embeddings to UMLS-entities with RotatE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'cui': emb}\n",
    "Emb_path = r'T:\\\\laupodteam\\AIOS\\Bram\\data\\Supporting_Data\\External_embeddings'\n",
    "rotate_path = os.path.join(Emb_path, 'GraphEmbeddings', 'RotatE_entity2id.wv')\n",
    "cui2vec_rotate = KeyedVectors.load_word2vec_format(rotate_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui2vec_rotate['C0411888']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Word2vec from pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'token': embedding}\n",
    "# load clinical NLP embeddings, hear we are limited to fourgrams\n",
    "StaticEmbedding = KeyedVectors.load('path_to_embeddings.wv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add SBERT-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jegormeister/robbert-v2-dutch-base-mqa-finetuned, \n",
    "# textgain/allnli-GroNLP-bert-base-dutch-cased,\n",
    "# NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\n",
    "SentenceModel = SentenceTransformer('FremyCompany/BioLORD-2023-M')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate per document using LDA, UMLS, Word2Vec, SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
