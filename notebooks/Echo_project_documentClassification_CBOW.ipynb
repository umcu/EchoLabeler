{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import regex\n",
    "import clinlp\n",
    "#import medcat as mc\n",
    "import gensim as gs\n",
    "import spacy\n",
    "import numpy as np\n",
    "import dotenv\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, multilabel_confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
    "\n",
    "import gc\n",
    "#from numba import jit\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import echo_models, echo_utils, deabber\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pprint\n",
    "import benedict\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "EmbeddingPath = os.environ['WORD_EMBEDDINGS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = ['de', 'het', 'een', 'is', 'bij', 'van', 'met', 'en', 'in', 'voor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes = ['lv_dil', 'pe', 'rv_dil', 'aortic_regurgitation', \n",
    "           'lv_syst_func', 'rv_syst_func', 'aortic_stenosis', \n",
    "           'diastolic_dysfunction', 'mitral_regurgitation',\n",
    "           'tricuspid_regurgitation', 'wma']\n",
    "\n",
    "Class = 'aortic_regurgitation'\n",
    "lemmatize = True\n",
    "lowercase = False\n",
    "deabbreviate = False\n",
    "filter_reports = True\n",
    "remove_interpunction = True\n",
    "reduce_labels = True\n",
    "num_topics = 20 # 10, 20\n",
    "num_words_in_vocab = 5_000\n",
    "\n",
    "model_TDIDF = True\n",
    "model_ETM = True\n",
    "model_LDA = True\n",
    "model_embeddings= False\n",
    "EMBEDDING_AGGREGATOR = 'mean'\n",
    "\n",
    "FLAG_TERMS = ['uitslag zie medische status', 'zie status', 'zie verslag status', 'slecht echovenster', 'echo overwegen', 'ge echo',\n",
    "              'geen echovenster', 'geen beoordeelbaar echo', 'tee 190', 'hdf 36mnd', 'geen beoordeelbare echo', 'verslag op ic']\n",
    "SAVE_TERMS = ['goed', 'geen', 'normaal', 'normale']\n",
    "\n",
    "REDUCED_LABELMAP = {\n",
    "    'Present': 'Present',\n",
    "    'No label': 'No label',\n",
    "    'Normal': 'Normal',\n",
    "    'Moderate': 'Present',\n",
    "    'Severe': 'Present',\n",
    "    'Mild': 'Present'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deabbreviate:\n",
    "    ABBREVIATIONS = benedict.benedict(\"../assets/abbreviations.yml\")\n",
    "    \n",
    "if lemmatize:\n",
    "    nlp = spacy.load(\"nl_core_news_lg\", disable = ['parser','ner'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aortic_regurgitation.jsonl',\n",
       " 'aortic_stenosis.jsonl',\n",
       " 'diastolic_dysfunction.jsonl',\n",
       " 'lv_dil.jsonl',\n",
       " 'lv_syst_func.jsonl',\n",
       " 'merged_labels.jsonl',\n",
       " 'mitral_regurgitation.jsonl',\n",
       " 'old',\n",
       " 'pe.jsonl',\n",
       " 'rv_dil.jsonl',\n",
       " 'rv_syst_func.jsonl',\n",
       " 'tricuspid_regurgitation.jsonl',\n",
       " 'wma.jsonl']"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData')\n",
    "\n",
    "os.listdir(\"./echo_doc_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary with labeled_documents\n",
    "labeled_documents = pd.read_json(f\"./echo_doc_labels/{Class}.jsonl\", lines=True)\n",
    "label_col = 'label' if Class!='merged_labels' else 'labels'\n",
    "\n",
    "train_ids = pd.read_csv('./train_echoid.csv', sep=',').input_hash.unique()\n",
    "test_ids = pd.read_csv('./test_echoid.csv', sep=',').input_hash.unique()\n",
    "\n",
    "labeled_documents['_hash'] = labeled_documents.text.str.strip().apply(lambda x: hash(x))\n",
    "labeled_documents = labeled_documents.drop_duplicates(subset=['_hash']).reset_index(drop=True)\n",
    "\n",
    "if reduce_labels:\n",
    "    labeled_documents['label'] = labeled_documents['label'].map(REDUCED_LABELMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordLists = labeled_documents.text.str.split(\" \").tolist()\n",
    "WordCount = defaultdict(int)\n",
    "for d in WordLists:\n",
    "    for t in d:\n",
    "        WordCount[t.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('geen', 4886),\n",
       " ('goede', 4465),\n",
       " ('en', 3754),\n",
       " ('normale', 3703),\n",
       " ('met', 3170),\n",
       " ('lv', 2548),\n",
       " ('functie.', 2441),\n",
       " ('globaal', 2131),\n",
       " ('systolische', 1905),\n",
       " ('niet', 1832)]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(WordCount.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand with label columns\n",
    "if Class == 'merged_labels':\n",
    "    target_df = pd.DataFrame.from_records(labeled_documents[label_col])\n",
    "    Target_maps = {\n",
    "        _Class: {Label:i for i,Label in enumerate(target_df[Class].unique())}\n",
    "        for _Class in target_df.columns\n",
    "    }\n",
    "else:\n",
    "    Target_maps = {\n",
    "        Class: {Label: i for i,Label in enumerate(labeled_documents['label'].unique())} \n",
    "    }\n",
    "    \n",
    "if Class == 'merged_labels':\n",
    "    DF = labeled_documents[['text', '_input_hash']].join(target_df[Class])\n",
    "else:\n",
    "    DF = labeled_documents[['text', '_input_hash', 'label']]\n",
    "\n",
    "DF.columns = ['sentence', '_input_hash', 'labels']\n",
    "\n",
    "label2id = Target_maps[Class]\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "DF= DF.assign(label=DF['labels'].map(label2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing...\n",
      "Removing interpunction\n",
      "Filtering...\n"
     ]
    }
   ],
   "source": [
    "DF = DF.assign(sentence=DF.sentence.str.replace(r'[\\r\\n]', '', regex=True)) \n",
    "\n",
    "if lemmatize:\n",
    "    print(\"Lemmatizing...\")\n",
    "    docs = nlp.pipe(DF.sentence.values)\n",
    "    new_texts = [\" \".join([token.lemma_ for token in doc]) for doc in docs] \n",
    "    DF = DF.assign(sentence = new_texts)\n",
    "\n",
    "if lowercase:\n",
    "    print(\"Lowercasing...\")\n",
    "    DF = DF.assign(sentence = DF.sentence.str.lower())\n",
    "\n",
    "if remove_interpunction:\n",
    "    print(\"Removing interpunction\")\n",
    "    DF = DF.assign(sentence = DF.sentence.str.replace(r'([A-Z])[\\.]([A-Z])', '\\\\1\\\\2', regex=True))\n",
    "    \n",
    "if filter_reports:\n",
    "    print(\"Filtering...\")\n",
    "    DF = DF.assign(sentence = echo_utils.report_filter(DF.sentence, \n",
    "                                            flag_terms=FLAG_TERMS, \n",
    "                                            save_terms=SAVE_TERMS)[0])\n",
    "    DF = DF.loc[DF.sentence.notna()]\n",
    "\n",
    "if deabbreviate:\n",
    "    print(\"Deabbreviate...\")\n",
    "    DeAbber = deabber.deabber(model_type='sbert', abbreviations=ABBREVIATIONS['nl']['echocardiogram'], min_sim=0.5, top_k=10)\n",
    "    DF = DF.assign(sentence=DeAbber.deabb(DF.sentence.values, TokenRadius=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified cross-validation\n",
    "\n",
    "def fold_indices(targets: pd.Series=None, stratified: bool=True, seed: int=42, numfolds: int=10)->Tuple[List,List]:\n",
    "    if stratified:\n",
    "        splitter = StratifiedKFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = targets\n",
    "    else:\n",
    "        splitter = KFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = None\n",
    "\n",
    "    train_indcs, test_indcs = [], []\n",
    "    for train_index, test_index in splitter.split(X=targets, y=_Targets):\n",
    "        train_indcs.append(train_index)\n",
    "        test_indcs.append(test_index)\n",
    "\n",
    "    return zip(train_indcs, test_indcs)\n",
    "\n",
    "def make_folds(targets: pd.Series=None, \n",
    "               train_test: tuple=None, \n",
    "               n_folds: int=10, \n",
    "               stratified: bool=True,\n",
    "               splitting: str='CV',\n",
    "               label_col: str='labels',\n",
    "               text_col: str='sentence'):\n",
    "\n",
    "    TTDict = defaultdict(dict)\n",
    "    if splitting == 'CV':\n",
    "        for k,(train_index, test_index) in enumerate(fold_indices(targets=targets[label_col], \n",
    "                                                                  stratified=stratified,\n",
    "                                                                  numfolds=n_folds)):\n",
    "            TTDict[k]['Xtrain'] = targets.iloc[train_index][text_col]\n",
    "            TTDict[k]['Xtest'] = targets.iloc[test_index][text_col]\n",
    "            \n",
    "            TTDict[k]['ytrain'] = targets.iloc[train_index][label_col]\n",
    "            TTDict[k]['ytest'] = targets.iloc[test_index][label_col]\n",
    "    else:\n",
    "        train_ids, test_ids = train_test\n",
    "        TTDict[0]['Xtrain'] = targets.loc[targets._input_hash.isin(train_ids)][text_col]\n",
    "        TTDict[0]['Xtest'] = targets.loc[targets._input_hash.isin(test_ids)][text_col]\n",
    "        \n",
    "        TTDict[0]['ytrain'] = targets.loc[targets._input_hash.isin(train_ids)][label_col]\n",
    "        TTDict[0]['ytest'] = targets.loc[targets._input_hash.isin(test_ids)][label_col]\n",
    "    \n",
    "    return TTDict\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(probs, labels, threshold=0.5):\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    # finally, compute metrics\n",
    "    \n",
    "    y_true = labels\n",
    "    #y_true = tf.keras.backend.eval(y_true)\n",
    "    #y_pred = tf.keras.backend.eval(y_pred)\n",
    "    \n",
    "    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    prec_macro = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    prec_weighted = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    prec_micro = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    recall_macro = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_micro = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    try:\n",
    "        roc_auc_weighted = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "        roc_auc_macro = roc_auc_score(y_true, probs, average = 'macro')\n",
    "        roc_auc_micro = roc_auc_score(y_true, probs, average = 'micro')\n",
    "    except ValueError:\n",
    "        roc_auc_weighted = None\n",
    "        roc_auc_macro = None\n",
    "        roc_auc_micro = None\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_macro': f1_macro,\n",
    "               'f1_weighted': f1_weighted,\n",
    "               'prec_macro': prec_macro,\n",
    "               'prec_weighted': prec_weighted,\n",
    "               'recall_macro': recall_macro,\n",
    "               'recall_weighted': recall_weighted,\n",
    "               'roc_auc_macro': roc_auc_macro,\n",
    "               'roc_auc_weighted': roc_auc_weighted,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTestDict = make_folds(DF, \n",
    "                           (train_ids, test_ids), \n",
    "                           n_folds=10, \n",
    "                           stratified=True, \n",
    "                           splitting='from_file',\n",
    "                           label_col='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topic models using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_embeddings:\n",
    "    Text2Vecs = echo_utils.TextToVectors(source='cardio_wv',\n",
    "                                     embedding_path=EmbeddingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF extraction\n",
      "Latent DA\n",
      "ETM\n",
      "Add LDA betas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do for each TrainTestDict[k]['Xtrain]\n",
    "DataVersions = dict()\n",
    "\n",
    "xtrain = TrainTestDict[0]['Xtrain']\n",
    "xtest = TrainTestDict[0]['Xtest']\n",
    "\n",
    "\n",
    "TFVEC = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=num_words_in_vocab, stop_words=STOPWORDS, lowercase=lowercase\n",
    ")\n",
    "\n",
    "print(\"TFIDF extraction\")\n",
    "TFVEC.fit(xtrain)\n",
    "tokenid2word =  {v:k for k,v in TFVEC.vocabulary_.items()}\n",
    "xmatrix_train = TFVEC.transform(xtrain)\n",
    "xmatrix_test = TFVEC.transform(xtest)\n",
    "\n",
    "\n",
    "DataVersions['tfidf'] = {\n",
    "    'train': xmatrix_train,\n",
    "    'test': xmatrix_test\n",
    "}\n",
    "\n",
    "if model_ETM | model_LDA:\n",
    "    print(\"Latent DA\")\n",
    "    LDA = LatentDirichletAllocation(n_components=num_topics)\n",
    "    LDA.fit(xmatrix_train)\n",
    "\n",
    "    lda_theta = LDA.transform(xmatrix_train)\n",
    "    lda_theta_test = LDA.transform(xmatrix_test)\n",
    "    lda_beta = LDA.components_ / LDA.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if model_ETM:\n",
    "        print(\"ETM\")\n",
    "        xmatrix_train = echo_utils.ETM(xmatrix_train, lda_theta, lda_beta)\n",
    "        xmatrix_test = echo_utils.ETM(xmatrix_test, lda_theta_test, lda_beta)\n",
    "\n",
    "        DataVersions['tfidf_ETM'] = {\n",
    "            'train': xmatrix_train,\n",
    "            'test': xmatrix_test\n",
    "        }\n",
    "        str_ETM = \"_ETM\"\n",
    "\n",
    "    if model_LDA:\n",
    "        print(\"Add LDA betas\")\n",
    "        train_weight_matrix_lil = xmatrix_train.tolil()\n",
    "        num_cols = train_weight_matrix_lil.shape[1]\n",
    "        train_weight_matrix_lil.resize((train_weight_matrix_lil.shape[0], \n",
    "                                        num_cols+num_topics))\n",
    "        \n",
    "        for i in range(train_weight_matrix_lil.shape[0]):\n",
    "            for j in range(num_topics):\n",
    "                train_weight_matrix_lil[i, num_cols+j] = lda_theta[i][j]\n",
    "        xmatrix_train = train_weight_matrix_lil.tocsr()\n",
    "\n",
    "        test_weight_matrix_lil = xmatrix_test.tolil()\n",
    "        num_cols = test_weight_matrix_lil.shape[1]\n",
    "        test_weight_matrix_lil.resize((test_weight_matrix_lil.shape[0], \n",
    "                                        num_cols+num_topics))\n",
    "        \n",
    "        for i in range(test_weight_matrix_lil.shape[0]):\n",
    "            for j in range(num_topics):\n",
    "                test_weight_matrix_lil[i, num_cols+j] = lda_theta_test[i][j]\n",
    "        xmatrix_test = test_weight_matrix_lil.tocsr()\n",
    "\n",
    "        DataVersions[f'tfidf{str_ETM}_LDA'] = {\n",
    "            'train': xmatrix_train,\n",
    "            'test': xmatrix_test\n",
    "        }\n",
    "        DataVersions['LDA'] = {\n",
    "            'train': lda_theta,\n",
    "            'test': lda_theta_test\n",
    "        }        \n",
    "        \n",
    "        prior_str = f\"tfidf{str_ETM}_LDA\"\n",
    "        \n",
    "    if model_embeddings:\n",
    "        xmatrix_train_emb = Text2Vecs.vector_aggregation_LIL(xtrain)\n",
    "        xmatrix_test_emb = Text2Vecs.vector_aggregation_LIL(xtest)                 \n",
    "\n",
    "        test_weight_matrix_lil = xmatrix_test.tolil()\n",
    "        num_cols = test_weight_matrix_lil.shape[1]\n",
    "        test_weight_matrix_lil.resize((test_weight_matrix_lil.shape[0], \n",
    "                                        num_cols+xmatrix_test_emb.shape[1]))\n",
    "\n",
    "        for i in range(xmatrix_test.shape[0]):\n",
    "            for j in range(xmatrix_test_emb.shape[1]):\n",
    "                test_weight_matrix_lil[i, num_cols+j] = xmatrix_test_emb[i,j]\n",
    "        xmatrix_test = test_weight_matrix_lil.tocsr()\n",
    "\n",
    "        train_weight_matrix_lil = xmatrix_train.tolil()\n",
    "        num_cols = train_weight_matrix_lil.shape[1]\n",
    "        train_weight_matrix_lil.resize((train_weight_matrix_lil.shape[0], \n",
    "                                        num_cols+xmatrix_train_emb.shape[1]))\n",
    "\n",
    "        for i in range(xmatrix_train.shape[0]):\n",
    "            for j in range(xmatrix_train_emb.shape[1]):\n",
    "                train_weight_matrix_lil[i, num_cols+j] = xmatrix_train_emb[i,j]\n",
    "        xmatrix_train = train_weight_matrix_lil.tocsr() \n",
    "       \n",
    "        DataVersions[f\"{prior_str}_EMBS\"] = {\n",
    "            'train': xmatrix_train,\n",
    "            'test': xmatrix_test\n",
    "        }\n",
    "        \n",
    "        DataVersions[\"EMBS\"] = {\n",
    "            'train': xmatrix_train_emb,\n",
    "            'test': xmatrix_test_emb\n",
    "        }\n",
    "        \n",
    "       \n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_ETM\n",
      "tfidf_ETM_LDA\n",
      "LDA\n"
     ]
    }
   ],
   "source": [
    "for k,v in DataVersions.items():\n",
    "    print(k)\n",
    "    \n",
    "    clf = XGBClassifier(seed=42, n_estimators=150, max_depth=5, learning_rate=1e-1)\n",
    "    clf.fit(v['train'], TrainTestDict[0]['ytrain'])\n",
    "\n",
    "    preds = clf.predict_proba(v['test'])\n",
    "    ytest = TrainTestDict[0]['ytest']\n",
    "\n",
    "    DataVersions[k].update({\"results\": multi_label_metrics(preds,\n",
    "                                                           ytest, \n",
    "                                                           threshold=0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for tfidf\n",
      "F1 0.917 (0.882), REC 0.918 (0.875), PREC 0.918 (0.891)\n",
      "++++++++++++++++++++++++++++++\n",
      "Results for tfidf_ETM\n",
      "F1 0.918 (0.885), REC 0.919 (0.883), PREC 0.92 (0.89)\n",
      "++++++++++++++++++++++++++++++\n",
      "Results for tfidf_ETM_LDA\n",
      "F1 0.921 (0.892), REC 0.922 (0.886), PREC 0.922 (0.899)\n",
      "++++++++++++++++++++++++++++++\n",
      "Results for LDA\n",
      "F1 0.604 (0.461), REC 0.637 (0.462), PREC 0.61 (0.552)\n",
      "++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "for k,v in DataVersions.items():\n",
    "    print(f\"Results for {k}\")\n",
    "    res_string = f\"F1 {round(v['results']['f1_weighted'], 3)} ({round(v['results']['f1_macro'], 3)})\"\n",
    "    res_string += f\", REC {round(v['results']['recall_weighted'], 3)} ({round(v['results']['recall_macro'], 3)})\"\n",
    "    res_string += f\", PREC {round(v['results']['prec_weighted'], 3)} ({round(v['results']['prec_macro'], 3)})\"\n",
    "    #pprint.pp(f\"F1 {round(v['results']['f1_weighted'], 3)} ({round(v['results']['f1_macro'],3)})\")\n",
    "    #pprint.pp(f\"REC {round(v['results']['recall_weighted'],3)} ({round(v['results']['recall_macro'], 3)})\")\n",
    "    #pprint.pp(f\"PREC {round(v['results']['prec_weighted'],3)} ({round(v['results']['prec_macro'], 3)})\")\n",
    "    print(res_string)\n",
    "    print(30*\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echolabeler-DyKQcQCO-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
