{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:03.484607Z",
     "start_time": "2024-04-25T15:38:35.109090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Tuple, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel\n",
    "from setfit import Trainer, TrainingArguments\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "#from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "from torchinfo import summary\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import evaluate\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "import benedict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX 4000 Ada Generation\n",
      "Memory Allocated: 3415247872\n",
      "Max memory Allocated: 7992522752\n",
      "Memory reserved: 12106858496\n",
      "Max memory reserved: 12106858496\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated()}\")\n",
    "    print(f\"Max memory Allocated: {torch.cuda.max_memory_allocated()}\") \n",
    "    print(f\"Memory reserved: {torch.cuda.memory_reserved()}\")\n",
    "    print(f\"Max memory reserved: {torch.cuda.max_memory_reserved()}\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "deabbreviate = False\n",
    "filter_reports = True\n",
    "Class = 'diastolic_dysfunction' # lv_dil, rv_dil, pe, aortic_regurgitation, diastolic_dysfunction, lv_syst_func, aortic_stenosis, rv_syst_func, mitral_regurgitation,tricuspid_regurgitation,wma\n",
    "FLAG_TERMS = ['uitslag zie medische status', 'zie status', 'zie verslag status', 'slecht echovenster', 'echo overwegen', 'ge echo',\n",
    "              'geen echovenster', 'geen beoordeelbaar echo', 'geen beoordeelbare echo', 'verslag op ic']\n",
    "SAVE_TERMS = ['goed', 'geen', 'normaal', 'normale']\n",
    "use_multilabel = False\n",
    "reduce_labels = False\n",
    "num_epochs = 3\n",
    "NUM_SAMPLES = 500\n",
    "\n",
    "#body = \"FremyCompany/BioLORD-2023-M\"\n",
    "body = \"NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\"\n",
    "\n",
    "MULTILABELS = {'Mild': ['Mild', 'Present'], \n",
    "               'Severe': ['Severe', 'Present'],\n",
    "               'Moderate': ['Moderate', 'Present'],\n",
    "               'Normal': ['Normal'],\n",
    "               'No label': ['No label'],\n",
    "               'Present': ['Present'],\n",
    "               }\n",
    "\n",
    "Target_remapper = {'No label': 'No label', \n",
    "                   'Normal': 'Normal',\n",
    "                   'Mild': 'Not normal',\n",
    "                   'Moderate': 'Not normal',\n",
    "                   'Severe': 'Not normal',\n",
    "                   'Present':  'Not normal'\n",
    "                   }\n",
    "\n",
    "#Target_remapper = {'No label': 'Normal', \n",
    "#                   'Normal': 'Normal',\n",
    "#                   'Mild': 'Not normal',\n",
    "#                   'Moderate': 'Not normal',\n",
    "#                   'Severe': 'Not normal',\n",
    "#                   'Present':  'Not normal'\n",
    "#                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the src folder to the path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import deabber, echo_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deabbreviate:\n",
    "    ABBREVIATIONS = benedict.benedict(\"../assets/abbreviations.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:03.996106Z",
     "start_time": "2024-04-25T15:39:03.489109Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "def plot_history(history, val=0):\n",
    "    acc = history.history['accuracy']\n",
    "    if val == 1:\n",
    "        val_acc = history.history['val_accuracy'] # we can add a validation set in our fit function with nn\n",
    "    loss = history.history['loss']\n",
    "    if val == 1:\n",
    "        val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training accuracy')\n",
    "    if val == 1:\n",
    "        plt.plot(x, val_acc, 'r', label='Validation accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    if val == 1:\n",
    "        plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:04.476606Z",
     "start_time": "2024-04-25T15:39:03.998607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef multi_label_metrics(predictions, labels, threshold=0.5):\\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\\n    sigmoid = torch.nn.Sigmoid()\\n    probs = sigmoid(torch.Tensor(predictions))\\n    # next, use threshold to turn them into integer predictions\\n    y_pred = np.zeros(probs.shape)\\n    y_pred[np.where(probs >= threshold)] = 1\\n    # finally, compute metrics\\n    y_true = labels\\n    \\n    \\n    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\\n    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\\n    prec_macro = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\\n    prec_weighted = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\\n    recall_macro = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\\n    recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')    \\n    \\n    try:\\n        roc_auc_weighted = roc_auc_score(y_true, probs, average = 'weighted')\\n        roc_auc_macro = roc_auc_score(y_true, probs, average = 'macro')\\n    except:\\n        roc_auc_weighted = None\\n        roc_auc_macro = None\\n    \\n    accuracy = accuracy_score(y_true, y_pred)\\n    # return as dictionary\\n    metrics = {'f1_macro': f1_macro,\\n               'f1_weighted': f1_weighted,\\n               'prec_macro': prec_macro,\\n               'prec_weighted': prec_weighted,\\n               'recall_macro': recall_macro,\\n               'recall_weighted': recall_weighted,\\n               'roc_auc_macro': roc_auc_macro,\\n               'roc_auc_weighted': roc_auc_weighted,\\n               'accuracy': accuracy}\\n    return metrics\\n\\ndef compute_metrics(p: EvalPrediction):\\n    preds = p.predictions[0] if isinstance(p.predictions, \\n            tuple) else p.predictions\\n    result = multi_label_metrics(\\n        predictions=preds, \\n        labels=p.label_ids)\\n    return result\\n\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics_binomial(logits_and_labels, averaging='macro'):\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  acc = np.mean(predictions == labels)\n",
    "  f1 = f1_score(labels, predictions, average = averaging)\n",
    "  prec = precision_score(labels, predictions, average = averaging)\n",
    "  rec = recall_score(labels, predictions, average = averaging)\n",
    "  return {\n",
    "          'accuracy': acc, \n",
    "          'f1_score': f1,\n",
    "          'precision': prec,\n",
    "          'recal': rec\n",
    "          }\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  \n",
    "  f1_micro = f1_score(labels, predictions, average = 'micro')\n",
    "  f1_macro = f1_score(labels, predictions, average = 'macro')\n",
    "  \n",
    "  precision_micro = precision_score(labels, predictions, average = 'micro')\n",
    "  precision_macro = precision_score(labels, predictions, average = 'macro')\n",
    "\n",
    "  recall_micro = recall_score(labels, predictions, average = 'micro')\n",
    "  recall_macro = recall_score(labels, predictions, average = 'macro')\n",
    "  \n",
    "  return {'precision_micro': precision_micro, 'precision_macro': precision_macro,\n",
    "          'f1_score_micro': f1_micro, 'f1_score_macro': f1_macro, \n",
    "          'recall_micro': recall_micro, 'recall_macro': recall_macro}\n",
    "  \n",
    "def extract_metrics_agg(probas, labels, thresholds, average = 'macro'):\n",
    "  metrics = []\n",
    "  for t in thresholds:\n",
    "    preds = probas.argmax(axis=1)\n",
    "    preds = np.where(probas.max(axis=1)<t, -1, preds)\n",
    "    \n",
    "    idx_check = np.argwhere(preds > -1)[:,0]\n",
    "    idx_ignore = np.argwhere(preds == -1)[:,0]\n",
    "        \n",
    "    precision = precision_score(labels[idx_check], preds[idx_check], average = average)\n",
    "    recall = recall_score(labels[idx_check], preds[idx_check], average =average)\n",
    "    f1 = f1_score(labels[idx_check], preds[idx_check], average = average)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    metrics.append([precision, recall, f1, accuracy, len(idx_check) / len(labels)])\n",
    "  return np.array(metrics)\n",
    "\n",
    "\n",
    "def extract_confusion_matrix(probas, labels, thresholds):\n",
    "  # confusion_class_0 = [(threshold, TP, FP, TN, FN)]\n",
    "  \n",
    "  res_dict = defaultdict(tuple)\n",
    "    \n",
    "  for t in thresholds:\n",
    "    preds = probas.argmax(axis=1)\n",
    "    preds = np.where(probas.max(axis=1)<t, -1, preds)    \n",
    "    idx_check = np.argwhere(preds > -1)[:,0]\n",
    "    #\n",
    "    for _class in labels.unique():\n",
    "      TP = np.sum(np.logical_and(preds[idx_check] == 0, labels[idx_check] == 0))\n",
    "      TN = np.sum(np.logical_and(preds[idx_check] != 0, labels[idx_check] != 0))\n",
    "      FP = np.sum(np.logical_and(preds[idx_check] == 0, labels[idx_check] != 0))\n",
    "      FN = np.sum(np.logical_and(preds[idx_check] != 0, labels[idx_check] == 0))\n",
    "      res_dict[_class].append((t, TP, FP, TN, FN))\n",
    "  return res_dict\n",
    "\n",
    "\n",
    "def plot_AUROC(ax, probas, labels, class_idx = 0, label = None):\n",
    "  from sklearn.metrics import roc_curve, auc\n",
    "  fpr, tpr, _ = roc_curve((labels==class_idx).astype('int'),\n",
    "                          probas[:, class_idx])\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  ax.plot(fpr, tpr, lw=2, label=f'{label} (area = {roc_auc:.2f})')\n",
    "  ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "  ax.set_xlim([0.0, 1.0])\n",
    "  ax.set_ylim([0.0, 1.05])\n",
    "  ax.set_xlabel('False Positive Rate')\n",
    "  ax.set_ylabel('True Positive Rate')\n",
    "  ax.set_title(f'ROC curve')\n",
    "  ax.legend(loc=\"lower right\")\n",
    "  return ax\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "'''\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    \n",
    "    \n",
    "    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    prec_macro = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    prec_weighted = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_macro = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')    \n",
    "    \n",
    "    try:\n",
    "        roc_auc_weighted = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "        roc_auc_macro = roc_auc_score(y_true, probs, average = 'macro')\n",
    "    except:\n",
    "        roc_auc_weighted = None\n",
    "        roc_auc_macro = None\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_macro': f1_macro,\n",
    "               'f1_weighted': f1_weighted,\n",
    "               'prec_macro': prec_macro,\n",
    "               'prec_weighted': prec_weighted,\n",
    "               'recall_macro': recall_macro,\n",
    "               'recall_weighted': recall_weighted,\n",
    "               'roc_auc_macro': roc_auc_macro,\n",
    "               'roc_auc_weighted': roc_auc_weighted,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:05.003636Z",
     "start_time": "2024-04-25T15:39:04.479108Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # labels should be a list or a 1D tensor\n",
    "    one_hot = torch.zeros((len(labels), num_classes))\n",
    "    rows = torch.arange(len(labels))\n",
    "    one_hot[rows, labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "def _mlabel_tuple_creator(x: List[int],\n",
    "                          multilabels:Dict[int,List[int]],\n",
    "                          num_classes: int=None)\\\n",
    "                          ->List[Tuple[int,...]]:\n",
    "                              \n",
    "    res = [(_sc for _sc in multilabels[sc]) for sc in x]\n",
    "    return res\n",
    "\n",
    "def multi_hot_encoding(x: List[int], \n",
    "                       multilabels: Union[Dict[int,List[int]], None]=None,\n",
    "                       num_classes: int=None)\\\n",
    "                           ->torch.Tensor:    \n",
    "    if multilabels is None:\n",
    "        return one_hot_encode(x, num_classes=num_classes)\n",
    "    else:\n",
    "        return torch.Tensor(MultiLabelBinarizer(classes=range(num_classes))\\\n",
    "                    .fit_transform(_mlabel_tuple_creator(x,multilabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:07.328611Z",
     "start_time": "2024-04-25T15:39:06.261613Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:08.119113Z",
     "start_time": "2024-04-25T15:39:07.331615Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_documents = pd.read_json(f\"./echo_doc_labels/{Class}.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "No label    3474\n",
       "Mild         632\n",
       "Normal       521\n",
       "Moderate     243\n",
       "Severe       130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_documents.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_remapper = {'No label': 'No label', \n",
    "                   'Normal': 'Normal',\n",
    "                   'Mild': 'Not normal',\n",
    "                   'Moderate': 'Not normal',\n",
    "                   'Severe': 'Not normal',\n",
    "                   'Present': 'Not normal'\n",
    "                   }\n",
    "if reduce_labels:\n",
    "    labeled_documents['label'] = labeled_documents.label.map(Target_remapper)\n",
    "    print(labeled_documents.label.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:09.049113Z",
     "start_time": "2024-04-25T15:39:08.571112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Expand with label columns\n",
    "Target_maps = {Label:i for i,Label in enumerate(labeled_documents['label'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'No label': 0, 'Normal': 1, 'Mild': 2, 'Severe': 3, 'Moderate': 4}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:10.056612Z",
     "start_time": "2024-04-25T15:39:09.051621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ECHO_StudyID', 'ECHO_StudyID.1', 'input_hash', 'task_hash'], dtype='object')\n",
      "Train hashes: 96026\n",
      "Test hashes: 24051\n"
     ]
    }
   ],
   "source": [
    "# Load the train/test hashes\n",
    "test_hashes = pd.read_csv('./test_echoid.csv', sep=',')\n",
    "train_hashes = pd.read_csv('./train_echoid.csv', sep=',')\n",
    "print(train_hashes.columns)\n",
    "\n",
    "print(f\"Train hashes: {train_hashes.input_hash.nunique()}\")\n",
    "print(f\"Test hashes: {test_hashes.input_hash.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['No label', 'Normal', 'Mild', 'Severe', 'Moderate'])\n"
     ]
    }
   ],
   "source": [
    "print(Target_maps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:14.753116Z",
     "start_time": "2024-04-25T15:39:10.601615Z"
    }
   },
   "outputs": [],
   "source": [
    "# We now make DataSets (a special HuggingFace structure)\n",
    "# assuming cross-validation\n",
    "\n",
    "DF = labeled_documents\n",
    "DF.columns = ['sentence', 'labels', '_input_hash']\n",
    "\n",
    "label2id = Target_maps\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "DF['labels'] = DF['labels'].map(label2id)\n",
    "\n",
    "\n",
    "if filter_reports:\n",
    "    DF = DF.assign(sentence = echo_utils.report_filter(DF.sentence, \n",
    "                                            flag_terms=FLAG_TERMS, \n",
    "                                            save_terms=SAVE_TERMS)[0])\n",
    "    DF = DF.loc[DF.sentence.notna()]\n",
    "\n",
    "if deabbreviate:\n",
    "    DeAbber = deabber.deabber(model_type='sbert', \n",
    "                              abbreviations=ABBREVIATIONS['nl']['echocardiogram'], \n",
    "                              min_sim=0.5, top_k=10)\n",
    "    DF = DF.assign(sentence=DeAbber.deabb(DF.sentence.values, TokenRadius=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_multilabel:\n",
    "    _multilabels = {label2id[k]: [label2id[l] for l in v]\n",
    "                    for k,v in MULTILABELS.items()}\n",
    "else:\n",
    "    _multilabels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:\n",
      "labels\n",
      "0    2694\n",
      "2     501\n",
      "1     419\n",
      "4     191\n",
      "3     104\n",
      "Name: count, dtype: int64\n",
      "Train labels sampled:\n",
      "labels\n",
      "0    500\n",
      "1    500\n",
      "2    500\n",
      "3    500\n",
      "4    500\n",
      "Name: count, dtype: int64\n",
      "Test labels:\n",
      "labels\n",
      "0    682\n",
      "1    105\n",
      "2    105\n",
      "4     44\n",
      "3     22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: make proper\n",
    "DFtrain = DF.loc[DF._input_hash.isin(train_hashes.input_hash), ['sentence', 'labels']]\n",
    "DFtrain_SAMPLED = DFtrain.groupby('labels').sample(NUM_SAMPLES, replace=True)\n",
    "num_classes = DFtrain.labels.nunique()\n",
    "\n",
    "DFtest = DF.loc[DF._input_hash.isin(test_hashes.input_hash), ['sentence', 'labels']]\n",
    "\n",
    "print(\"Train labels:\")\n",
    "print(DFtrain.labels.value_counts())\n",
    "print(\"Train labels sampled:\")\n",
    "print(DFtrain_SAMPLED.labels.value_counts())\n",
    "print(\"Test labels:\")\n",
    "print(DFtest.labels.value_counts())\n",
    "\n",
    "TrainSet = Dataset.from_pandas(DFtrain_SAMPLED)\n",
    "TestSet = Dataset.from_pandas(DFtest)\n",
    "\n",
    "HF_DataSet = DatasetDict(\n",
    "    {'train' : TrainSet,\n",
    "     'test': TestSet,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"L://laupodteam/AIOS/Bram/language_modeling/Models/language_models/setfit\"\n",
    "logging_dir = os.path.join(base_dir, \"setfit_logs\")\n",
    "setfit_output = os.path.join(base_dir, \"./setfit_output\")\n",
    "\n",
    "os.makedirs(logging_dir, exist_ok=True)\n",
    "os.makedirs(setfit_output, exist_ok=True)\n",
    "\n",
    "train_args = TrainingArguments(logging_dir=logging_dir,\n",
    "                               output_dir=setfit_output, \n",
    "                               evaluation_strategy='no',\n",
    "                               num_iterations=6,\n",
    "                               num_epochs=num_epochs,\n",
    "                               body_learning_rate = (2e-5, 1e-5),\n",
    "                               head_learning_rate= (1e-3),\n",
    "                               batch_size=32,\n",
    "                               seed=42,                                                          \n",
    "                               loss=CosineSimilarityLoss,                               \n",
    "                               save_strategy=\"epoch\",\n",
    "                               load_best_model_at_end=False,\n",
    "                               show_progress_bar=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:15.961514Z",
     "start_time": "2024-04-25T15:39:14.755619Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bes3\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\echolabeler-DyKQcQCO-py3.10\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_body = SentenceTransformer(body)\n",
    "model_head = SVC()\n",
    "model_setfit = SetFitModel(model_body, model_head)\n",
    "                                           \n",
    "# use_differentiable_head=True, \n",
    "# head_params={\"out_features\": num_classes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:39:17.139141Z",
     "start_time": "2024-04-25T15:39:16.445015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acaf7f8eaed485eb1f9cf4f93c7ac8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setfit_trainer = Trainer(model_setfit, \n",
    "                        train_dataset=TrainSet, \n",
    "                        eval_dataset=TestSet,\n",
    "                        args = train_args,   \n",
    "                        metric = compute_metrics,                         \n",
    "                        column_mapping={\"sentence\": \"text\", \n",
    "                                        \"labels\": \"label\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 30000\n",
      "  Batch size = 32\n",
      "  Num epochs = 3\n",
      "  Total optimization steps = 2814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4aa7b9111646bab4fafec383018a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7811c40291b44994ae9aa841ca08b387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2754, 'learning_rate': 7.092198581560284e-08, 'epoch': 0.0}\n",
      "{'embedding_loss': 0.2749, 'learning_rate': 3.5460992907801423e-06, 'epoch': 0.05}\n"
     ]
    }
   ],
   "source": [
    "setfit_trainer.train(train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, labels):\n",
    "  #logits, labels = logits_and_labels\n",
    "  #predictions = np.argmax(logits, axis=-1)\n",
    "  \n",
    "  f1_micro = f1_score(labels, predictions, average = 'micro')\n",
    "  f1_macro = f1_score(labels, predictions, average = 'macro')\n",
    "  \n",
    "  precision_micro = precision_score(labels, predictions, average = 'micro')\n",
    "  precision_macro = precision_score(labels, predictions, average = 'macro')\n",
    "\n",
    "  recall_micro = recall_score(labels, predictions, average = 'micro')\n",
    "  recall_macro = recall_score(labels, predictions, average = 'macro')\n",
    "  \n",
    "  return {'precision_micro': precision_micro, 'precision_macro': precision_macro,\n",
    "          'f1_score_micro': f1_micro, 'f1_score_macro': f1_macro, \n",
    "          'recall_micro': recall_micro, 'recall_macro': recall_macro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    }
   ],
   "source": [
    "setfit_trainer.metric = compute_metrics\n",
    "metrics = setfit_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_micro': 0.8402922755741128,\n",
       " 'precision_macro': 0.6018548071135666,\n",
       " 'f1_score_micro': 0.8402922755741128,\n",
       " 'f1_score_macro': 0.6613681103796277,\n",
       " 'recall_micro': 0.8402922755741128,\n",
       " 'recall_macro': 0.7740217403181765}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_res = dict()\n",
    "robert_res['aortic regurgitation'] = \\\n",
    "    {'precision_micro': 0.9303621169916435,\n",
    "    'precision_macro': 0.8428764056416101,\n",
    "    'f1_score_micro': 0.9303621169916435,\n",
    "    'f1_score_macro': 0.8587103420346984,\n",
    "    'recall_micro': 0.9303621169916435,\n",
    "    'recall_macro': 0.8807387791045382\n",
    "    }\n",
    "\n",
    "robert_res['aortic stenosis'] =  \\\n",
    "   {'precision_micro': 0.8935281837160751,\n",
    " 'precision_macro': 0.6898065746737378,\n",
    " 'f1_score_micro': 0.8935281837160751,\n",
    " 'f1_score_macro': 0.7744423670865763,\n",
    " 'recall_micro': 0.8935281837160751,\n",
    " 'recall_macro': 0.9305292256686146}\n",
    "\n",
    "\n",
    "robert_res['diastolic_dysfunction'] =  \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (48850072.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[66], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    biolord_res['aortic stenosis'] =\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "biolord_res = dict()\n",
    "biolord_res['aortic regurgitation'] = \\\n",
    "{'precision_micro': 0.8180129990714949,\n",
    " 'precision_macro': 0.6544492741142449,\n",
    " 'f1_score_micro': 0.8180129990714949,\n",
    " 'f1_score_macro': 0.6814811218681188,\n",
    " 'recall_micro': 0.8180129990714949,\n",
    " 'recall_macro': 0.7195548322882586\n",
    " }\n",
    "\n",
    "biolord_res['aortic stenosis'] = \\\n",
    "{'precision_micro': 0.8402922755741128,\n",
    " 'precision_macro': 0.6018548071135666,\n",
    " 'f1_score_micro': 0.8402922755741128,\n",
    " 'f1_score_macro': 0.6613681103796277,\n",
    " 'recall_micro': 0.8402922755741128,\n",
    " 'recall_macro': 0.7740217403181765}\n",
    "\n",
    "biolord_res['diastolic_dysfunction'] = \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
