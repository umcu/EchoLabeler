{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gc\n",
    "import deduce\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from typing import Callable\n",
    "\n",
    "from gensim.models import phrases\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dill\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import medcat_utils as mu"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context:\n",
    "* $100$ K echocardiographic reports available. \n",
    "* we want to extract diagnoses regarding the left-ventricle function\n",
    "* we have $5000$ reports with labeled spans.\n",
    "\n",
    "# Goal:\n",
    "Train a \"model\" that can\n",
    "1. identify the spans\n",
    "2. classify the spans\n",
    "\n",
    "# Approach: MedCAT - MetaCAT\n",
    "\n",
    "## Two-step approach\n",
    "\n",
    "* unsupervised training on the documents\n",
    "* add a single custom entity with a custom identifier\n",
    "* train a model to identify the custom entities\n",
    "* supervised training on the meta-annotations of the entities\n",
    "\n",
    "## One-step approach\n",
    "\n",
    "* unsupervised training on the documents\n",
    "* add custom entities based on the spans and their labels\n",
    "* train a model to identify the custom entities\n",
    "\n",
    "# Approach: biLSTM/transformer\n",
    "\n",
    "## Two-step approach\n",
    "\n",
    "* Train a model to identify the spans: self-supervision by random selecting non-span ranges as negative examples\n",
    "* Train a model to classify the spans: supervised based on the labeled spans \n",
    "* Combine the model in one pipeline\n",
    "\n",
    "## One-step approach\n",
    "* Assign a label to each span\n",
    "* Train a model to identify the spans"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO:..\n",
    "# 1. by adding a \"no_label\" class to each label, in memory\n",
    "# 2. re-train with reduced labels\n",
    "# 3. re-evaluate with whole pipeline!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load Medcat modelpack"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "# extract the environment variable 'medcat_pack'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from medcat.cat import CAT\n",
    "from medcat.vocab import Vocab\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config import Config\n",
    "from medcat.meta_cat import MetaCAT"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "base_medcat_path = os.getenv('medcat_pack')\n",
    "pack_location = 'umls-dutch-v1-10_echo'\n",
    "prep_medcat = False\n",
    "train_with_negatives = True\n",
    "REDUCED=False\n",
    "PRETRAIN_META=False "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load texts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "echo_path = 'T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData'\n",
    "# load the jsonl in a dataframe\n",
    "texts = pd.read_json(os.path.join(echo_path, 'outdb_140423.jsonl'), lines=True)\n",
    "#texts_zipped = zip(texts['text'], texts['_input_hash'], texts['_task_hash'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_indcs = pd.read_csv(os.path.join(echo_path, 'train_echoid.csv'), sep=',')\n",
    "test_indcs = pd.read_csv(os.path.join(echo_path, 'test_echoid.csv'), sep=',')\n",
    "\n",
    "train_indcs = train_indcs[train_indcs.input_hash.notna()]\n",
    "test_indcs = test_indcs[test_indcs.input_hash.notna()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "TRAIN_INPUT_HASH = set(train_indcs['input_hash'].astype(int).to_list())\n",
    "TRAIN_TASK_HASH = set(train_indcs['task_hash'].astype(int).to_list())\n",
    "\n",
    "TEST_INPUT_HASH = set(test_indcs['input_hash'].astype(int).to_list())\n",
    "TEST_TASK_HASH = set(test_indcs['task_hash'].astype(int).to_list())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "texts_train = texts.loc[texts._input_hash.isin(TRAIN_INPUT_HASH)]\n",
    "texts_test = texts.loc[texts._input_hash.isin(TEST_INPUT_HASH)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load MedCAT model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MCAT = CAT.load_model_pack(str(os.path.join(base_medcat_path, pack_location)))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make splitted data\n",
    "\n",
    "* MedCAT train files\n",
    "* class-dictionary with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# psyheartanalyse\n",
    "# psyheartanalyse baseline\n",
    "\n",
    "# TODO:\n",
    "# lv_syst_func: \n",
    "# lv_sys_func_unchanged, lv_sys_func_improved -> lv_sys_func_unknown\n",
    "# lv_syst_func_normal -> lv_sys_func_normal\n",
    "\n",
    "# rv_syst_func:\n",
    "# rv_syst_func_normal -> rv_sys_func_normal"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if REDUCED==True:\n",
    "    file_dir = os.path.join(echo_path, 'echo_span_labels', 'reduced_labels')\n",
    "    class_names = os.listdir(file_dir)\n",
    "else:\n",
    "    file_dir = os.path.join(echo_path, 'echo_span_labels', 'full_labels')\n",
    "    class_names = os.listdir(file_dir)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_index = [i for i,t in enumerate(class_names) \n",
    "                    if ('merged' in t) | ('old' in t)]\n",
    "merged_index = sorted(merged_index, reverse=True)\n",
    "merged_name = class_names[merged_index[0]]\n",
    "\n",
    "for mind in merged_index:\n",
    "    class_names.pop(mind)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class_ds = defaultdict(dict)\n",
    "\n",
    "for file_name in class_names:   \n",
    "    _class = file_name.split(\".\")[0]\n",
    "    fn = os.path.join(file_dir, file_name)\n",
    "    class_ds[_class]['ds'] = pd.read_json(fn, lines=True)\n",
    "    \n",
    "    class_ds[_class]['train'] = {}\n",
    "    class_ds[_class]['test'] = {}    \n",
    "    \n",
    "    medcat_json = mu.get_medcat_json_per_class(fn, TRAIN_INPUT_HASH, _class)\n",
    "    if train_with_negatives:\n",
    "        medcat_json = mu.update_medcat_json_per_class_with_negatives(MedCATJSON=medcat_json, ClassName=_class)\n",
    "        \n",
    "    medcat_out = os.path.join(echo_path, 'medcat_labels', 'train', f'medcat_{_class}.json')\n",
    "    json.dump(medcat_json, open(medcat_out, 'w'))\n",
    "    class_ds[_class]['train']['location'] = medcat_out\n",
    "    class_ds[_class]['train']['json'] = medcat_json\n",
    "     \n",
    "    medcat_json = mu.get_medcat_json_per_class(fn, TEST_INPUT_HASH, _class)\n",
    "    if train_with_negatives:\n",
    "        medcat_json = mu.update_medcat_json_per_class_with_negatives(MedCATJSON=medcat_json, ClassName=_class)  \n",
    "    \n",
    "    medcat_out = os.path.join(echo_path, 'medcat_labels', 'test', f'medcat_{_class}.json')\n",
    "    json.dump(medcat_json, open(medcat_out, 'w'))\n",
    "    class_ds[_class]['test']['location'] = medcat_out\n",
    "    class_ds[_class]['test']['json'] = medcat_json\n",
    "    \n",
    "    class_ds[_class]['labels'] = set([span_dict['label'] \n",
    "                                        for span_list in class_ds[_class]['ds']['spans'].tolist()\n",
    "                                        for span_dict in span_list])\n",
    "    if train_with_negatives:\n",
    "        class_ds[_class]['labels'].add('nolabel')\n",
    "        # we want to collect per inputhash per span\n",
    "        # if there is any label\n",
    "        # if there is a label for the current class\n",
    "        spans_per_doc = class_ds[_class]['ds'][['_input_hash', 'spans']]\\\n",
    "                            .set_index('_input_hash')\\\n",
    "                            .to_dict(orient='index')\n",
    "\n",
    "        span_tuples_per_doc = {k:[(_v['start'], _v['end']) \n",
    "                                  for _v in v['spans']] \n",
    "                                  for k,v in spans_per_doc.items()\n",
    "                                  }\n",
    "        class_ds[_class]['spans_with_labels'] = span_tuples_per_doc\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if train_with_negatives:\n",
    "    SpansPresent = defaultdict(set)\n",
    "    SpanClassPresent = defaultdict(lambda: defaultdict(set))\n",
    "    for k,v in class_ds.items():\n",
    "        for inp_hash, sl in class_ds[k]['spans_with_labels'].items():\n",
    "            for _sl in sl:\n",
    "                SpansPresent[inp_hash].add(_sl)\n",
    "                SpanClassPresent[k][inp_hash].add(_sl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ClassMap = defaultdict(dict)\n",
    "for k, v in class_ds.items():\n",
    "    for lab in v['labels']:\n",
    "        ClassMap[lab] = k\n",
    "ClassMap['normal'] = 'normal'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning for NER+L"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if PRETRAIN_META:\n",
    "    MCAT.train(texts_train.text.values, \n",
    "                nepochs=3, \n",
    "                progress_print=10,  \n",
    "                is_resumed=True)\n",
    "    MCAT.create_model_pack(base_medcat_path + \"/umls-dutch-v1-10_echo\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add label spans from Prodigy annotations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "span_sets = defaultdict(set)\n",
    "for _class, dsd in tqdm(class_ds.items()):\n",
    "        ds = dsd['ds']\n",
    "        span_set = set()\n",
    "        span_list = []\n",
    "        ds = ds[ds._input_hash.isin(TRAIN_INPUT_HASH)]\n",
    "        for k, (_spans, text) in enumerate(zip(ds[ds.spans.notna()].spans.values,\n",
    "                                            ds[ds.spans.notna()].text.values)):\n",
    "            for _span in _spans:\n",
    "                start, end = _span['start'], _span['end']\n",
    "                span_set.add(text[start:end])\n",
    "                span_list.append(text[start:end])\n",
    "        span_sets[_class] = span_set"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: interesting to add variants without the abbreviations?\n",
    "# TODO: including paraphrasing?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if PRETRAIN_META:\n",
    "    for _class, span_set in tqdm(span_sets.items()):\n",
    "        for _span in span_set:\n",
    "            MCAT.add_and_train_concept(cui=_class,\n",
    "                                    name=_span, \n",
    "                                    do_add_concept=True,\n",
    "                                    negative=False,\n",
    "                                )\n",
    "            \n",
    "    MCAT.train_supervised(data_path=os.path.join(base_medcat_path, \n",
    "                                     \"umls-dutch-v1-10_echo\",\n",
    "                                     \"input/ner_l_anno/trainer_export.json\"), \n",
    "                          nepochs=4,\n",
    "                          print_stats=0,\n",
    "                          use_filters=False)\n",
    "    medcat_path = base_medcat_path + \"/umls-dutch-v1-10_echoV2\"\n",
    "    MCAT.create_model_pack(medcat_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning of MetaCAT models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load MCAT from file\n",
    "medcat_path = os.path.join(base_medcat_path, 'umls-dutch-v1-10_echoV2')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MCAT = CAT.load_model_pack(medcat_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from medcat.meta_cat import MetaCAT\n",
    "from medcat.config_meta_cat import ConfigMetaCAT\n",
    "from medcat.tokenizers.meta_cat_tokenizers import TokenizerWrapperBPE, ByteLevelBPETokenizer\n",
    "\n",
    "# TODO: train embedding based on training text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load tokenizer from negation_model\n",
    "# tokenizer folder \n",
    "tok_folder = 'T:/laupodteam/AIOS/Bram/language_modeling/Clinical_embeddings/unigrams/with_tokenizer/v2/tokenizer'\n",
    "emb_folder = 'T:/laupodteam/AIOS/Bram/language_modeling/Clinical_embeddings/unigrams/with_tokenizer/v2/SG'\n",
    "tokenizer = ByteLevelBPETokenizer.from_file(os.path.join(tok_folder, 'vocab.json'), \n",
    "                                            os.path.join(tok_folder, 'merges.txt'))\n",
    "wrapped_tokenizer = TokenizerWrapperBPE(hf_tokenizers=tokenizer)\n",
    "wrapped_tokenizer.save(medcat_path + \"/assets/tokenizer\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "vec_path = os.path.join(emb_folder, 'sg')\n",
    "print(vec_path)\n",
    "w2v = KeyedVectors.load(vec_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create embedding matrix\n",
    "embeddings = []\n",
    "words_not_present = []\n",
    "\n",
    "in_vocab = 0\n",
    "for i in range(tokenizer.get_vocab_size()):\n",
    "    word = tokenizer.id_to_token(i)\n",
    "    if word in w2v:\n",
    "        embeddings.append(w2v[word])\n",
    "        in_vocab +=1\n",
    "    else:\n",
    "        words_not_present.append(i)\n",
    "        embeddings.append(np.random.random(300))\n",
    "\n",
    "print(f\"In vocab:{in_vocab}\")\n",
    "mean_vector = np.mean(embeddings, axis=0)\n",
    "\n",
    "for i in words_not_present:\n",
    "    embeddings[i] = mean_vector\n",
    "\n",
    "# Save the embeddings\n",
    "embeddings_array = np.array(embeddings)\n",
    "np.save(open(medcat_path+\"/assets/embeddings/embedding.npy\", \n",
    "             'wb'), embeddings_array)\n",
    "\n",
    "print(f\"Words not present:{len(words_not_present)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "for _class, d in class_ds.items():\n",
    "    if _class != 'normal':\n",
    "        print(f\"Commencing training of biLSTM-span for {_class}...\")\n",
    "        config_metacat = ConfigMetaCAT()\n",
    "        config_metacat.general['category_name'] = _class\n",
    "        config_metacat.train['nepochs'] = 25\n",
    "        config_metacat.train['score_average'] = 'macro'\n",
    "        config_metacat.model['hidden_size'] = 16\n",
    "        config_metacat.model['input_size'] = 300\n",
    "        config_metacat.model['dropout'] = 0.25\n",
    "        config_metacat.model['num_layers'] = 32\n",
    "        config_metacat.model['num_directions'] = 2\n",
    "        config_metacat.model['nclasses'] = len(d['labels'])\n",
    "        config_metacat.model['model_name'] = 'lstm'\n",
    "        \n",
    "        meta_cat = MetaCAT(tokenizer=wrapped_tokenizer,\n",
    "                           embeddings=embeddings_array,\n",
    "                           config=config_metacat)\n",
    "        \n",
    "        train_path = d['train']['location']\n",
    "        model_path = os.path.join(medcat_path, f\"meta_{_class}\")\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        \n",
    "        meta_cat.train_from_json(json_path=train_path, \n",
    "                                 save_dir_path=model_path)\n",
    "        \n",
    "        meta_cat.save(save_dir_path=model_path)\n",
    "        # now manually add the model to the model_pack...\n",
    "        label_dict = config_metacat.general.category_value2id\n",
    "        \n",
    "        # add to config\n",
    "        medcat_config = json.load(open(os.path.join(medcat_path, 'model_card.json'), 'r'))\n",
    "        \n",
    "        # add to \"MetaCAT models\" list\n",
    "        medcat_config[\"MetaCAT models\"].append({\n",
    "                      \"Category Name\": _class,\n",
    "                      \"Description\": f\"Labels: {REDUCED}, Negatives: {train_with_negatives}\",\n",
    "                      \"Classes\": label_dict,\n",
    "                        \"Model\": \"lstm\"\n",
    "                    })\n",
    "        # write config to .json\n",
    "        gc.collect()\n",
    "        json.dump(medcat_config, open(os.path.join(medcat_path, 'model_card.json'), 'w'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print(f\"learnable parameters: {count_parameters(meta_cat.model)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  new model pack"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "os.path.join(medcat_path)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MCATnew = CAT.load_model_pack(os.path.join(medcat_path))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to texts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from spacy import displacy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "i = 1123\n",
    "doc = MCATnew(texts.text.values[i])\n",
    "displacy.render(doc, style='ent')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "doc = MCATnew(texts.text.values[i])\n",
    "inds = []\n",
    "res = []\n",
    "for ent in doc.ents:\n",
    "    inds.append(ent.text)\n",
    "    res.append(mu.create_dict_with_conf(ent))\n",
    "    \n",
    "res_df = pd.DataFrame(res, index=inds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_test_results(medcat_path:str=None, class_ds: dict=None, score_average: str=\"macro\"):\n",
    "    TEST_RESULTS = {}\n",
    "    for k in class_ds.keys():\n",
    "        if k != 'normal':\n",
    "            CATmodel = MetaCAT.load(os.path.join(medcat_path, f'meta_{k}'))\n",
    "            CATmodel.config['train']['score_average'] = score_average # weighted, macro\n",
    "            test_location = class_ds[k]['test']['location']\n",
    "            test_labels = json.load(open(test_location, 'r'))\n",
    "            bulk_res = CATmodel.eval(test_location)\n",
    "            \n",
    "            res_count = defaultdict(int)\n",
    "            for doc in test_labels['projects'][0]['documents']:\n",
    "                anns = doc['annotations']\n",
    "                if len(anns)>0:\n",
    "                    for ann in anns:            \n",
    "                        _ann = ann['meta_anns'][k]\n",
    "                        res_count[_ann['value']] += 1\n",
    "    \n",
    "            TEST_RESULTS[k] = {\n",
    "                'f1':  bulk_res['f1'],\n",
    "                'precision': bulk_res['precision'],\n",
    "                'recall': bulk_res['recall'],\n",
    "                'confusion_df': bulk_res['confusion matrix'],\n",
    "                'real_presence': res_count\n",
    "            }\n",
    "    return TEST_RESULTS"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TEST_RESULTS_MACRO = get_test_results(medcat_path=medcat_path, class_ds=class_ds, score_average='macro')\n",
    "TEST_RESULTS_WEIGHTED = get_test_results(medcat_path=medcat_path, class_ds=class_ds, score_average='weighted')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res_dict_list = []\n",
    "rnd_nmbr = 2\n",
    "for k,v in TEST_RESULTS_WEIGHTED.items():\n",
    "    res_dict_list.append({\n",
    "            'class': k,\n",
    "            'f1': str(round(TEST_RESULTS_WEIGHTED[k]['f1'],rnd_nmbr))+\"(\"+str(round(TEST_RESULTS_MACRO[k]['f1'],rnd_nmbr))+\")\",\n",
    "            'recall': str(round(TEST_RESULTS_WEIGHTED[k]['recall'],rnd_nmbr))+\"(\"+str(round(TEST_RESULTS_MACRO[k]['recall'],rnd_nmbr))+\")\",\n",
    "            'precision': str(round(TEST_RESULTS_WEIGHTED[k]['precision'],rnd_nmbr))+\"(\"+str(round(TEST_RESULTS_MACRO[k]['precision'],rnd_nmbr))+\")\",\n",
    "        })\n",
    "res_df = pd.DataFrame(res_dict_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dill.dump(res_df, file=open(\"../artifacts/MetaCAT_test_results_with_negatives_reduced.pkl\", \"wb\"))\n",
    "#TEST_RESULTS = dill.load(open(\"../artifacts/MetaCAT_test_results.pkl\", \"rb\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment of performance in the wild"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# refactor this code for seperate json file per class\n",
    "merged_labels_train = mu.get_medcat_json_per_class(open(os.path.join(file_dir, 'merged_labels.jsonl'), 'r'), \n",
    "                                    TRAIN_INPUT_HASH, 'merged', ClassMap=ClassMap)['projects'][0]['documents']\n",
    "\n",
    "merged_labels_test = mu.get_medcat_json_per_class(open(os.path.join(file_dir, 'merged_labels.jsonl'), 'r'), \n",
    "                                    TEST_INPUT_HASH, 'merged', ClassMap=ClassMap)['projects'][0]['documents']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "comparison_list = []\n",
    "for medcat_doc in tqdm(merged_labels_test):\n",
    "    txt = medcat_doc['text']\n",
    "    parsed_doc = MCATnew(txt)\n",
    "\n",
    "    res = []\n",
    "    start_stop = []\n",
    "    for ent in parsed_doc.ents:\n",
    "        start_stop.append((ent.start_char, ent.end_char))\n",
    "        res.append(mu.create_dict_with_conf(ent))\n",
    "    \n",
    "    medcat_doc['predicted'] = {k:v for k,v in zip(start_stop, res)}\n",
    "    comparison_list.append(medcat_doc)\n",
    "comparison_list_with_annotations_present = [d for d in comparison_list if len(d['annotations'])>0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_pred_list(pred_dict: dict=None):\n",
    "    return list(set([v for k,v in pred_dict.items() if 'conf_' not in k]))\n",
    "    \n",
    "span_suggester_comparison_list = []\n",
    "for d in tqdm(comparison_list_with_annotations_present):\n",
    "    anns = d['annotations']\n",
    "    span_cat_list = []\n",
    "    for ann in anns:\n",
    "        span = (ann['start'], ann['end'])\n",
    "        class_val = ann['meta_anns']['merged']['value']\n",
    "        span_cat_list.append((span, class_val))\n",
    "    \n",
    "    pred_list = []\n",
    "    for k, pred in d['predicted'].items():\n",
    "        pred_list.append((k, get_pred_list(pred)))\n",
    "    \n",
    "    span_suggester_comparison_list.append([span_cat_list, pred_list])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We want to get the coverage of spans -> a count of all (partially) overlapping spans\n",
    "# We want to get the token overlap of covered spans (Jaccard?)\n",
    "# span overlap count\n",
    "\n",
    "def _tuple_overlap(tL, tR):\n",
    "    # tL: tuple(begin, end)\n",
    "    # tR: tuple(begin, end)\n",
    "    tLrange = set(range(*tL))\n",
    "    tRrange = set(range(*tR))\n",
    "               \n",
    "    InterSection = len(tLrange.intersection(tRrange))\n",
    "    Union = len(tLrange.union(tRrange))\n",
    "    \n",
    "    return InterSection/Union if Union>0 else np.nan\n",
    "\n",
    "def span_overlap_counter(labeled_spans, reversed=False):\n",
    "    # label_spans: list[[list[((begin,end), label)], list[((begin,end), [labels])]]]\n",
    "    OverlapList = []\n",
    "    for span_labs in labeled_spans:\n",
    "        span_set_medcat = set()\n",
    "        span_set_labeled = set()\n",
    "        for lab_span in span_labs[0]:\n",
    "            span_set_labeled.add(lab_span[0])\n",
    "        \n",
    "        for med_span in span_labs[1]:\n",
    "            span_set_medcat.add(med_span[0])\n",
    "        \n",
    "        # check overlap\n",
    "        jaccard_indices = []\n",
    "        if reversed:\n",
    "            left_spans = span_set_medcat\n",
    "            right_spans = span_set_labeled          \n",
    "        else:\n",
    "            left_spans = span_set_labeled\n",
    "            right_spans = span_set_medcat\n",
    "        \n",
    "        for spanL in left_spans:\n",
    "            _jaccard_indices = []            \n",
    "            for spanR in right_spans:\n",
    "                _jaccard_indices.append(_tuple_overlap(spanL, spanR))\n",
    "            try:\n",
    "                max_ = max(_jaccard_indices)\n",
    "            except:\n",
    "                max_ = np.nan\n",
    "            jaccard_indices.append(max_)\n",
    "        OverlapList.append(jaccard_indices)\n",
    "    return OverlapList\n",
    "\n",
    "def span_overlap_counter_with_assignment(labeled_spans):\n",
    "    # label_spans: list[[list[((begin,end), label)], list[((begin,end), [labels])]]]\n",
    "    OverlapList = []\n",
    "    for span_labs in labeled_spans:\n",
    "        span_set_medcat = set()\n",
    "        span_dict_labeled = defaultdict(set)\n",
    "        for lab_span in span_labs[0]:\n",
    "            span_dict_labeled[lab_span[1]].add(lab_span[0])\n",
    "        \n",
    "        for med_span in span_labs[1]:\n",
    "            span_set_medcat.add(med_span[0])\n",
    "        \n",
    "        # check overlap\n",
    "        jaccard_indices = defaultdict(list)\n",
    "        for spanClass, spanLabs in span_dict_labeled.items():\n",
    "            for spanLab in spanLabs:                         \n",
    "                _jaccard_indices = []   \n",
    "                for spanMedcat in span_set_medcat:\n",
    "                    _jaccard_indices.append(_tuple_overlap(spanLab, spanMedcat))\n",
    "                try:\n",
    "                    max_ = max(_jaccard_indices)\n",
    "                except:\n",
    "                    max_ = np.nan\n",
    "                jaccard_indices[spanClass].append(max_)\n",
    "        OverlapList.append(jaccard_indices)\n",
    "    return OverlapList\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ignore_list = ['normal', 'lv_syst_func_normal', 'nolabel', 'lv_sys_func_unknown', 'lv_sys_func_unchanged', 'lv_sys_func_improved', 'pe']\n",
    "acceptableLabels = [_label for _label in ClassMap.keys() if _label not in ignore_list]\n",
    "\n",
    "def accepted_labels(labels, accepted_labels):\n",
    "    return any([(_label in accepted_labels) for _label in labels])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ClassMap.pop('normal')\n",
    "ClassMap.pop('pe')\n",
    "ClassMap.pop('lv_sys_func_improved')\n",
    "ClassMap.pop('lv_sys_func_unchanged')\n",
    "ClassMap.pop('lv_syst_func_normal')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# end-to-end performance, requires an implicit \"no label\" label for consistency\n",
    "# assuming one-versus-all for scoring\n",
    "# tp: present and pos pred & pos lab\n",
    "# tn: present and neg pred & neg lab (i.e. not a particular class value)\n",
    "# fp: present and pos pred & neg lab (i.e. not a particular class value)\n",
    "# fn: present and neg pred & neg lab \n",
    "# The only difference with the exact matching is that we add a negative label called \"no label\"\n",
    "\n",
    "'''\n",
    "ConfusionDict = {'Class': {\n",
    "    'ClassValue1': {\n",
    "        'fp': \n",
    "        'fn': \n",
    "        'tp': \n",
    "        'tn': \n",
    "    } \n",
    "}}\n",
    "'''\n",
    "# from true -> predicted \n",
    "##########################\n",
    "min_jacc = 0.5 # This is somewhat arbitrary: caveat it is :)\n",
    "totList = []\n",
    "for comp in span_suggester_comparison_list:\n",
    "    docDict = defaultdict(lambda: defaultdict(lambda : defaultdict(int)))\n",
    "    conc_true_list = defaultdict(list)\n",
    "    for (tTrue_l, tTrue_r), True_class_value in comp[0]:\n",
    "        conc_true_list[(tTrue_l, tTrue_r)].append(True_class_value)\n",
    "        \n",
    "    for tTrue, True_labs in conc_true_list.items():\n",
    "        # check if there is an overlapping span (with some minimum Jaccard)\n",
    "        _classes = [ClassMap.get(True_lab) for True_lab in True_labs] \n",
    "        pred_checked = []  # to avoid double counting false positives\n",
    "        for i, _c in enumerate(_classes):\n",
    "            found = False\n",
    "            if (_c is not None) & (isinstance(_c, str)):        \n",
    "                for tPred, Pred_labs in comp[1]:\n",
    "                    # TODO: check if any of the labels is in accepted list of labels\n",
    "                    if accepted_labels(Pred_labs, acceptableLabels):\n",
    "                        if _tuple_overlap(tTrue, tPred)>min_jacc:                        \n",
    "                            if True_labs[i] in Pred_labs:\n",
    "                                docDict[_c][True_labs[i]]['tp'] += 1\n",
    "                                # fp for all other class values in Pred_labs?\n",
    "                                for m, pred_lab in enumerate(Pred_labs):\n",
    "                                   if m not in pred_checked: \n",
    "                                       if (pred_lab not in True_labs) and (pred_lab not in ['nolabel', \n",
    "                                                                                            'negated', \n",
    "                                                                                            'not negated',\n",
    "                                                                                            'lv_syst_func_normal',\n",
    "                                                                                            'lv_sys_func_improved',\n",
    "                                                                                            'lv_sys_func_unchanged'\n",
    "                                                                                            ]):\n",
    "                                           __c = ClassMap.get(pred_lab)\n",
    "                                           pred_checked.append(m)\n",
    "                                           if (__c is not None) & (isinstance(__c, str)):\n",
    "                                               docDict[__c][pred_lab]['fp'] += 1 \n",
    "                                found = True\n",
    "                            else:\n",
    "                                docDict[_c][True_labs[i]]['fn'] += 1\n",
    "                if not found:\n",
    "                    docDict[_c][True_labs[i]]['fn'] += 1\n",
    "    totList.append(docDict)\n",
    "    \n",
    "# collect per class per class value the amount of tp, fp, fn\n",
    "finalDict = defaultdict(lambda: defaultdict(lambda : defaultdict(int)))\n",
    "for d in totList:\n",
    "    for k,v in d.items():\n",
    "        for _k, _v in v.items():\n",
    "            for __k, __v in _v.items():\n",
    "                finalDict[k][_k][__k] += __v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# span_suggester_comparison_list\n",
    "# make dictionary with the count of predicted spans per class and the number of false prediction per class\n",
    "\n",
    "ClassTpCount = defaultdict(int)\n",
    "ClassPredCount = defaultdict(int)\n",
    "TotSpans = 0\n",
    "min_jacc = 0.25\n",
    "for SpanLists in span_suggester_comparison_list:\n",
    "    # [((l,r), label)]\n",
    "    TrueSpans = SpanLists[0]\n",
    "    # [((l,r), [list of labels])]\n",
    "    PredictedSpans = SpanLists[1]\n",
    "    \n",
    "    for PredictedSpan in PredictedSpans:\n",
    "        TotSpans += 1 \n",
    "        predSpan = PredictedSpan[0]\n",
    "        predLabels = PredictedSpan[1]\n",
    "        for predLabel in predLabels:\n",
    "            if predLabel in acceptableLabels:\n",
    "                ClassPredCount[ClassMap[predLabel]] += 1\n",
    "        \n",
    "        for TrueSpan in TrueSpans:\n",
    "            trueSpan = TrueSpan[0]\n",
    "            trueLabel = TrueSpan[1]\n",
    "            if trueLabel in acceptableLabels:\n",
    "                if _tuple_overlap(trueSpan, predSpan)>min_jacc:\n",
    "                    if trueLabel in predLabels:\n",
    "                        ClassTpCount[ClassMap[trueLabel]] += 1\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ClassFpRate = defaultdict(float)\n",
    "ClassFpRateC = defaultdict(float)\n",
    "for k,v in ClassPredCount.items():\n",
    "    ClassFpRate[k] = round((v-ClassTpCount[k])/v, 2)\n",
    "    ClassFpRateC[k] = round((v-ClassTpCount[k])/TotSpans, 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res_list = []\n",
    "for k,v in ClassFpRate.items():\n",
    "    res_list.append({'class': k, \n",
    "                     'fprate': str(ClassFpRateC[k])+\"(\"+str(v)+\")\"})\n",
    "pd.DataFrame(res_list).sort_values(by='class').set_index('class')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# given the tp, fp, fn the precision, recall, f1 can be calculated\n",
    "eps = 1e-7\n",
    "precision = lambda tp,fp: tp/(tp+fp+eps)\n",
    "recall = lambda tp,fn: tp/(tp+fn+eps)\n",
    "f1 = lambda p,r: 2*p*r/(p+r+eps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scoreDict = defaultdict(lambda: defaultdict(lambda : defaultdict(int)))\n",
    "for k,v in finalDict.items():\n",
    "    for _k, _v in v.items():\n",
    "        fp = _v['fp'] if not isinstance(_v['fp'], dict) else 0\n",
    "        fn = _v['fn'] if not isinstance(_v['fn'], dict) else 0\n",
    "        tp = _v['tp'] if not isinstance(_v['tp'], dict) else 0\n",
    "        totCount = fp + fn + tp        \n",
    "        prec = precision(tp,fp)\n",
    "        rec = recall(tp,fn)\n",
    "        scoreDict[k][_k]['count'] = totCount\n",
    "        scoreDict[k][_k]['precision'] = prec\n",
    "        scoreDict[k][_k]['recall'] = rec\n",
    "        scoreDict[k][_k]['f1'] = f1(prec,rec)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "classScores = defaultdict(lambda: defaultdict(float))\n",
    "for k,v in scoreDict.items():\n",
    "    totCount = 0\n",
    "    precF, precM = 0,0\n",
    "    recF, recM = 0,0\n",
    "    f1F, f1M = 0,0\n",
    "    for i, _v in enumerate(v.values()):\n",
    "        totCount += _v['count']\n",
    "        precF += _v['precision']*_v['count']\n",
    "        recF += _v['recall']*_v['count']\n",
    "        f1F += _v['f1']*_v['count']\n",
    "        \n",
    "        precM += _v['precision']\n",
    "        recM += _v['recall']\n",
    "        f1M += _v['f1']\n",
    "        \n",
    "    classScores[k]['precision'] = str(round(precF/totCount,2)) + \"(\"+str(round(precM/(i+1),2))+\")\"\n",
    "    classScores[k]['recall'] = str(round(recF/totCount,2)) + \"(\"+str(round(recM/(i+1),2))+\")\"\n",
    "    classScores[k]['f1'] = str(round(f1F/totCount,2)) + \"(\"+str(round(f1M/(i+1),2))+\")\"\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pd.DataFrame(classScores).round(2).T.sort_index(ascending=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "OverlapJaccardIndices = span_overlap_counter(span_suggester_comparison_list, reversed=False)\n",
    "OverlapJaccardIndicesReversed = span_overlap_counter(span_suggester_comparison_list, reversed=True)\n",
    "OverlapJaccardClassIndices = span_overlap_counter_with_assignment(span_suggester_comparison_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "PerClassValueJaccard = defaultdict(list)\n",
    "for d in OverlapJaccardClassIndices:\n",
    "    for k,v in d.items():\n",
    "        PerClassValueJaccard[k].extend(v)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "PerClassJaccard = defaultdict(list)\n",
    "failed_cvs = set()\n",
    "for cv, v in PerClassValueJaccard.items():\n",
    "    try:\n",
    "        PerClassJaccard[ClassMap[cv]].extend(v)\n",
    "    except:\n",
    "        failed_cvs.add(cv)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MeanPerDocument = [np.nanmean(v) for v in OverlapJaccardIndices]\n",
    "MeanOverall = np.nanmean([_v for v in OverlapJaccardIndices for _v in v])\n",
    "\n",
    "MeanPerDocumentRev = [np.nanmean(v) for v in OverlapJaccardIndicesReversed]\n",
    "MeanOverallRev = np.nanmean([_v for v in OverlapJaccardIndicesReversed for _v in v])\n",
    "\n",
    "\n",
    "print(f\"Mean document coverage: {np.mean(MeanPerDocument)}, with {MeanOverall} over all tokens\")\n",
    "\n",
    "plt.hist(MeanPerDocument, bins=50, color='green', histtype='step', lw=2);\n",
    "plt.hist(MeanPerDocumentRev, bins=50, color='red', histtype='step', lw=2);\n",
    "\n",
    "plt.title(\"Span overlap per document\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "[{'class': k, 'mean': np.nanmean(v)} for k,v in PerClassJaccard.items()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MinMean = 0.25\n",
    "CheckRes = [{'Jaccard': v, \n",
    "             'res': comparison_list_with_annotations_present[i]} \n",
    "            for i,v in enumerate(OverlapJaccardIndices)\n",
    "            if (np.mean(v)<MinMean) | (np.isnan(np.mean(v)))]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"We have {len(CheckRes)} documents to check\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CheckRes[0]['res']['predicted']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Document classification from span classification"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AllowedClasses = set(ClassMap.values())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SeverityMap = {\n",
    "    '_mild':2,\n",
    "    '_moderate':3,\n",
    "    '_severe':4,\n",
    "    '_not_present': 0,\n",
    "    '_unknown': 1,\n",
    "    '_normal': 0,\n",
    "    '_present': 1,\n",
    "    'nolabel': -1\n",
    "}\n",
    "SeverityMapFull = {}\n",
    "for lab in acceptableLabels:\n",
    "    for k, score in SeverityMap.items():\n",
    "        if k in lab:\n",
    "            SeverityMapFull[lab] = score\n",
    "            break\n",
    "SeverityMapFull['nolabel'] = -1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trueClass = []\n",
    "predClass = []\n",
    "# [{'input_hash': xx, 'aortic_regurgitation': f_worst(aortic_regurgitation_labels}...]\n",
    "\n",
    "def get_worst_labels(LabelDict: dict=None, ScoreMap: dict=None):\n",
    "    # per class get worst labels, if no labels present for class return 'nolabel' for this class\n",
    "    OutDict = {_class: \"nolabel\" for _class in AllowedClasses}\n",
    "    \n",
    "    for k,v in LabelDict.items():\n",
    "        worst=-1\n",
    "        for label in v:\n",
    "            score = ScoreMap[label]\n",
    "            if score>worst:\n",
    "                worst = score\n",
    "                OutDict[k] = label\n",
    "    return OutDict\n",
    "    \n",
    "for d in comparison_list:\n",
    "    r = {}\n",
    "    p = {}\n",
    "    ld = defaultdict(set)\n",
    "    r['input_hash'] = d['input_hash']\n",
    "    p['input_hash'] = d['input_hash']\n",
    "    \n",
    "    for ann in d['annotations']:\n",
    "        ClassName = ann['meta_anns']['merged']['name']\n",
    "        LabelName = ann['meta_anns']['merged']['value']\n",
    "        if isinstance(ClassName, str):\n",
    "            if (ClassName in AllowedClasses) and (LabelName in acceptableLabels):\n",
    "                ld[ClassName].add(LabelName)\n",
    "        else:\n",
    "            print(f\"True ClassName is wrong type {ClassName}\")\n",
    "    lw = get_worst_labels(ld, SeverityMapFull)  \n",
    "    r.update(lw)\n",
    "    trueClass.append(r)\n",
    "    \n",
    "    rd = defaultdict(set)\n",
    "    for _, predDict in d['predicted'].items():\n",
    "        for _class, pred in predDict.items():\n",
    "            if (_class in AllowedClasses) and (pred in acceptableLabels):\n",
    "                rd[_class].add(pred)\n",
    "    pw = get_worst_labels(rd, SeverityMapFull)\n",
    "    p.update(pw)\n",
    "    predClass.append(p)\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predClassDF = pd.DataFrame(predClass).set_index('input_hash')\n",
    "trueClassDF = pd.DataFrame(trueClass).set_index('input_hash')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_dict = {\n",
    "    c: predClassDF[[c]].join(trueClassDF[[c]],lsuffix='_pred', rsuffix='_true')\n",
    "    for c in predClassDF.columns\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                             confusion_matrix, roc_auc_score, multilabel_confusion_matrix)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_dict.keys()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res_list = []\n",
    "for c in df_dict.keys():\n",
    "    ypred = df_dict[c].iloc[:,0]\n",
    "    ytrue = df_dict[c].iloc[:,1]\n",
    "    f1_macro = f1_score(ytrue,ypred, average='macro')\n",
    "    f1_weighted = f1_score(ytrue,ypred, average='weighted')\n",
    "    recall_macro = recall_score(ytrue,ypred, average='macro')\n",
    "    recall_weighted = recall_score(ytrue,ypred, average='weighted')\n",
    "    precision_macro = precision_score(ytrue,ypred, average='macro')\n",
    "    precision_weighted = precision_score(ytrue,ypred, average='weighted')\n",
    "    \n",
    "    res_list.append({\n",
    "        'class': c,\n",
    "        'f1': str(round(f1_weighted,2)) + \"(\" + str(round(f1_macro,2)) + \")\",\n",
    "        'recall': str(round(recall_weighted,2)) + \"(\" + str(round(recall_macro,2)) + \")\",\n",
    "        'precision': str(round(precision_weighted,2)) + \"(\" + str(round(precision_macro,2)) + \")\",\n",
    "    })"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.DataFrame(res_list).sort_values(by='class')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nlp_310)",
   "language": "python",
   "name": "python3_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a14be5a308536305ca6f1be76534a0246168409855a6497a1510fcfd428ed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
