{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import deduce\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import phrases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context:\n",
    "* $100$ K echocardiographic reports available. \n",
    "* we want to extract diagnoses regarding the left-ventricle function\n",
    "* we have $5000$ reports with labeled spans.\n",
    "\n",
    "# Goal:\n",
    "Train a \"model\" that can\n",
    "1. identify the spans\n",
    "2. classify the spans\n",
    "\n",
    "# Approach: MedCAT - MetaCAT\n",
    "\n",
    "## Two-step approach\n",
    "\n",
    "* unsupervised training on the documents\n",
    "* add a single custom entity with a custom identifier\n",
    "* train a model to identify the custom entities\n",
    "* supervised training on the meta-annotations of the entities\n",
    "\n",
    "## One-step approach\n",
    "\n",
    "* unsupervised training on the documents\n",
    "* add custom entities based on the spans and their labels\n",
    "* train a model to identify the custom entities\n",
    "\n",
    "# Approach: biLSTM/transformer\n",
    "\n",
    "## Two-step approach\n",
    "\n",
    "* Train a model to identify the spans: self-supervision by random selecting non-span ranges as negative examples\n",
    "* Train a model to classify the spans: supervised based on the labeled spans \n",
    "* Combine the model in one pipeline\n",
    "\n",
    "## One-step approach\n",
    "* Assign a label to each span\n",
    "* Train a model to identify the spans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load Medcat modelpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "# extract the environment variable 'medcat_pack'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcat.cat import CAT\n",
    "from medcat.vocab import Vocab\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config import Config\n",
    "from medcat.meta_cat import MetaCAT\n",
    "\n",
    "base_medcat_path = os.getenv('medcat_pack')\n",
    "pack_location = 'umls-dutch-v1-10_echo'\n",
    "prep_medcat = False\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo_path = 'T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData'\n",
    "# load the jsonl in a dataframe\n",
    "texts = pd.read_json(os.path.join(echo_path, 'outdb_140423.jsonl'), lines=True)\n",
    "#texts_zipped = zip(texts['text'], texts['_input_hash'], texts['_task_hash'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indcs = pd.read_csv(os.path.join(echo_path, 'train_echoid.csv'), sep=',')\n",
    "test_indcs = pd.read_csv(os.path.join(echo_path, 'test_echoid.csv'), sep=',')\n",
    "\n",
    "train_indcs = train_indcs[train_indcs.input_hash.notna()]\n",
    "test_indcs = test_indcs[test_indcs.input_hash.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUT_HASH = set(train_indcs['input_hash'].astype(int).to_list())\n",
    "TRAIN_TASK_HASH = set(train_indcs['task_hash'].astype(int).to_list())\n",
    "\n",
    "TEST_INPUT_HASH = set(test_indcs['input_hash'].astype(int).to_list())\n",
    "TEST_TASK_HASH = set(test_indcs['task_hash'].astype(int).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = texts.loc[texts._input_hash.isin(TRAIN_INPUT_HASH)]\n",
    "texts_test = texts.loc[texts._input_hash.isin(TEST_INPUT_HASH)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make splitted data\n",
    "\n",
    "* MedCAT train files\n",
    "* class-dictionary with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_medcat_json(filename: str=None, HashSet: set=False, ClassName: str=None):\n",
    "    texts = pd.read_json(filename, lines=True)\n",
    "    output = {\"projects\": [{\n",
    "                \"name\": ClassName,\n",
    "                \"id\": 42,\n",
    "                \"cuis\": \"\",\n",
    "                \"tuis\": \"\",\n",
    "                \"documents\": None\n",
    "            }]}\n",
    "    \n",
    "    documents = []\n",
    "    for i, _row in enumerate(texts.iterrows()):\n",
    "        row = _row[1]\n",
    "        if (row[\"_input_hash\"] in HashSet) | (HashSet is False):\n",
    "            txt = row['text']\n",
    "            id = i\n",
    "            input_hash = row[\"_input_hash\"]\n",
    "            task_hash = row[\"_task_hash\"]\n",
    "            annotations = []\n",
    "            for j, ann in enumerate(row[\"spans\"]):\n",
    "                res = {}\n",
    "                res['user'] = 'BVE'\n",
    "                res['cui'] = 123\n",
    "                res['id'] = j\n",
    "                res['start'] = ann['start']\n",
    "                res['end'] = ann['end']\n",
    "                res['value'] = txt[ann['start']:ann['end']]\n",
    "                res['validated'] = True\n",
    "                res['correct'] = True\n",
    "                res['deleted'] = False\n",
    "                res['alternative'] = False\n",
    "                res['killed'] = False\n",
    "                res[\"meta_anns\"] = {\n",
    "                    ClassName: {\n",
    "                        \"name\": ClassName,\n",
    "                        \"validated\": True,\n",
    "                        \"accuracy\": 1.0,\n",
    "                        \"value\": ann['label']\n",
    "                    }\n",
    "                }\n",
    "                annotations.append(res)\n",
    "            doc = {\n",
    "                'id': id,\n",
    "                'text': txt,\n",
    "                'input_hash': input_hash,\n",
    "                'task_hash': task_hash,\n",
    "                'annotations': annotations         \n",
    "            }\n",
    "            documents.append(doc)\n",
    "    output['projects'][0]['documents'] = documents\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = os.listdir(os.path.join(echo_path, 'echo_span_labels'))\n",
    "merged_index = [i for i,t in enumerate(class_names) \n",
    "                    if ('merged' in t) | ('old' in t)]\n",
    "merged_index = sorted(merged_index, reverse=True)\n",
    "\n",
    "for mind in merged_index:\n",
    "    class_names.pop(mind)\n",
    "class_ds = defaultdict(dict)\n",
    "\n",
    "for file_name in class_names:\n",
    "    _class = file_name.split(\".\")[0]\n",
    "    fn = os.path.join(echo_path, 'echo_span_labels', file_name)\n",
    "    class_ds[_class]['ds'] = pd.read_json(fn, lines=True)\n",
    "    \n",
    "    #TODO: has to be refactored\n",
    "    medcat_json = get_medcat_json(fn, TRAIN_INPUT_HASH, _class)\n",
    "    medcat_out = os.path.join(echo_path, 'medcat_labels', 'train', f'medcat_{_class}.json')\n",
    "    json.dump(medcat_json, open(medcat_out, 'w'))\n",
    "    class_ds[_class]['train_location'] = medcat_out\n",
    "    \n",
    "    medcat_json = get_medcat_json(fn, TEST_INPUT_HASH, _class)\n",
    "    medcat_out = os.path.join(echo_path, 'medcat_labels', 'test', f'medcat_{_class}.json')\n",
    "    json.dump(medcat_json, open(medcat_out, 'w'))\n",
    "    class_ds[_class]['test_location'] = medcat_out\n",
    "    \n",
    "    class_ds[_class]['labels'] = set([span_dict['label'] \n",
    "                                        for span_list in class_ds[_class]['ds']['spans'].tolist()\n",
    "                                        for span_dict in span_list])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning for NER+L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCAT = CAT.load_model_pack(os.path.join(base_medcat_path, pack_location))\n",
    "\n",
    "MCAT.train(texts_train.text.values, \n",
    "            nepochs=3, \n",
    "            progress_print=10,  \n",
    "            is_resumed=True)\n",
    "MCAT.create_model_pack(base_medcat_path + \"/umls-dutch-v1-10_echo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add label spans from Prodigy annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_sets = defaultdict(set)\n",
    "for _class, dsd in tqdm(class_ds.items()):\n",
    "        ds = dsd['ds']\n",
    "        span_set = set()\n",
    "        span_list = []\n",
    "        ds = ds[ds._input_hash.isin(TRAIN_INPUT_HASH)]\n",
    "        for k, (_spans, text) in enumerate(zip(ds[ds.spans.notna()].spans.values,\n",
    "                                            ds[ds.spans.notna()].text.values)):\n",
    "            for _span in _spans:\n",
    "                start, end = _span['start'], _span['end']\n",
    "                span_set.add(text[start:end])\n",
    "                span_list.append(text[start:end])\n",
    "        span_sets[_class] = span_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: interesting to add variants without the abbreviations?\n",
    "# TODO: including paraphrasing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _class, span_set in tqdm(span_sets.items()):\n",
    "    for _span in span_set:\n",
    "        MCAT.add_and_train_concept(cui=_class,\n",
    "                                name=_span, \n",
    "                                do_add_concept=True,\n",
    "                                negative=False,\n",
    "                            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning for NER+L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCAT.train_supervised(data_path=os.path.join(base_medcat_path, \n",
    "                                 \"umls-dutch-v1-10_echo\",\n",
    "                                 \"input/ner_l_anno/trainer_export.json\"), \n",
    "                      nepochs=4,\n",
    "                      print_stats=0,\n",
    "                      use_filters=False)\n",
    "medcat_path = base_medcat_path + \"/umls-dutch-v1-10_echoV2\"\n",
    "MCAT.create_model_pack(medcat_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning of MetaCAT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MCAT from file\n",
    "medcat_path = os.path.join(base_medcat_path, 'umls-dutch-v1-10_echoV2')\n",
    "MCAT = CAT.load_model_pack(medcat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcat.meta_cat import MetaCAT\n",
    "from medcat.config_meta_cat import ConfigMetaCAT\n",
    "from medcat.tokenizers.meta_cat_tokenizers import TokenizerWrapperBPE, ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer from negation_model\n",
    "# tokenizer folder \n",
    "tok_folder = 'T:/laupodteam/AIOS/Bram/language_modeling/Clinical_embeddings/unigrams/with_tokenizer/v2/tokenizer'\n",
    "emb_folder = 'T:/laupodteam/AIOS/Bram/language_modeling/Clinical_embeddings/unigrams/with_tokenizer/v2/SG'\n",
    "tokenizer = ByteLevelBPETokenizer.from_file(os.path.join(tok_folder, 'vocab.json'), \n",
    "                                            os.path.join(tok_folder, 'merges.txt'))\n",
    "wrapped_tokenizer = TokenizerWrapperBPE(hf_tokenizers=tokenizer)\n",
    "wrapped_tokenizer.save(medcat_path + \"/assets/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "vec_path = os.path.join(emb_folder, 'sg')\n",
    "print(vec_path)\n",
    "w2v = KeyedVectors.load(vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "embeddings = []\n",
    "words_not_present = []\n",
    "\n",
    "for i in range(tokenizer.get_vocab_size()):\n",
    "    word = tokenizer.id_to_token(i)\n",
    "    if word in w2v:\n",
    "        embeddings.append(w2v[word])\n",
    "    else:\n",
    "        words_not_present.append(i)\n",
    "        embeddings.append(np.random.random(300))\n",
    "        \n",
    "mean_vector = np.mean(embeddings, axis=0)\n",
    "\n",
    "for i in words_not_present:\n",
    "    embeddings[i] = mean_vector\n",
    "\n",
    "# Save the embeddings\n",
    "embeddings_array = np.array(embeddings)\n",
    "np.save(open(medcat_path+\"/assets/embeddings/embedding.npy\", \n",
    "             'wb'), embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _class, d in class_ds.items():\n",
    "    if _class != 'normal':\n",
    "        print(f\"Commencing training of biLSTM-span for {_class}...\")\n",
    "        config_metacat = ConfigMetaCAT()\n",
    "        config_metacat.general['category_name'] = _class\n",
    "        config_metacat.train['nepochs'] = 25\n",
    "        config_metacat.train['score_average'] = 'weighted'\n",
    "        config_metacat.model['hidden_size'] = 256\n",
    "        config_metacat.model['input_size'] = 300\n",
    "        config_metacat.model['dropout'] = 0.3\n",
    "        config_metacat.model['num_layers'] = 3\n",
    "        config_metacat.model['num_directions'] = 2\n",
    "        config_metacat.model['nclasses'] = len(d['labels'])\n",
    "        config_metacat.model['model_name'] = 'lstm'\n",
    "        \n",
    "        meta_cat = MetaCAT(tokenizer=wrapped_tokenizer,\n",
    "                    embeddings=embeddings_array, \n",
    "                    config=config_metacat)\n",
    "        \n",
    "        train_path = d['train_location']\n",
    "        model_path = os.path.join(medcat_path, f\"meta_{_class}\")\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        \n",
    "        meta_cat.train(json_path=train_path, \n",
    "                        save_dir_path=model_path)\n",
    "        \n",
    "        meta_cat.save(save_dir_path=model_path)\n",
    "        # now manually add the model to the model_pack...\n",
    "        label_dict = config_metacat.general.category_value2id\n",
    "        \n",
    "        # add to config\n",
    "        medcat_config = json.load(open(os.path.join(medcat_path, 'model_card.json'), 'r'))\n",
    "        \n",
    "        # add to \"MetaCAT models\" list\n",
    "        medcat_config[\"MetaCAT models\"].append({\n",
    "                      \"Category Name\": _class,\n",
    "                      \"Description\": \"No description\",\n",
    "                      \"Classes\": label_dict,\n",
    "                        \"Model\": \"lstm\"\n",
    "                    })\n",
    "        # write config to .json\n",
    "        json.dump(medcat_config, open(os.path.join(medcat_path, 'model_card.json'), 'w'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  new model pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCATnew = CAT.load_model_pack(os.path.join(medcat_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 123\n",
    "doc = MCATnew(texts.text.values[i])\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_with_conf(ent):\n",
    "    result_dict = {}\n",
    "    for k, v in ent._.meta_anns.items():\n",
    "        result_dict[k] = v['value']\n",
    "        result_dict[f'conf_{k}'] = v['confidence']\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = MCATnew(texts.text.values[i])\n",
    "inds = []\n",
    "res = []\n",
    "for ent in doc.ents:\n",
    "    inds.append(ent.text)\n",
    "    res.append(create_dict_with_conf(ent))\n",
    "    \n",
    "res_df = pd.DataFrame(res, index=inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RESULTS = {}\n",
    "for k in class_ds.keys():\n",
    "    if k != 'normal':\n",
    "        CATmodel = MetaCAT.load(os.path.join(medcat_path, f'meta_{k}'))\n",
    "        test_location = class_ds[k]['test_location']\n",
    "        test_labels = json.load(open(test_location, 'r'))\n",
    "        bulk_res = CATmodel.eval(test_location)\n",
    "        \n",
    "        res_count = defaultdict(int)\n",
    "        for doc in test_labels['projects'][0]['documents']:\n",
    "            anns = doc['annotations']\n",
    "            if len(anns)>0:\n",
    "                for ann in anns:            \n",
    "                    _ann = ann['meta_anns'][k]\n",
    "                    res_count[_ann['value']] += 1\n",
    "\n",
    "        TEST_RESULTS[k] = {\n",
    "            'f1':  bulk_res['f1'],\n",
    "            'precision': bulk_res['precision'],\n",
    "            'recall': bulk_res['recall'],\n",
    "            'confusion_df': bulk_res['confusion matrix'],\n",
    "            'real_presence': res_count\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment of performance in the wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "medcat_doc = test_labels['projects'][0]['documents'][0]\n",
    "txt = medcat_doc['text']\n",
    "parsed_doc = MCATnew(txt)\n",
    "\n",
    "inds = []\n",
    "res = []\n",
    "for ent in parsed_doc.ents:\n",
    "    inds.append(ent.text)\n",
    "    res.append(create_dict_with_conf(ent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nlp_310)",
   "language": "python",
   "name": "python3_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a14be5a308536305ca6f1be76534a0246168409855a6497a1510fcfd428ed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
