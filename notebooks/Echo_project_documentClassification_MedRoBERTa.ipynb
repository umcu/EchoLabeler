{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# All classes\n",
    "MedRoBERTa.nl – {“no label”: no label for all classes, \n",
    "                 “Normal”: normal for all classes,\n",
    "                 “Present”: any non-normal present}\n",
    " \n",
    "\n",
    "MedRoBERTa.nl – {“no label”: no label for all classes, \n",
    "                 “Normal”: normal for all classes,\n",
    "                 “aortic_regurgitation_present”: ..,\n",
    "                 “aortic_stenosis_present”: ..\n",
    "                  …\n",
    "                }\n",
    " \n",
    "# Per class, i.e. 11 models\n",
    "MedRoBERTa.nl – {“no label”: ..\n",
    "                 “mild”: ..\n",
    "                 “moderate”: ..\n",
    "                 “severe”: ..\n",
    "                “present”: ..}\n",
    " \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-19T10:43:40.377542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from torchinfo import summary\n",
    "\n",
    "from typing import List, Tuple, Dict, Union"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "from torchinfo import summary\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "import benedict\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated()}\")\n",
    "    print(f\"Max memory Allocated: {torch.cuda.max_memory_allocated()}\") \n",
    "    print(f\"Memory reserved: {torch.cuda.memory_reserved()}\")\n",
    "    print(f\"Max memory reserved: {torch.cuda.max_memory_reserved()}\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "deabbreviate = False\n",
    "filter_reports = True\n",
    "Classes = ['mitral_regurgitation','tricuspid_regurgitation', 'wma'] # 'aortic_regurgitation'\n",
    "FLAG_TERMS = ['uitslag zie medische status', 'zie status', 'zie verslag status', 'slecht echovenster', 'echo overwegen', 'ge echo',\n",
    "              'geen echovenster', 'geen beoordeelbaar echo', 'tee 190', 'hdf 36mnd', 'geen beoordeelbare echo', 'verslag op ic', 'psyheartanalyse']\n",
    "SAVE_TERMS = ['goed', 'geen', 'normaal', 'normale']\n",
    "use_multilabel = False\n",
    "reduce_labels = True\n",
    "num_epochs = 5\n",
    "\n",
    "MULTILABELS = {'Mild': ['Mild', 'Present'], \n",
    "               'Severe': ['Severe', 'Present'],\n",
    "               'Moderate': ['Moderate', 'Present'],\n",
    "               'Normal': ['Normal'],\n",
    "               'No label': ['No label'],\n",
    "               'Present': ['Present'],\n",
    "               }\n",
    "Target_remapper = {'No label': 'No label', \n",
    "                   'Normal': 'Normal',\n",
    "                   'Mild': 'Not normal',\n",
    "                   'Moderate': 'Not normal',\n",
    "                   'Severe': 'Not normal',\n",
    "                   'Present': 'Not normal'\n",
    "                   }\n",
    "#Target_remapper = {'No label': 'Normal', \n",
    "#                   'Normal': 'Normal',\n",
    "#                   'Mild': 'Not normal',\n",
    "#                   'Moderate': 'Not normal',\n",
    "#                   'Severe': 'Not normal',\n",
    "#                   'Present':  'Not normal'\n",
    "#                   }\n",
    "\n",
    "# train a MedRoBERTa.nl model that trains on\n",
    "# No label: whether there is \"no label\" for all classes\n",
    "# Normal: whether all the classes are normal\n",
    "# Present: \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "#Add the src folder to the path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import deabber, echo_utils"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "if deabbreviate:\n",
    "    ABBREVIATIONS = benedict.benedict(\"../assets/abbreviations.yml\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "plt.style.use('ggplot')\n",
    "def plot_history(history, val=0):\n",
    "    acc = history.history['accuracy']\n",
    "    if val == 1:\n",
    "        val_acc = history.history['val_accuracy'] # we can add a validation set in our fit function with nn\n",
    "    loss = history.history['loss']\n",
    "    if val == 1:\n",
    "        val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training accuracy')\n",
    "    if val == 1:\n",
    "        plt.plot(x, val_acc, 'r', label='Validation accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    if val == 1:\n",
    "        plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def compute_metrics_binomial(logits_and_labels, averaging='macro'):\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  acc = np.mean(predictions == labels)\n",
    "  f1 = f1_score(labels, predictions, average = averaging)\n",
    "  prec = precision_score(labels, predictions, average = averaging)\n",
    "  rec = recall_score(labels, predictions, average = averaging)\n",
    "  return {\n",
    "          'accuracy': acc, \n",
    "          'f1_score': f1,\n",
    "          'precision': prec,\n",
    "          'recal': rec\n",
    "          }\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    \n",
    "    \n",
    "    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    prec_macro = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    prec_weighted = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_macro = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')    \n",
    "    \n",
    "    try:\n",
    "        roc_auc_weighted = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "        roc_auc_macro = roc_auc_score(y_true, probs, average = 'macro')\n",
    "    except:\n",
    "        roc_auc_weighted = None\n",
    "        roc_auc_macro = None\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_macro': f1_macro,\n",
    "               'f1_weighted': f1_weighted,\n",
    "               'prec_macro': prec_macro,\n",
    "               'prec_weighted': prec_weighted,\n",
    "               'recall_macro': recall_macro,\n",
    "               'recall_weighted': recall_weighted,\n",
    "               'roc_auc_macro': roc_auc_macro,\n",
    "               'roc_auc_weighted': roc_auc_weighted,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "     \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # labels should be a list or a 1D tensor\n",
    "    one_hot = torch.zeros((len(labels), num_classes))\n",
    "    rows = torch.arange(len(labels))\n",
    "    one_hot[rows, labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "def _mlabel_tuple_creator(x: List[int],\n",
    "                          multilabels:Dict[int,List[int]],\n",
    "                          num_classes: int=None)\\\n",
    "                          ->List[Tuple[int,...]]:\n",
    "                              \n",
    "    res = [(_sc for _sc in multilabels[sc]) for sc in x]\n",
    "    return res\n",
    "\n",
    "def multi_hot_encoding(x: List[int], \n",
    "                       multilabels: Union[Dict[int,List[int]], None]=None,\n",
    "                       num_classes: int=None)\\\n",
    "                           ->torch.Tensor:    \n",
    "    if multilabels is None:\n",
    "        return one_hot_encode(x, num_classes=num_classes)\n",
    "    else:\n",
    "        return torch.Tensor(MultiLabelBinarizer(classes=range(num_classes))\\\n",
    "                    .fit_transform(_mlabel_tuple_creator(x,multilabels)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"CLTL/MedRoBERTa.nl\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "os.chdir('T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "for Class in Classes:\n",
    "    labeled_documents = pd.read_json(f\"./echo_doc_labels/{Class}.jsonl\", lines=True)\n",
    "    print(f\"Label counts: {labeled_documents.label.value_counts()}\")\n",
    "    if reduce_labels:\n",
    "        labeled_documents['label'] = labeled_documents.label.map(Target_remapper)\n",
    "        print(f\"Remapped labels: {labeled_documents.label.value_counts(dropna=False)}\")\n",
    "    # Expand with label columns\n",
    "    Target_maps = {Label:i for i,Label in enumerate(labeled_documents['label'].unique())}\n",
    "    \n",
    "    # Load the train/test hashes\n",
    "    test_hashes = pd.read_csv('./test_echoid.csv', sep=',')\n",
    "    train_hashes = pd.read_csv('./train_echoid.csv', sep=',')\n",
    "    print(train_hashes.columns)\n",
    "    \n",
    "    print(f\"Train hashes: {train_hashes.input_hash.nunique()}\")\n",
    "    print(f\"Test hashes: {test_hashes.input_hash.nunique()}\")\n",
    "    print(Target_maps.keys())\n",
    "    \n",
    "    # We now make DataSets (a special HuggingFace structure)\n",
    "    # assuming cross-validation\n",
    "    \n",
    "    DF = labeled_documents\n",
    "    DF.columns = ['sentence', 'labels', '_input_hash']\n",
    "    \n",
    "    label2id = Target_maps\n",
    "    id2label = {v:k for k,v in label2id.items()}\n",
    "    num_labels = len(label2id)\n",
    "    DF['labels'] = DF['labels'].map(label2id)\n",
    "    \n",
    "    \n",
    "    if filter_reports:\n",
    "        DF = DF.assign(sentence = echo_utils.report_filter(DF.sentence, \n",
    "                                                flag_terms=FLAG_TERMS, \n",
    "                                                save_terms=SAVE_TERMS)[0])\n",
    "        DF = DF.loc[DF.sentence.notna()]\n",
    "    \n",
    "    if deabbreviate:\n",
    "        DeAbber = deabber.deabber(model_type='sbert', \n",
    "                                  abbreviations=ABBREVIATIONS['nl']['echocardiogram'], \n",
    "                                  min_sim=0.5, top_k=10)\n",
    "        DF = DF.assign(sentence=DeAbber.deabb(DF.sentence.values, TokenRadius=3))\n",
    "        \n",
    "    if use_multilabel:\n",
    "        _multilabels = {label2id[k]: [label2id[l] for l in v]\n",
    "                        for k,v in MULTILABELS.items()}\n",
    "    else:\n",
    "        _multilabels = None\n",
    "        \n",
    "    \n",
    "    # TODO: make proper\n",
    "    DFtrain = DF.loc[DF._input_hash.isin(train_hashes.input_hash), ['sentence', 'labels']]\n",
    "    DFtest = DF.loc[DF._input_hash.isin(test_hashes.input_hash), ['sentence', 'labels']]\n",
    "    \n",
    "    print(\"Train labels:\")\n",
    "    print(DFtrain.labels.value_counts())\n",
    "    print(\"Test labels:\")\n",
    "    print(DFtest.labels.value_counts())\n",
    "    \n",
    "    TrainSet = Dataset.from_pandas(DFtrain)\n",
    "    TestSet = Dataset.from_pandas(DFtest)\n",
    "    \n",
    "    HF_DataSet = DatasetDict(\n",
    "        {'train' : TrainSet,\n",
    "         'test': TestSet,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    Tokenized_DataSet = HF_DataSet.map(lambda batch: tokenizer(batch, truncation=True, \n",
    "                                                                      padding=True, \n",
    "                                                                      max_length=256),\n",
    "                                      input_columns='sentence',\n",
    "                                      batched=True,\n",
    "                                      remove_columns=['sentence'])\n",
    "    \n",
    "    Tokenized_DataSet = (Tokenized_DataSet\n",
    "                          #.map(lambda x : {\"float_labels\": x[\"labels\"].to(torch.float)}, remove_columns=[\"labels\"])\n",
    "                          .map(lambda x: {\"labels\": \n",
    "                              multi_hot_encoding(x['labels'], \n",
    "                                                 multilabels=_multilabels, \n",
    "                                                 num_classes=num_labels)}, \n",
    "                               batched=True, remove_columns=['labels']))                      \n",
    "                          #.rename_column(\"float_labels\", \"labels\"))\n",
    "    \n",
    "    Tokenized_DataSet.set_format(\"torch\", \n",
    "                                 columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # https://colab.research.google.com/drive/1aue7x525rKy6yYLqqt-5Ll96qjQvpqS7#scrollTo=1eVCRpcLUW-y\n",
    "    # https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\n",
    "    medroberta_clf = AutoModelForSequenceClassification.from_pretrained(\"CLTL/MedRoBERTa.nl\", \n",
    "                                                    num_labels=len(id2label.keys()),\n",
    "                                                    problem_type='multi_label_classification',\n",
    "                                                    id2label=id2label,\n",
    "                                                    label2id=label2id)\n",
    "    \n",
    "    train_dir = \"T:\\\\laupodteam/AIOS/Bram/data/tmp\"\n",
    "    metric_name = 'f1_macro'\n",
    "    training_args = TrainingArguments(output_dir=train_dir,\n",
    "                                      evaluation_strategy='epoch',\n",
    "                                      save_strategy='epoch',\n",
    "                                      num_train_epochs=num_epochs,\n",
    "                                      learning_rate=5e-5,\n",
    "                                      per_device_train_batch_size=16,\n",
    "                                      weight_decay=0.01,\n",
    "                                      per_device_eval_batch_size=10,\n",
    "                                      load_best_model_at_end=True, \n",
    "                                      metric_for_best_model=metric_name)\n",
    "    \n",
    "    trainer = Trainer(medroberta_clf,\n",
    "                      training_args,\n",
    "                      train_dataset = Tokenized_DataSet[\"train\"],\n",
    "                      eval_dataset = Tokenized_DataSet[\"test\"],\n",
    "                      tokenizer=tokenizer,\n",
    "                      data_collator=data_collator,\n",
    "                      compute_metrics=compute_metrics)\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.eval_dataset = Tokenized_DataSet[\"test\"]\n",
    "    _metrics = trainer.evaluate()\n",
    "    \n",
    "    \n",
    "    reduced_label_str = \"reduced_labels\" if reduce_labels else \"full_labels\"\n",
    "    mod_string = f\"{Class}_Epochs{num_epochs}_{reduced_label_str}\"\n",
    "    new_dir = f\"T://lab_research/RES-Folder-UPOD/Echo_label/G_Output/3_OtherOutput/MedRoBERTa/{mod_string}\"\n",
    "    os.mkdir(new_dir)\n",
    "    trainer.save_model(new_dir)\n",
    "    trainer.save_metrics(new_dir, metrics=_metrics)\n",
    "    \n",
    "    # clear GPU memory \n",
    "    del trainer, medroberta_clf    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "summary(medroberta_clf)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "# consider quantisation using bitsandbytes or quanto",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
