{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:22.770632Z",
     "start_time": "2024-05-13T15:16:21.147129Z"
    }
   },
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import benedict"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For multilabel the BCE loss collapses to a positive label for all classes. A better way to start is to use categorical cross-entropy initially.",
   "id": "685a24a9f0e29e94"
  },
  {
   "cell_type": "code",
   "id": "1dd8f28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:59:53.775586Z",
     "start_time": "2024-05-13T15:59:53.063584Z"
    }
   },
   "source": [
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers, models, utils as keras_utils\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "\n",
    "from typing import Callable, Tuple, Dict, List, Literal, Union"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "1461462f259a5bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:34.742103Z",
     "start_time": "2024-05-13T15:16:34.419570Z"
    }
   },
   "source": [
    "# TODO: add pre-trained word embeddings\n",
    "# Generic Spacy\n",
    "# Medical Dutch"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4efa158612b8bcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:35.083177Z",
     "start_time": "2024-05-13T15:16:34.745072Z"
    }
   },
   "source": [
    "#  settings\n",
    "embedding_dim = 300\n",
    "max_len = 200\n",
    "num_words_in_vocab = 5_000\n",
    "num_epochs = 5\n",
    "warmup_steps = 40 # only relevant for multilabel problems\n",
    "warmup_lr = 2e-3 # only relevant for multilabel problems\n",
    "\n",
    "num_folds = 10\n",
    "# multilabel: lv_dil, pe, rv_dil\n",
    "# categorical: aortic_regurgitation, lv_syst_func, rv_syst_func, aortic_stenosis, diastolic_dysfunction, mitral_regurgitation, tricuspid_regurgitation,wma\n",
    "\n",
    "Class = 'rv_dil' \n",
    "batch_size = 128\n",
    "ModelType = 'textcnn' # bigru, bilstm, cnn, hstacked_dcnn, textcnn\n",
    "ProcessorType = \"cpu\" if len(tf.config.experimental.list_physical_devices(\"GPU\"))==0 else \"gpu\"\n",
    "Splitting = 'from_file' # CV or from_file\n",
    "UseClassWeights = False\n",
    "LR = 0.0025\n",
    "dilation=1\n",
    "num_layers = 96\n",
    "dropout=0.2\n",
    "lemmatize = True\n",
    "lowercase = False\n",
    "pre_trained_embeddings = False\n",
    "deabbreviate = False\n",
    "typo_removal = False # TODO: remove typos\n",
    "filter_reports = True\n",
    "use_multilabel = False\n",
    "\n",
    "FLAG_TERMS = ['uitslag zie medische status', 'zie status', 'zie verslag status', 'slecht echovenster', 'echo overwegen', 'ge echo',\n",
    "              'geen echovenster', 'geen beoordeelbaar echo', 'geen beoordeelbare echo', 'verslag op ic']\n",
    "SAVE_TERMS = ['goed', 'geen', 'normaal', 'normale']\n",
    "MULTILABELS = {'Mild': ['Mild', 'Present'], \n",
    "               'Severe': ['Severe', 'Present'],\n",
    "               'Moderate': ['Moderate', 'Present'],\n",
    "               'Normal': ['Normal'],\n",
    "               'No label': ['No label'],\n",
    "               'Present': ['Present'],\n",
    "               }\n",
    "\n",
    "reduce_labels = False\n",
    "REDUCED_LABELMAP = {\n",
    "    'Present': 'Moderate',\n",
    "    'No label': 'No label',\n",
    "    'Normal': 'Normal',\n",
    "    'Moderate': 'Moderate',\n",
    "    'Severe': 'Severe',\n",
    "    'Mild': 'Mild'\n",
    "}\n",
    "#REDUCED_LABELMAP = {\n",
    "#    'Present': 'Present',\n",
    "#    'No label': 'No label',\n",
    "#    'Normal': 'Normal',\n",
    "#    'Moderate': 'Present',\n",
    "#    'Severe': 'Present',\n",
    "#    'Mild': 'Present'\n",
    "#}\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:40.532539Z",
     "start_time": "2024-05-13T15:16:35.085427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if deabbreviate | typo_removal | filter_reports:\n",
    "    sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "    import deabber, echo_utils\n",
    "    ABBREVIATIONS = benedict.benedict(\"../assets/abbreviations.yml\")"
   ],
   "id": "77b120d5960b8d45",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:40.966552Z",
     "start_time": "2024-05-13T15:16:40.537475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.chdir('T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData')\n",
    "os.listdir(\"./echo_doc_labels\")"
   ],
   "id": "ef30961852feec0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aortic_regurgitation.jsonl',\n",
       " 'aortic_stenosis.jsonl',\n",
       " 'diastolic_dysfunction.jsonl',\n",
       " 'lv_dil.jsonl',\n",
       " 'lv_syst_func.jsonl',\n",
       " 'merged_labels.jsonl',\n",
       " 'mitral_regurgitation.jsonl',\n",
       " 'pe.jsonl',\n",
       " 'rv_dil.jsonl',\n",
       " 'rv_syst_func.jsonl',\n",
       " 'tricuspid_regurgitation.jsonl',\n",
       " 'wma.jsonl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9a824afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:41.384492Z",
     "start_time": "2024-05-13T15:16:40.969562Z"
    }
   },
   "source": [
    "print(ProcessorType)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "353550069f4de85f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:44.422372Z",
     "start_time": "2024-05-13T15:16:41.386962Z"
    }
   },
   "source": [
    "plt.style.use('ggplot')\n",
    "if lemmatize:\n",
    "    nlp = spacy.load(\"nl_core_news_lg\", disable = ['parser','ner'])    "
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "96e4a0adc5cce0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:44.840724Z",
     "start_time": "2024-05-13T15:16:44.425215Z"
    }
   },
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(probs, labels, threshold=0.5):\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    \n",
    "    y_true = labels\n",
    "    #y_true = tf.keras.backend.eval(y_true)\n",
    "    #y_pred = tf.keras.backend.eval(y_pred)\n",
    "    \n",
    "    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    prec_macro = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    prec_weighted = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    prec_micro = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    recall_macro = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_micro = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    try:\n",
    "        roc_auc_weighted = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "        roc_auc_macro = roc_auc_score(y_true, probs, average = 'macro')\n",
    "        roc_auc_micro = roc_auc_score(y_true, probs, average = 'micro')\n",
    "    except ValueError:\n",
    "        roc_auc_weighted = None\n",
    "        roc_auc_macro = None\n",
    "        roc_auc_micro = None\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_macro': f1_macro,\n",
    "               'f1_weighted': f1_weighted,\n",
    "               'f1_micro': f1_micro,\n",
    "               'prec_macro': prec_macro,\n",
    "               'prec_weighted': prec_weighted,\n",
    "               'prec_micro': prec_micro,\n",
    "               'recall_macro': recall_macro,\n",
    "               'recall_weighted': recall_weighted,\n",
    "               'recall_micro': recall_micro,\n",
    "               'roc_auc_macro': roc_auc_macro,\n",
    "               'roc_auc_weighted': roc_auc_weighted,\n",
    "               'roc_auc_micro': roc_auc_micro,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "     \n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ecd0b0ba39bca6be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:45.274858Z",
     "start_time": "2024-05-13T15:16:44.845718Z"
    }
   },
   "source": [
    "# inference pipe\n",
    "def tokenize_and_pad(x, tokenizer, maxlen = 256):\n",
    "    toks = tokenizer.texts_to_sequences(x)\n",
    "    toks_padded = pad_sequences(toks, padding = 'post', maxlen = maxlen)\n",
    "    return toks_padded\n",
    "\n",
    "def evaluate_model(clf, labels):\n",
    "    probas = clf.predict(X_test)\n",
    "    cf_matrix = confusion_matrix(np.argmax(labels, axis=1),\n",
    "                                 np.argmax(probas, axis=1))\n",
    "    return cf_matrix\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "3331f26fd0c509f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:45.693316Z",
     "start_time": "2024-05-13T15:16:45.276725Z"
    }
   },
   "source": [
    "# Stratified cross-validation\n",
    "def fold_indices(targets: pd.Series=None, stratified: bool=True, seed: int=42, numfolds: int=10)->Tuple[List,List]:\n",
    "    if stratified:\n",
    "        splitter = StratifiedKFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = targets\n",
    "    else:\n",
    "        splitter = KFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = None\n",
    "\n",
    "    train_indcs, test_indcs = [], []\n",
    "    for train_index, test_index in splitter.split(X=targets, y=_Targets):\n",
    "        train_indcs.append(train_index)\n",
    "        test_indcs.append(test_index)\n",
    "\n",
    "    return zip(train_indcs, test_indcs)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a2ce3cf91202770e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:46.111942Z",
     "start_time": "2024-05-13T15:16:45.695293Z"
    }
   },
   "source": [
    "class CustomMetrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super(CustomMetrics, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_predict = self.model.predict(self.validation_data[0])\n",
    "        val_targ = self.validation_data[1]\n",
    "\n",
    "        _val_metrics = multi_label_metrics(val_targ, val_predict)\n",
    "        for name, value in _val_metrics.items():\n",
    "            logs['val_' + name] = value\n",
    "\n",
    "        print(\" — val_f1_macro: %f — val_accuracy: %f\" % (logs['val_f1_macro'], logs['val_accuracy']))\n",
    "        return"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "e4c02977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:00:03.448988Z",
     "start_time": "2024-05-13T16:00:02.744984Z"
    }
   },
   "source": [
    "def CurrentModel(modelselection: Literal['bigru', 'bilstm', 'cnn', 'hstacked_dcnn', 'textcnn'], \n",
    "                 embeddingdim: int=128, \n",
    "                 maxlen: int=256, \n",
    "                 vocabsize: int=50_000,\n",
    "                 numclasses: int=5,\n",
    "                 learningrate: float=0.001,\n",
    "                 num_layers: int=128,\n",
    "                 pre_trained_vectors: bool=False,\n",
    "                 multilabel: bool=False,\n",
    "                 dilation: int=4, \n",
    "                 dropout: float=0.,\n",
    "                 dilations: List[int]=[1,2,4,8],\n",
    "                 filters: List[int]=[1,2,3,5],\n",
    "                 attention: bool=False\n",
    "                 ):\n",
    "    if modelselection != 'textcnn':\n",
    "        model = Sequential()\n",
    "        \n",
    "    accuracy = 'binary_accuracy'\n",
    "    if multilabel:\n",
    "        finalact = 'sigmoid'\n",
    "        lossfunction = 'binary_crossentropy'\n",
    "    else:\n",
    "        finalact = 'softmax'\n",
    "        lossfunction = 'categorical_crossentropy'\n",
    "        \n",
    "    if pre_trained_vectors:\n",
    "        num_dense = 20\n",
    "    else:\n",
    "        num_dense = 10\n",
    "        \n",
    "    if modelselection =='bigru':\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Bidirectional(layers.GRU(num_layers, name='biGRU'), input_shape=(maxlen, embeddingdim)))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length=maxlen, name='EmbeddingLayer'))\n",
    "            model.add(layers.Bidirectional(layers.GRU(num_layers)))\n",
    "    ##################################################################################################\n",
    "    elif modelselection =='rcnn':\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Bidirectional(layers.GRU(num_layers, name='biGRU'), input_shape=(maxlen, embeddingdim)))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length=maxlen, name='EmbeddingLayer'))\n",
    "            model.add(layers.Bidirectional(layers.GRU(num_layers)))   \n",
    "        model.add(layers.Conv1D(num_layers//4, 5, activation = 'relu', dilation_rate = dilation, name='conv'))\n",
    "        model.add(layers.GlobalMaxPooling1D(name='GlobalMaxPooling'))\n",
    "    ##################################################################################################    \n",
    "    elif modelselection =='bilstm':\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Bidirectional(layers.LSTM(num_layers, name='biLSTM'), input_shape=(maxlen, embeddingdim)))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length=maxlen, name='EmbeddingLayer'))\n",
    "            model.add(layers.Bidirectional(layers.LSTM(num_layers)))\n",
    "    ##################################################################################################        \n",
    "    elif modelselection =='cnn':\n",
    "        # basic cnn with one dilation\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Conv1D(num_layers, 5, activation = 'relu', dilation_rate = dilation, input_shape = (maxlen, embeddingdim), name='conv'))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length = maxlen, name='EmbeddingLayer'))\n",
    "            model.add(layers.Conv1D(num_layers, 5, activation = 'relu', dilation_rate = dilation, name='conv'))\n",
    "        model.add(layers.GlobalMaxPooling1D(name='GlobalMaxPooling'))\n",
    "    ##################################################################################################    \n",
    "    elif modelselection == 'hstacked_dcnn':\n",
    "        # CNN with multiple dilations; 0,1,2,4,8 stacked, before going into the final layer\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Conv1D(num_layers, 5, activation = 'relu', dilation_rate = dilations[0], input_shape = (maxlen, embeddingdim), name=f'conv_dil1'))\n",
    "            for _dilation in dilations[1:]:\n",
    "                model.add(layers.Conv1D(num_layers, 5, activation = 'relu', dilation_rate = _dilation, name=f'conv_dil{_dilation}'))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length = maxlen, name='EmbeddingLayer'))\n",
    "            for _dilation in dilations:\n",
    "                model.add(layers.Conv1D(num_layers, 5, activation = 'relu', dilation_rate = _dilation, name=f'conv_dil{_dilation}'))\n",
    "        model.add(layers.GlobalMaxPooling1D(name='GlobalMaxPooling'))\n",
    "    ##################################################################################################    \n",
    "    elif modelselection == 'vstacked_dcnn':\n",
    "        # CNN with multiple dilations; 1,2,4,8 stacked, before going into the final layer\n",
    "        # basic cnn with one dilation\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Conv1D(num_layers, 5, activation = 'relu', dilation_rate = dilations[0], input_shape = (maxlen, embeddingdim), name=f'conv_dil1'))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length = maxlen, name='EmbeddingLayer'))\n",
    "        # TODO: add logic for vstacking\n",
    "        model.add(layers.Conv1D(num_layers//4, 5, activation = 'relu', dilation_rate = 1))\n",
    "        model.add(layers.GlobalMaxPooling1D(name='GlobalMaxPooling'))    \n",
    "    ##################################################################################################    \n",
    "    elif modelselection == 'textcnn':\n",
    "        # also see: https://github.com/ShaneTian/TextCNN/blob/master/text_cnn.py and https://arxiv.org/abs/1408.5882\n",
    "        inputs = layers.Input(shape = (maxlen,),  name='InputData')\n",
    "        if pre_trained_vectors:\n",
    "            xlayer = layers.Embedding(vocabsize, embeddingdim, input_length = maxlen, name='EmbeddingLayer', trainable=False)(inputs)\n",
    "        else:\n",
    "            xlayer = layers.Embedding(vocabsize, embeddingdim, input_length = maxlen, name='EmbeddingLayer', trainable=True)(inputs)\n",
    "        xlayer = layers.Reshape((maxlen, embeddingdim, 1))(xlayer)\n",
    "        \n",
    "        pool_outputs = []\n",
    "        for filter_size in filters:\n",
    "            filter_shape = (filter_size, embeddingdim)\n",
    "            conv  = layers.Conv2D(num_layers, \n",
    "                                 kernel_initializer='he_normal',\n",
    "                                 kernel_size=filter_shape, \n",
    "                                 activation = 'relu',\n",
    "                                 name=f'conv_{filter_size}')(xlayer)\n",
    "            pool  = layers.MaxPool2D(pool_size=(maxlen-filter_size+1, 1),\n",
    "                                     strides=(1, 1), padding='valid',\n",
    "                                     data_format='channels_last',\n",
    "                                     name='max_pooling_{:d}'.format(filter_size))(conv)\n",
    "            pool_outputs.append(pool)\n",
    "            \n",
    "        x = layers.concatenate(pool_outputs, axis=-1, name='concatenate')\n",
    "        x = layers.Flatten(data_format='channels_last', name='flatten')(x)\n",
    "        \n",
    "        x = layers.Dropout(dropout,  name='dropout')(x)\n",
    "        x = layers.Dense(num_dense, activation = 'relu', name='Dense1')(x)\n",
    "        outputs = layers.Dense(numclasses, activation = finalact, name='Dense2')(x)\n",
    "        model = models.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer = optimizers.Adam(learning_rate=learningrate),\n",
    "                      loss = lossfunction,\n",
    "                      metrics = [accuracy])\n",
    "        return model\n",
    "    \n",
    "    if attention:\n",
    "        model.add(layers.Attention(use_scale=True, name='attention'))\n",
    "    \n",
    "    model.add(layers.Dropout(dropout,  name='dropout'))\n",
    "    model.add(layers.Dense(num_dense, activation = 'relu', name='Dense1'))\n",
    "    model.add(layers.Dense(numclasses, activation = finalact, name='Dense2'))\n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate=learningrate),\n",
    "                  loss = lossfunction,\n",
    "                  metrics = [accuracy])  \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "0c8d3cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:32:22.605158Z",
     "start_time": "2024-05-13T15:32:22.016653Z"
    }
   },
   "source": [
    "def run_model_pipe(x: list, model: Sequential, maxlen: int=256, tokenizer=None):\n",
    "    x_tok = tokenize_and_pad(x, tokenizer=tokenizer, maxlen=maxlen)\n",
    "    return model.predict(x_tok)"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:47.397317Z",
     "start_time": "2024-05-13T15:16:46.982313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # labels should be a list or a 1D tensor\n",
    "    return keras_utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "def _mlabel_tuple_creator(x: List[int],\n",
    "                          multilabels:Dict[int,List[int]])\\\n",
    "                          ->List[Tuple[int,...]]:\n",
    "                              \n",
    "    res = [(_sc for _sc in multilabels[sc]) for sc in x]\n",
    "    return res\n",
    "\n",
    "def multi_hot_encoding(x: List[int], \n",
    "                       multilabels: Union[Dict[int,List[int]], None]=None,\n",
    "                       num_classes: int=None)\\\n",
    "                           ->np.array:    \n",
    "    if multilabels is None:\n",
    "        return one_hot_encode(x, num_classes=num_classes)\n",
    "    else:\n",
    "        return MultiLabelBinarizer(classes=range(num_classes))\\\n",
    "                    .fit_transform(_mlabel_tuple_creator(x,multilabels))"
   ],
   "id": "4606a13c3ffd81a6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "3d471e0819f26480",
   "metadata": {},
   "source": " # Load data"
  },
  {
   "cell_type": "code",
   "id": "9d0028d86cdb445f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:47.893582Z",
     "start_time": "2024-05-13T15:16:47.399814Z"
    }
   },
   "source": [
    "labeled_documents = pd.read_json(f\"./echo_doc_labels/{Class}.jsonl\", lines=True)\n",
    "label_col = 'label' if Class!='merged_labels' else 'labels'\n",
    "target_df = pd.DataFrame.from_records(labeled_documents[label_col])"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7581d300fbdc685b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:48.312095Z",
     "start_time": "2024-05-13T15:16:47.895633Z"
    }
   },
   "source": [
    "labeled_documents.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8226, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "91a201c2411f947b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:48.916632Z",
     "start_time": "2024-05-13T15:16:48.315098Z"
    }
   },
   "source": [
    "num_tokens = labeled_documents.text.apply(lambda x: len(x.split(\" \")))\n",
    "plt.hist(num_tokens, bins=30);"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "49a00fc68e98cfdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:53.116951Z",
     "start_time": "2024-05-13T15:16:48.919102Z"
    }
   },
   "source": [
    "_set = set()\n",
    "for __set in labeled_documents.text.apply(lambda x: set(x.split(\" \"))).values:\n",
    "    _set = _set.union(__set)\n",
    "print(f\"Number of unique tokens: {len(_set)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 25457\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "2bd53d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:53.721450Z",
     "start_time": "2024-05-13T15:16:53.119431Z"
    }
   },
   "source": [
    "# load train/test hashes\n",
    "#hash_docs_link = pd.read_json(\"./echo_span_labels/reduced_labels/merged_labels.jsonl\", lines=True)[['text', '_input_hash']]\n",
    "train_ids = pd.read_csv('./train_echoid.csv', sep=',').input_hash.unique()\n",
    "test_ids = pd.read_csv('./test_echoid.csv', sep=',').input_hash.unique()\n",
    "\n",
    "labeled_documents['_hash'] = labeled_documents.text.str.strip().apply(lambda x: hash(x))\n",
    "#hash_docs_link['_hash'] = hash_docs_link.text.str.strip().apply(lambda x: hash(x))\n",
    "\n",
    "#labeled_documents = labeled_documents.merge(hash_docs_link[['_input_hash', '_hash']], \n",
    "#                                            on='_hash', how='inner')\n",
    "\n",
    "labeled_documents = labeled_documents.drop_duplicates(subset=['_hash']).reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "6632439635dd361f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:54.154989Z",
     "start_time": "2024-05-13T15:16:53.723839Z"
    }
   },
   "source": [
    "# Expand with label columns\n",
    "if Class == 'merged_labels':\n",
    "    Target_maps = {\n",
    "        _Class: {Label:i for i,Label in enumerate(target_df[Class].unique())}\n",
    "        for _Class in target_df.columns\n",
    "    }\n",
    "else:\n",
    "    Target_maps = {\n",
    "        Class: {Label: i for i,Label in enumerate(labeled_documents['label'].unique())} \n",
    "    }\n",
    "    \n",
    "if Class == 'merged_labels':\n",
    "    DF = labeled_documents[['text', '_input_hash']].join(target_df[Class])\n",
    "else:\n",
    "    DF = labeled_documents[['text', '_input_hash', 'label']]\n",
    "\n",
    "DF.columns = ['sentence', '_input_hash', 'labels']\n",
    "\n",
    "label2id = Target_maps[Class]\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "if use_multilabel:\n",
    "    _multilabels = {label2id.get(k): [label2id.get(l) for l in v if label2id.get(l) is not None]\n",
    "                    for k,v in MULTILABELS.items() if label2id.get(k) is not None}\n",
    "else:\n",
    "    _multilabels = None\n",
    "\n",
    "DF= DF.assign(label=DF['labels'].map(label2id))"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:54.573373Z",
     "start_time": "2024-05-13T15:16:54.157341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if use_multilabel:\n",
    "    if all([len(v)==1 for v in _multilabels.values()]):\n",
    "        use_multilabel = False\n",
    "        _multilabels = None"
   ],
   "id": "6fa41c34d45cc27e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "134425514247d5e6",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cd611ac6ba3027e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:16:55.022916Z",
     "start_time": "2024-05-13T15:16:54.575858Z"
    }
   },
   "source": [
    "DF = DF.assign(sentence=DF.sentence.str.replace(r'[\\r\\n]', '', regex=True)) "
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "ef0e8d7af868e41f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:17:11.375649Z",
     "start_time": "2024-05-13T15:16:55.024850Z"
    }
   },
   "source": [
    "if lemmatize:\n",
    "    docs = nlp.pipe(DF.sentence.values)\n",
    "    new_texts = [\" \".join([token.lemma_ for token in doc]) for doc in docs] \n",
    "    DF = DF.assign(sentence = new_texts)\n",
    "\n",
    "if lowercase:\n",
    "    DF = DF.assign(sentence = DF.sentence.str.lower())\n",
    "    \n",
    "if filter_reports:\n",
    "    DF = DF.assign(sentence = echo_utils.report_filter(DF.sentence, \n",
    "                                            flag_terms=FLAG_TERMS, \n",
    "                                            save_terms=SAVE_TERMS)[0])\n",
    "    DF = DF.loc[DF.sentence.notna()]\n",
    "\n",
    "if deabbreviate:\n",
    "    DeAbber = deabber.deabber(model_type='sbert', abbreviations=ABBREVIATIONS['nl']['echocardiogram'], min_sim=0.5, top_k=10)\n",
    "    DF = DF.assign(sentence=DeAbber.deabb(DF.sentence.values, TokenRadius=3))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "a3c68f12e14c3d84",
   "metadata": {},
   "source": [
    "# Make folds"
   ]
  },
  {
   "cell_type": "code",
   "id": "83203825517c00c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:17:11.825027Z",
     "start_time": "2024-05-13T15:17:11.378508Z"
    }
   },
   "source": [
    "# prepping\n",
    "TrainTestDict = defaultdict(dict)\n",
    "\n",
    "if Splitting == 'from_file':\n",
    "    TrainTestDict[0]['Xtrain'] = DF.loc[DF._input_hash.isin(train_ids)].sentence\n",
    "    TrainTestDict[0]['Xtest'] = DF.loc[DF._input_hash.isin(test_ids)].sentence\n",
    "    \n",
    "    TrainTestDict[0]['ytrain'] = DF.loc[DF._input_hash.isin(train_ids)].label\n",
    "    TrainTestDict[0]['ytest'] = DF.loc[DF._input_hash.isin(test_ids)].label\n",
    "    \n",
    "elif Splitting == 'CV':\n",
    "    for k,(train_index, test_index) in enumerate(fold_indices(targets=DF['label'], stratified=True)):\n",
    "        TrainTestDict[k]['Xtrain'] = DF.iloc[train_index].sentence\n",
    "        TrainTestDict[k]['Xtest'] = DF.iloc[test_index].sentence\n",
    "        \n",
    "        TrainTestDict[k]['ytrain'] = DF.iloc[train_index].label\n",
    "        TrainTestDict[k]['ytest'] = DF.iloc[test_index].label\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "87a4b47e71c1ca13",
   "metadata": {},
   "source": [
    "## Initiate models"
   ]
  },
  {
   "cell_type": "code",
   "id": "a28bb238849448ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:17:12.274619Z",
     "start_time": "2024-05-13T15:17:11.826997Z"
    }
   },
   "source": [
    "if pre_trained_embeddings:\n",
    "    embedding_dim = nlp.vocab.vectors_length"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "664871e9df2dde6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:00:11.960372Z",
     "start_time": "2024-05-13T16:00:10.320373Z"
    }
   },
   "source": [
    "ModelDict = defaultdict(dict)\n",
    "for k,v in TrainTestDict.items():\n",
    "    if pre_trained_embeddings:\n",
    "        LSTM_Tokenizer = None\n",
    "        vocab_size = None\n",
    "    else:\n",
    "        LSTM_Tokenizer = KerasTokenizer(num_words=num_words_in_vocab)\n",
    "        LSTM_Tokenizer.fit_on_texts(v['Xtrain'])\n",
    "        vocab_size = len(LSTM_Tokenizer.word_index)+1\n",
    "\n",
    "    num_classes = np.unique(v['ytrain']).shape[0]\n",
    "    \n",
    "    ModelDict[k]['tokenizer'] = LSTM_Tokenizer    \n",
    "    if use_multilabel:\n",
    "        ModelDict[k]['warmup_model'] = CurrentModel(modelselection=ModelType, \n",
    "                                     embeddingdim=embedding_dim,\n",
    "                                     maxlen=max_len,\n",
    "                                     vocabsize=vocab_size,\n",
    "                                     numclasses=num_classes,\n",
    "                                     learningrate=LR,\n",
    "                                     dilation=dilation,\n",
    "                                     num_layers=num_layers,\n",
    "                                     multilabel=False,\n",
    "                                     pre_trained_vectors=pre_trained_embeddings,\n",
    "                                     dropout=dropout\n",
    "                                     )\n",
    "    else:\n",
    "        ModelDict[k]['model'] = CurrentModel(modelselection=ModelType, \n",
    "                                     embeddingdim=embedding_dim,\n",
    "                                     maxlen=max_len,\n",
    "                                     vocabsize=vocab_size,\n",
    "                                     numclasses=num_classes,\n",
    "                                     learningrate=LR,\n",
    "                                     dilation=dilation,\n",
    "                                     num_layers=num_layers,\n",
    "                                     multilabel=use_multilabel,\n",
    "                                     pre_trained_vectors=pre_trained_embeddings,\n",
    "                                     dropout=dropout\n",
    "                                         )\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Warmup models\n",
    "\n",
    "If we are running a multilabel classification with BCE loss we might get stuck in a local minimum where recall is 1 for all\n",
    "classes. To avoid this we pre-run the model with categorical cross-entropy loss, with a softmax in the final layer. To be able to apply\n",
    "the CCEloss we want 1 label per sample, we force by taking the minority class in each multilabel instance."
   ],
   "id": "d89bc82e1f567223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:00:18.958375Z",
     "start_time": "2024-05-13T16:00:18.340804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_multilabel(Model: Sequential, learningrate: float=1e-4) -> Sequential:\n",
    "    \"\"\"\n",
    "    Make a multilabel classification model, given a pre-trained Sequential model.\n",
    "    It replaces the last Dense layer with 'softmax' activation to a 'sigmoid' activation,\n",
    "    making it suitable for multilabel classification.\n",
    "\n",
    "    :param Model: A pre-trained Keras Sequential model.\n",
    "    :return: A modified Sequential model ready for multilabel classification.\n",
    "    \"\"\"\n",
    "    assert isinstance(Model, Sequential), \"Model must be a Keras Sequential model.\"\n",
    "\n",
    "    # Create a new Sequential model\n",
    "    new_model = Sequential()\n",
    "\n",
    "    # Copy all layers except the last one from the original model to the new model\n",
    "    for layer in Model.layers[:-1]:\n",
    "        new_model.add(layer)\n",
    "\n",
    "    # Assume the last layer is a Dense layer and get the number of units from it\n",
    "    num_classes = Model.layers[-1].output_shape[-1]\n",
    "\n",
    "    # Add a new Dense layer with sigmoid activation suited for multilabel classification\n",
    "    new_model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    # Important: this line copies the weights of all layers except the last one from the original model\n",
    "    for layer, new_layer in zip(Model.layers[:-1], new_model.layers[:-1]):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "    new_model.compile(optimizer = optimizers.Adam(learning_rate=learningrate),\n",
    "                      loss = 'binary_crossentropy',\n",
    "                      metrics = ['binary_accuracy'])\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "# def make_multilabel(model: Sequential):\n",
    "#     '''\n",
    "#     Make a multilabel classification model, given a pre-trained Sequential model.\n",
    "#     Actions:\n",
    "#      1. Replace layers.Dense(numclasses, activation = 'softmax') with\n",
    "#         layers.Dense(numclasses, activation = 'sigmoid')\n",
    "#      2. Replace the loss function 'categorical_crossentropy' with 'binary_crossentropy'.\n",
    "# \n",
    "#     :param model: A pre-trained Keras Sequential model.\n",
    "#     :return: Sequential model with pretrained weights, adapted for multi-label classification.\n",
    "#     '''\n",
    "# \n",
    "#     assert isinstance(model, Sequential), \"Multilabel model must be Sequential\"\n",
    "# \n",
    "#     # Modify the output layer\n",
    "#     for layer in model.layers:\n",
    "#         if isinstance(layer, Dense) and layer.activation == tf.keras.activations.softmax:\n",
    "#             num_classes = layer.units  # Preserve number of classes\n",
    "#             model.pop()  # Remove old output layer\n",
    "#             model.add(Dense(num_classes, activation='sigmoid'))\n",
    "#             break  # Assuming only one softmax output layer\n",
    "# \n",
    "#     # Replace loss function (assuming 'categorical_crossentropy' was used originally)\n",
    "#     model.compile(optimizer=model.optimizer,  # Preserve the optimizer\n",
    "#                   loss='binary_crossentropy', \n",
    "#                   metrics=['accuracy'])  # Or other suitable metrics\n",
    "# \n",
    "#     return model"
   ],
   "id": "415ea56d0aed8b22",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:00:19.888474Z",
     "start_time": "2024-05-13T16:00:19.039470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WarmupDict = defaultdict(lambda: defaultdict(list))\n",
    "if use_multilabel:   \n",
    "    # run with softmax and CCEloss\n",
    "    for k,v in ModelDict.items():\n",
    "        _model =  v['warmup_model']\n",
    "        _tokenizer = v['tokenizer']\n",
    "        \n",
    "        if pre_trained_embeddings:\n",
    "            X_train = echo_utils.text_to_vectors(TrainTestDict[k]['Xtrain'], max_len) \n",
    "            X_test = echo_utils.text_to_vectors(TrainTestDict[k]['Xtest'], max_len) \n",
    "        else:\n",
    "            X_train = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtrain'])\n",
    "            X_test = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtest'])\n",
    "        \n",
    "            X_train = pad_sequences(X_train, padding = 'post', maxlen=max_len)\n",
    "            X_test = pad_sequences(X_test, padding = 'post', maxlen=max_len)\n",
    "    \n",
    "        _y = TrainTestDict[k]['ytrain']\n",
    "        y_train = multi_hot_encoding(_y, multilabels=None, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "    \n",
    "        _y = TrainTestDict[k]['ytest']\n",
    "        y_test = multi_hot_encoding(_y, multilabels=None, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "    \n",
    "        if UseClassWeights:\n",
    "            PosSum = 1/np.sum(y_train, axis=0)\n",
    "            ClassWeights = dict(enumerate(PosSum/np.min(PosSum)))\n",
    "        else:\n",
    "            ClassWeights = None\n",
    "        \n",
    "        result_list = []\n",
    "        history_list = []\n",
    "        confusion_list = []\n",
    "        for epoch_num in range(1, warmup_steps+1):\n",
    "            _history = _model.fit(X_train,\n",
    "                                 y_train,                         \n",
    "                                 epochs = 1,\n",
    "                                 verbose = False,\n",
    "                                 validation_split = 0,\n",
    "                                 class_weight=ClassWeights,\n",
    "                                 batch_size = batch_size)\n",
    "        \n",
    "            y_pred = _model.predict(X_test, verbose=False)\n",
    "            _res = multi_label_metrics(labels=y_test, probs=y_pred)\n",
    "            \n",
    "            result_list.append(_res)\n",
    "            history_list.append({'loss': _history.history['loss'][0], \n",
    "                                 'accuracy': _history.history['binary_accuracy'][0]})\n",
    "            confusion_list.append(confusion_matrix(np.argmax(y_test, axis=1),\n",
    "                                                   np.argmax(np.round(y_pred),axis=1),  normalize='true'))\n",
    "            \n",
    "            \n",
    "            print(f\"Epoch {epoch_num}, F1 Macro={_res['f1_macro']}\")\n",
    "            \n",
    "        WarmupDict[k]['history'] = pd.DataFrame(history_list)\n",
    "        WarmupDict[k]['result'] = pd.DataFrame(result_list)\n",
    "        WarmupDict[k]['confusion'] = confusion_list\n",
    "        \n",
    "        ModelDict[k]['model'] = make_multilabel(_model, learningrate=warmup_lr)\n",
    "         \n",
    "    "
   ],
   "id": "9549dae54c0f97f4",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:00:20.570625Z",
     "start_time": "2024-05-13T16:00:20.035621Z"
    }
   },
   "cell_type": "code",
   "source": "id2label",
   "id": "e98939a8a987ded8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'No label',\n",
       " 1: 'Normal',\n",
       " 2: 'Present',\n",
       " 3: 'Severe',\n",
       " 4: 'Mild',\n",
       " 5: 'Moderate'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "26a30b4c2d9bfaf3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:00:22.934579Z",
     "start_time": "2024-05-13T16:00:22.243063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if use_multilabel:\n",
    "    _confusion_matrix = multilabel_confusion_matrix\n",
    "else:\n",
    "    _confusion_matrix = confusion_matrix"
   ],
   "id": "dc9ef4a32da8a35a",
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "a15851917a011494",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-13T16:00:57.141097Z"
    }
   },
   "source": [
    "ResultDict = defaultdict(lambda: defaultdict(list))\n",
    "for k,v in ModelDict.items():\n",
    "    _model =  v['model']\n",
    "    _tokenizer = v['tokenizer']\n",
    "    \n",
    "    if pre_trained_embeddings:\n",
    "        X_train = echo_utils.text_to_vectors(TrainTestDict[k]['Xtrain'], max_len) \n",
    "        X_test = echo_utils.text_to_vectors(TrainTestDict[k]['Xtest'], max_len) \n",
    "    else:\n",
    "        X_train = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtrain'])\n",
    "        X_test = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtest'])\n",
    "    \n",
    "        X_train = pad_sequences(X_train, padding = 'post', maxlen=max_len)\n",
    "        X_test = pad_sequences(X_test, padding = 'post', maxlen=max_len)\n",
    "\n",
    "    _y = TrainTestDict[k]['ytrain']\n",
    "    y_train = multi_hot_encoding(_y, multilabels=_multilabels, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "\n",
    "    _y = TrainTestDict[k]['ytest']\n",
    "    y_test = multi_hot_encoding(_y, multilabels=_multilabels, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "\n",
    "    if UseClassWeights:\n",
    "        PosSum = 1/np.sum(y_train, axis=0)\n",
    "        ClassWeights = dict(enumerate(PosSum/np.min(PosSum)))\n",
    "    else:\n",
    "        ClassWeights = None\n",
    "    \n",
    "    result_list = []\n",
    "    history_list = []\n",
    "    confusion_list = []\n",
    "    for epoch_num in range(1, num_epochs+1):\n",
    "        _history = _model.fit(X_train,\n",
    "                             y_train,                         \n",
    "                             epochs = 1,\n",
    "                             verbose = False,\n",
    "                             validation_split = 0,\n",
    "                             class_weight=ClassWeights,\n",
    "                             batch_size = batch_size)\n",
    "    \n",
    "        y_pred = _model.predict(X_test, verbose=False)\n",
    "        _res = multi_label_metrics(labels=y_test, probs=y_pred)\n",
    "        \n",
    "        result_list.append(_res)\n",
    "        history_list.append({'loss': _history.history['loss'][0], \n",
    "                             'accuracy': _history.history['binary_accuracy'][0]})\n",
    "        confusion_list.append(_confusion_matrix(np.argmax(y_test, axis=1),\n",
    "                                               np.argmax(np.round(y_pred),axis=1)))\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch_num}, F1 Macro={_res['f1_macro']}\")\n",
    "        \n",
    "    ResultDict[k]['history'] = pd.DataFrame(history_list)\n",
    "    ResultDict[k]['result'] = pd.DataFrame(result_list)\n",
    "    ResultDict[k]['confusion'] = confusion_list\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "confusion_list[-1]",
   "id": "369d94430d64ea0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "caace1698b3b2400",
   "metadata": {},
   "source": [
    "heatmap_df = \\\n",
    "    pd.DataFrame(\n",
    "    confusion_list[-1],\n",
    "        columns=label2id.keys(),\n",
    "        index=label2id.keys()\n",
    "    )\n",
    "sns.heatmap(heatmap_df.round(4), annot=True, cmap='Blues', fmt='g')\n",
    "    # df.style.background_gradient(cmap='Blues')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Class: {Class}, Model: {ModelType}, Splitting: {Splitting}\")",
   "id": "cb19bce9e1f896ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b7cad5ab1538d3",
   "metadata": {},
   "source": [
    "MultiLabelStr = 'multilabel' if use_multilabel else 'categorical'\n",
    "ResultDict[k]['result'].to_csv(f\"../../G_Output/2_Data/results_{MultiLabelStr}_{Class}_{ModelType}_{Splitting}.csv\", sep=\";\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e627eadb8f623ba",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a77e2c88cef599d3"
  },
  {
   "cell_type": "code",
   "id": "1bfb9a2f9db3a0d4",
   "metadata": {},
   "source": [
    "plt.plot(ResultDict[k]['result']['f1_macro'], label='F1 Macro')\n",
    "plt.plot(ResultDict[k]['result']['prec_macro'], label='Precision Macro')\n",
    "plt.plot(ResultDict[k]['result']['recall_macro'], label='Recall Macro')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ead311ab73fa4d25",
   "metadata": {},
   "source": [
    "plt.plot(ResultDict[k]['history'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c27d70a8f816b3d9",
   "metadata": {},
   "source": [
    "ResultDict[k]['result']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_model(_model, to_file='./model_plot.png', show_shapes=True, show_layer_names=True)",
   "id": "3f536753e31dcdb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de0495325550a5c7",
   "metadata": {},
   "source": [
    "## Upload to huggingface hub"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "72fd788ebc123c44",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2452b646be1643b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nlp_310)",
   "language": "python",
   "name": "python3_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
