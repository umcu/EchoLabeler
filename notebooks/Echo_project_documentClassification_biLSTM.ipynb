{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:24.884097Z",
     "start_time": "2024-05-10T10:39:23.138517Z"
    }
   },
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import benedict"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For multilabel the BCE loss collapses to a positive label for all classes. A better way to start is to use categorical cross-entropy initially.",
   "id": "685a24a9f0e29e94"
  },
  {
   "cell_type": "code",
   "id": "1dd8f28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:38.506684Z",
     "start_time": "2024-05-10T10:39:24.886606Z"
    }
   },
   "source": [
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers, utils as keras_utils\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "\n",
    "from typing import Callable, Tuple, Dict, List, Literal, Union"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1461462f259a5bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:38.832191Z",
     "start_time": "2024-05-10T10:39:38.509157Z"
    }
   },
   "source": [
    "# TODO: add pre-trained word embeddings\n",
    "# Generic Spacy\n",
    "# Medical Dutch"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4efa158612b8bcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:39.204265Z",
     "start_time": "2024-05-10T10:39:38.836151Z"
    }
   },
   "source": [
    "#  settings\n",
    "embedding_dim = 256\n",
    "max_len = 200\n",
    "num_words_in_vocab = 5_000\n",
    "num_epochs = 50\n",
    "warmup_steps = 40 # only relevant for multilabel problems\n",
    "warmup_lr = 2e-3 # only relevant for multilabel problems\n",
    "\n",
    "num_folds = 10\n",
    "# multilabel: lv_dil, pe, rv_dil\n",
    "# categorical: aortic_regurgitation, lv_syst_func, rv_syst_func, aortic_stenosis, diastolic_dysfunction, mitral_regurgitation, tricuspid_regurgitation\n",
    "\n",
    "Class = 'lv_syst_func' \n",
    "batch_size = 128\n",
    "ModelType = 'bigru' # bigru, bilstm, cnn\n",
    "ProcessorType = \"cpu\" if len(tf.config.experimental.list_physical_devices(\"GPU\"))==0 else \"gpu\"\n",
    "Splitting = 'from_file' # CV or from_file\n",
    "UseClassWeights = False\n",
    "LR = 0.005\n",
    "dilation=2\n",
    "num_layers = 96\n",
    "lemmatize = True\n",
    "lowercase = False\n",
    "pre_trained_embeddings = False\n",
    "deabbreviate = False\n",
    "typo_removal = False # TODO: remove typos\n",
    "filter_reports = True\n",
    "use_multilabel = False\n",
    "\n",
    "\n",
    "\n",
    "FLAG_TERMS = ['uitslag zie medische status', 'zie status', 'zie verslag status', 'slecht echovenster', 'echo overwegen', 'ge echo',\n",
    "              'geen echovenster', 'geen beoordeelbaar echo', 'geen beoordeelbare echo', 'verslag op ic']\n",
    "SAVE_TERMS = ['goed', 'geen', 'normaal', 'normale']\n",
    "MULTILABELS = {'Mild': ['Mild', 'Present'], \n",
    "               'Severe': ['Severe', 'Present'],\n",
    "               'Moderate': ['Moderate', 'Present'],\n",
    "               'Normal': ['Normal'],\n",
    "               'No label': ['No label'],\n",
    "               'Present': ['Present'],\n",
    "               }\n",
    "\n",
    "reduce_labels = False\n",
    "REDUCED_LABELMAP = {\n",
    "    'Present': 'Moderate',\n",
    "    'No label': 'No label',\n",
    "    'Normal': 'Normal',\n",
    "    'Moderate': 'Moderate',\n",
    "    'Severe': 'Severe',\n",
    "    'Mild': 'Mild'\n",
    "}\n",
    "#REDUCED_LABELMAP = {\n",
    "#    'Present': 'Present',\n",
    "#    'No label': 'No label',\n",
    "#    'Normal': 'Normal',\n",
    "#    'Moderate': 'Present',\n",
    "#    'Severe': 'Present',\n",
    "#    'Mild': 'Present'\n",
    "#}\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:45.199147Z",
     "start_time": "2024-05-10T10:39:39.206694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if deabbreviate | typo_removal | filter_reports:\n",
    "    sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "    import deabber, echo_utils\n",
    "    ABBREVIATIONS = benedict.benedict(\"../assets/abbreviations.yml\")"
   ],
   "id": "77b120d5960b8d45",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:46.036159Z",
     "start_time": "2024-05-10T10:39:45.202116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.chdir('T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData')\n",
    "os.listdir(\"./echo_doc_labels\")"
   ],
   "id": "ef30961852feec0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aortic_regurgitation.jsonl',\n",
       " 'aortic_stenosis.jsonl',\n",
       " 'diastolic_dysfunction.jsonl',\n",
       " 'lv_dil.jsonl',\n",
       " 'lv_syst_func.jsonl',\n",
       " 'merged_labels.jsonl',\n",
       " 'mitral_regurgitation.jsonl',\n",
       " 'pe.jsonl',\n",
       " 'rv_dil.jsonl',\n",
       " 'rv_syst_func.jsonl',\n",
       " 'tricuspid_regurgitation.jsonl',\n",
       " 'wma.jsonl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9a824afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:46.501142Z",
     "start_time": "2024-05-10T10:39:46.039628Z"
    }
   },
   "source": [
    "print(ProcessorType)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "353550069f4de85f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:49.601219Z",
     "start_time": "2024-05-10T10:39:46.503653Z"
    }
   },
   "source": [
    "plt.style.use('ggplot')\n",
    "if lemmatize:\n",
    "    nlp = spacy.load(\"nl_core_news_lg\", disable = ['parser','ner'])    "
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "96e4a0adc5cce0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:50.019692Z",
     "start_time": "2024-05-10T10:39:49.603199Z"
    }
   },
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(probs, labels, threshold=0.5):\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    \n",
    "    y_true = labels\n",
    "    #y_true = tf.keras.backend.eval(y_true)\n",
    "    #y_pred = tf.keras.backend.eval(y_pred)\n",
    "    \n",
    "    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    prec_macro = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    prec_weighted = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    prec_micro = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    recall_macro = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_micro = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    try:\n",
    "        roc_auc_weighted = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "        roc_auc_macro = roc_auc_score(y_true, probs, average = 'macro')\n",
    "        roc_auc_micro = roc_auc_score(y_true, probs, average = 'micro')\n",
    "    except ValueError:\n",
    "        roc_auc_weighted = None\n",
    "        roc_auc_macro = None\n",
    "        roc_auc_micro = None\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_macro': f1_macro,\n",
    "               'f1_weighted': f1_weighted,\n",
    "               'f1_micro': f1_micro,\n",
    "               'prec_macro': prec_macro,\n",
    "               'prec_weighted': prec_weighted,\n",
    "               'prec_micro': prec_micro,\n",
    "               'recall_macro': recall_macro,\n",
    "               'recall_weighted': recall_weighted,\n",
    "               'recall_micro': recall_micro,\n",
    "               'roc_auc_macro': roc_auc_macro,\n",
    "               'roc_auc_weighted': roc_auc_weighted,\n",
    "               'roc_auc_micro': roc_auc_micro,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "     \n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ecd0b0ba39bca6be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:50.438228Z",
     "start_time": "2024-05-10T10:39:50.021707Z"
    }
   },
   "source": [
    "# inference pipe\n",
    "def tokenize_and_pad(x, tokenizer, maxlen = 256):\n",
    "    toks = tokenizer.texts_to_sequences(x)\n",
    "    toks_padded = pad_sequences(toks, padding = 'post', maxlen = maxlen)\n",
    "    return toks_padded\n",
    "\n",
    "def evaluate_model(clf, labels):\n",
    "    probas = clf.predict(X_test)\n",
    "    cf_matrix = confusion_matrix(np.argmax(labels, axis=1),\n",
    "                                 np.argmax(probas, axis=1))\n",
    "    return cf_matrix\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "3331f26fd0c509f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:50.872205Z",
     "start_time": "2024-05-10T10:39:50.441197Z"
    }
   },
   "source": [
    "# Stratified cross-validation\n",
    "def fold_indices(targets: pd.Series=None, stratified: bool=True, seed: int=42, numfolds: int=10)->Tuple[List,List]:\n",
    "    if stratified:\n",
    "        splitter = StratifiedKFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = targets\n",
    "    else:\n",
    "        splitter = KFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = None\n",
    "\n",
    "    train_indcs, test_indcs = [], []\n",
    "    for train_index, test_index in splitter.split(X=targets, y=_Targets):\n",
    "        train_indcs.append(train_index)\n",
    "        test_indcs.append(test_index)\n",
    "\n",
    "    return zip(train_indcs, test_indcs)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a2ce3cf91202770e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:51.321713Z",
     "start_time": "2024-05-10T10:39:50.874203Z"
    }
   },
   "source": [
    "class CustomMetrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super(CustomMetrics, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_predict = self.model.predict(self.validation_data[0])\n",
    "        val_targ = self.validation_data[1]\n",
    "\n",
    "        _val_metrics = multi_label_metrics(val_targ, val_predict)\n",
    "        for name, value in _val_metrics.items():\n",
    "            logs['val_' + name] = value\n",
    "\n",
    "        print(\" — val_f1_macro: %f — val_accuracy: %f\" % (logs['val_f1_macro'], logs['val_accuracy']))\n",
    "        return"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "e4c02977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:51.786748Z",
     "start_time": "2024-05-10T10:39:51.325211Z"
    }
   },
   "source": [
    "def CurrentModel(modelselection: Literal['bigru', 'bilstm', 'cnn'], \n",
    "                 embeddingdim: int=128, \n",
    "                 maxlen: int=256, \n",
    "                 vocabsize: int=50_000,\n",
    "                 numclasses: int=5,\n",
    "                 learningrate: float=0.001,\n",
    "                 num_layers: int=128,\n",
    "                 pre_trained_vectors: bool=False,\n",
    "                 multilabel: bool=False,\n",
    "                 dilation=4):\n",
    "    model = Sequential()\n",
    "    accuracy = 'binary_accuracy'\n",
    "    if multilabel:\n",
    "        finalact = 'sigmoid'\n",
    "        lossfunction = 'binary_crossentropy'\n",
    "    else:\n",
    "        finalact = 'softmax'\n",
    "        lossfunction = 'categorical_crossentropy'\n",
    "        \n",
    "    if pre_trained_vectors:\n",
    "        num_dense = 20\n",
    "    else:\n",
    "        num_dense = 10\n",
    "    if modelselection =='bigru':\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Bidirectional(layers.GRU(num_layers), input_shape=(maxlen, embeddingdim)))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length=maxlen))\n",
    "            model.add(layers.Bidirectional(layers.GRU(num_layers)))\n",
    "        model.add(layers.Dense(num_dense, activation = 'relu'))\n",
    "        model.add(layers.Dense(numclasses, activation = finalact))\n",
    "        model.compile(optimizer = optimizers.Adam(learning_rate=learningrate),\n",
    "                      loss = lossfunction, \n",
    "                      metrics = [accuracy])\n",
    "    elif modelselection =='bilstm':\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Bidirectional(layers.LSTM(num_layers), input_shape=(maxlen, embeddingdim)))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length=maxlen))\n",
    "            model.add(layers.Bidirectional(layers.LSTM(num_layers)))\n",
    "        model.add(layers.Dense(num_dense, activation = 'relu'))\n",
    "        model.add(layers.Dense(numclasses, activation = finalact))\n",
    "        model.compile(optimizer = optimizers.Adam(learning_rate=learningrate),\n",
    "                      loss = lossfunction,\n",
    "                      metrics = [accuracy])\n",
    "    elif modelselection =='cnn':\n",
    "        if pre_trained_vectors:\n",
    "            model.add(layers.Conv1D(num_layers, 5, activation = 'relu', dilation_rate = dilation, input_shape = (maxlen, embeddingdim)))\n",
    "        else:\n",
    "            model.add(layers.Embedding(vocabsize, embeddingdim, input_length = maxlen))\n",
    "            model.add(layers.Conv1D(num_layers, 5, activation = 'relu', dilation_rate = dilation))\n",
    "        model.add(layers.GlobalMaxPooling1D())\n",
    "        model.add(layers.Dense(num_dense, activation = 'relu'))\n",
    "        model.add(layers.Dense(numclasses, activation = finalact))\n",
    "        model.compile(optimizer = optimizers.Adam(learning_rate=learningrate),\n",
    "                      loss = lossfunction,\n",
    "                      metrics = [accuracy])\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "0c8d3cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:52.205305Z",
     "start_time": "2024-05-10T10:39:51.788720Z"
    }
   },
   "source": [
    "def run_model_pipe(x: list, model: Sequential, maxlen: int=256, tokenizer=None):\n",
    "    x_tok = tokenize_and_pad(x, tokenizer=tokenizer, maxlen=maxlen)\n",
    "    return model.predict(x_tok)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:52.623749Z",
     "start_time": "2024-05-10T10:39:52.207737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # labels should be a list or a 1D tensor\n",
    "    return keras_utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "def _mlabel_tuple_creator(x: List[int],\n",
    "                          multilabels:Dict[int,List[int]])\\\n",
    "                          ->List[Tuple[int,...]]:\n",
    "                              \n",
    "    res = [(_sc for _sc in multilabels[sc]) for sc in x]\n",
    "    return res\n",
    "\n",
    "def multi_hot_encoding(x: List[int], \n",
    "                       multilabels: Union[Dict[int,List[int]], None]=None,\n",
    "                       num_classes: int=None)\\\n",
    "                           ->np.array:    \n",
    "    if multilabels is None:\n",
    "        return one_hot_encode(x, num_classes=num_classes)\n",
    "    else:\n",
    "        return MultiLabelBinarizer(classes=range(num_classes))\\\n",
    "                    .fit_transform(_mlabel_tuple_creator(x,multilabels))"
   ],
   "id": "4606a13c3ffd81a6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "3d471e0819f26480",
   "metadata": {},
   "source": " # Load data"
  },
  {
   "cell_type": "code",
   "id": "9d0028d86cdb445f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:53.166254Z",
     "start_time": "2024-05-10T10:39:52.627238Z"
    }
   },
   "source": [
    "labeled_documents = pd.read_json(f\"./echo_doc_labels/{Class}.jsonl\", lines=True)\n",
    "label_col = 'label' if Class!='merged_labels' else 'labels'\n",
    "target_df = pd.DataFrame.from_records(labeled_documents[label_col])"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7581d300fbdc685b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:53.569259Z",
     "start_time": "2024-05-10T10:39:53.168751Z"
    }
   },
   "source": [
    "labeled_documents.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "91a201c2411f947b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:54.220285Z",
     "start_time": "2024-05-10T10:39:53.572278Z"
    }
   },
   "source": [
    "num_tokens = labeled_documents.text.apply(lambda x: len(x.split(\" \")))\n",
    "plt.hist(num_tokens, bins=30);"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "49a00fc68e98cfdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:55.817311Z",
     "start_time": "2024-05-10T10:39:54.222754Z"
    }
   },
   "source": [
    "_set = set()\n",
    "for __set in labeled_documents.text.apply(lambda x: set(x.split(\" \"))).values:\n",
    "    _set = _set.union(__set)\n",
    "print(f\"Number of unique tokens: {len(_set)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 17106\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "2bd53d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:56.514791Z",
     "start_time": "2024-05-10T10:39:55.819777Z"
    }
   },
   "source": [
    "# load train/test hashes\n",
    "#hash_docs_link = pd.read_json(\"./echo_span_labels/reduced_labels/merged_labels.jsonl\", lines=True)[['text', '_input_hash']]\n",
    "train_ids = pd.read_csv('./train_echoid.csv', sep=',').input_hash.unique()\n",
    "test_ids = pd.read_csv('./test_echoid.csv', sep=',').input_hash.unique()\n",
    "\n",
    "labeled_documents['_hash'] = labeled_documents.text.str.strip().apply(lambda x: hash(x))\n",
    "#hash_docs_link['_hash'] = hash_docs_link.text.str.strip().apply(lambda x: hash(x))\n",
    "\n",
    "#labeled_documents = labeled_documents.merge(hash_docs_link[['_input_hash', '_hash']], \n",
    "#                                            on='_hash', how='inner')\n",
    "\n",
    "labeled_documents = labeled_documents.drop_duplicates(subset=['_hash']).reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "6632439635dd361f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:56.979827Z",
     "start_time": "2024-05-10T10:39:56.516792Z"
    }
   },
   "source": [
    "# Expand with label columns\n",
    "if Class == 'merged_labels':\n",
    "    Target_maps = {\n",
    "        _Class: {Label:i for i,Label in enumerate(target_df[Class].unique())}\n",
    "        for _Class in target_df.columns\n",
    "    }\n",
    "else:\n",
    "    Target_maps = {\n",
    "        Class: {Label: i for i,Label in enumerate(labeled_documents['label'].unique())} \n",
    "    }\n",
    "    \n",
    "if Class == 'merged_labels':\n",
    "    DF = labeled_documents[['text', '_input_hash']].join(target_df[Class])\n",
    "else:\n",
    "    DF = labeled_documents[['text', '_input_hash', 'label']]\n",
    "\n",
    "DF.columns = ['sentence', '_input_hash', 'labels']\n",
    "\n",
    "label2id = Target_maps[Class]\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "if use_multilabel:\n",
    "    _multilabels = {label2id.get(k): [label2id.get(l) for l in v if label2id.get(l) is not None]\n",
    "                    for k,v in MULTILABELS.items() if label2id.get(k) is not None}\n",
    "else:\n",
    "    _multilabels = None\n",
    "\n",
    "DF= DF.assign(label=DF['labels'].map(label2id))"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:57.413918Z",
     "start_time": "2024-05-10T10:39:56.982307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if use_multilabel:\n",
    "    if all([len(v)==1 for v in _multilabels.values()]):\n",
    "        use_multilabel = False\n",
    "        _multilabels = None"
   ],
   "id": "6fa41c34d45cc27e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "134425514247d5e6",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cd611ac6ba3027e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:39:57.832092Z",
     "start_time": "2024-05-10T10:39:57.416305Z"
    }
   },
   "source": [
    "DF = DF.assign(sentence=DF.sentence.str.replace(r'[\\r\\n]', '', regex=True)) "
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "ef0e8d7af868e41f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:40:07.597063Z",
     "start_time": "2024-05-10T10:39:57.837383Z"
    }
   },
   "source": [
    "if lemmatize:\n",
    "    docs = nlp.pipe(DF.sentence.values)\n",
    "    new_texts = [\" \".join([token.lemma_ for token in doc]) for doc in docs] \n",
    "    DF = DF.assign(sentence = new_texts)\n",
    "\n",
    "if lowercase:\n",
    "    DF = DF.assign(sentence = DF.sentence.str.lower())\n",
    "    \n",
    "if filter_reports:\n",
    "    DF = DF.assign(sentence = echo_utils.report_filter(DF.sentence, \n",
    "                                            flag_terms=FLAG_TERMS, \n",
    "                                            save_terms=SAVE_TERMS)[0])\n",
    "    DF = DF.loc[DF.sentence.notna()]\n",
    "\n",
    "if deabbreviate:\n",
    "    DeAbber = deabber.deabber(model_type='sbert', abbreviations=ABBREVIATIONS['nl']['echocardiogram'], min_sim=0.5, top_k=10)\n",
    "    DF = DF.assign(sentence=DeAbber.deabb(DF.sentence.values, TokenRadius=3))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "a3c68f12e14c3d84",
   "metadata": {},
   "source": [
    "# Make folds"
   ]
  },
  {
   "cell_type": "code",
   "id": "83203825517c00c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:40:08.139571Z",
     "start_time": "2024-05-10T10:40:07.600037Z"
    }
   },
   "source": [
    "# prepping\n",
    "TrainTestDict = defaultdict(dict)\n",
    "\n",
    "if Splitting == 'from_file':\n",
    "    TrainTestDict[0]['Xtrain'] = DF.loc[DF._input_hash.isin(train_ids)].sentence\n",
    "    TrainTestDict[0]['Xtest'] = DF.loc[DF._input_hash.isin(test_ids)].sentence\n",
    "    \n",
    "    TrainTestDict[0]['ytrain'] = DF.loc[DF._input_hash.isin(train_ids)].label\n",
    "    TrainTestDict[0]['ytest'] = DF.loc[DF._input_hash.isin(test_ids)].label\n",
    "    \n",
    "elif Splitting == 'CV':\n",
    "    for k,(train_index, test_index) in enumerate(fold_indices(targets=DF['label'], stratified=True)):\n",
    "        TrainTestDict[k]['Xtrain'] = DF.iloc[train_index].sentence\n",
    "        TrainTestDict[k]['Xtest'] = DF.iloc[test_index].sentence\n",
    "        \n",
    "        TrainTestDict[k]['ytrain'] = DF.iloc[train_index].label\n",
    "        TrainTestDict[k]['ytest'] = DF.iloc[test_index].label\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "87a4b47e71c1ca13",
   "metadata": {},
   "source": [
    "## Initiate models"
   ]
  },
  {
   "cell_type": "code",
   "id": "a28bb238849448ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:40:08.604561Z",
     "start_time": "2024-05-10T10:40:08.142040Z"
    }
   },
   "source": [
    "if pre_trained_embeddings:\n",
    "    embedding_dim = nlp.vocab.vectors_length"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "664871e9df2dde6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:40:10.201112Z",
     "start_time": "2024-05-10T10:40:08.607071Z"
    }
   },
   "source": [
    "ModelDict = defaultdict(dict)\n",
    "for k,v in TrainTestDict.items():\n",
    "    if pre_trained_embeddings:\n",
    "        LSTM_Tokenizer = None\n",
    "        vocab_size = None\n",
    "    else:\n",
    "        LSTM_Tokenizer = KerasTokenizer(num_words=num_words_in_vocab)\n",
    "        LSTM_Tokenizer.fit_on_texts(v['Xtrain'])\n",
    "        vocab_size = len(LSTM_Tokenizer.word_index)+1\n",
    "\n",
    "    num_classes = np.unique(v['ytrain']).shape[0]\n",
    "    \n",
    "    ModelDict[k]['tokenizer'] = LSTM_Tokenizer    \n",
    "    if use_multilabel:\n",
    "        ModelDict[k]['warmup_model'] = CurrentModel(modelselection=ModelType, \n",
    "                                     embeddingdim=embedding_dim,\n",
    "                                     maxlen=max_len,\n",
    "                                     vocabsize=vocab_size,\n",
    "                                     numclasses=num_classes,\n",
    "                                     learningrate=LR,\n",
    "                                     dilation=dilation,\n",
    "                                     num_layers=num_layers,\n",
    "                                     multilabel=False,\n",
    "                                     pre_trained_vectors=pre_trained_embeddings\n",
    "                                     )\n",
    "    else:\n",
    "        ModelDict[k]['model'] = CurrentModel(modelselection=ModelType, \n",
    "                                     embeddingdim=embedding_dim,\n",
    "                                     maxlen=max_len,\n",
    "                                     vocabsize=vocab_size,\n",
    "                                     numclasses=num_classes,\n",
    "                                     learningrate=LR,\n",
    "                                     dilation=dilation,\n",
    "                                     num_layers=num_layers,\n",
    "                                     multilabel=use_multilabel,\n",
    "                                     pre_trained_vectors=pre_trained_embeddings\n",
    "                                         )\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Warmup models\n",
    "\n",
    "If we are running a multilabel classification with BCE loss we might get stuck in a local minimum where recall is 1 for all\n",
    "classes. To avoid this we pre-run the model with categorical cross-entropy loss, with a softmax in the final layer. To be able to apply\n",
    "the CCEloss we want 1 label per sample, we force by taking the minority class in each multilabel instance."
   ],
   "id": "d89bc82e1f567223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:40:10.712612Z",
     "start_time": "2024-05-10T10:40:10.203594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_multilabel(Model: Sequential, learningrate: float=1e-4) -> Sequential:\n",
    "    \"\"\"\n",
    "    Make a multilabel classification model, given a pre-trained Sequential model.\n",
    "    It replaces the last Dense layer with 'softmax' activation to a 'sigmoid' activation,\n",
    "    making it suitable for multilabel classification.\n",
    "\n",
    "    :param Model: A pre-trained Keras Sequential model.\n",
    "    :return: A modified Sequential model ready for multilabel classification.\n",
    "    \"\"\"\n",
    "    assert isinstance(Model, Sequential), \"Model must be a Keras Sequential model.\"\n",
    "\n",
    "    # Create a new Sequential model\n",
    "    new_model = Sequential()\n",
    "\n",
    "    # Copy all layers except the last one from the original model to the new model\n",
    "    for layer in Model.layers[:-1]:\n",
    "        new_model.add(layer)\n",
    "\n",
    "    # Assume the last layer is a Dense layer and get the number of units from it\n",
    "    num_classes = Model.layers[-1].output_shape[-1]\n",
    "\n",
    "    # Add a new Dense layer with sigmoid activation suited for multilabel classification\n",
    "    new_model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    # Important: this line copies the weights of all layers except the last one from the original model\n",
    "    for layer, new_layer in zip(Model.layers[:-1], new_model.layers[:-1]):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "    new_model.compile(optimizer = optimizers.Adam(learning_rate=learningrate),\n",
    "                      loss = 'binary_crossentropy',\n",
    "                      metrics = ['binary_accuracy'])\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "# def make_multilabel(model: Sequential):\n",
    "#     '''\n",
    "#     Make a multilabel classification model, given a pre-trained Sequential model.\n",
    "#     Actions:\n",
    "#      1. Replace layers.Dense(numclasses, activation = 'softmax') with\n",
    "#         layers.Dense(numclasses, activation = 'sigmoid')\n",
    "#      2. Replace the loss function 'categorical_crossentropy' with 'binary_crossentropy'.\n",
    "# \n",
    "#     :param model: A pre-trained Keras Sequential model.\n",
    "#     :return: Sequential model with pretrained weights, adapted for multi-label classification.\n",
    "#     '''\n",
    "# \n",
    "#     assert isinstance(model, Sequential), \"Multilabel model must be Sequential\"\n",
    "# \n",
    "#     # Modify the output layer\n",
    "#     for layer in model.layers:\n",
    "#         if isinstance(layer, Dense) and layer.activation == tf.keras.activations.softmax:\n",
    "#             num_classes = layer.units  # Preserve number of classes\n",
    "#             model.pop()  # Remove old output layer\n",
    "#             model.add(Dense(num_classes, activation='sigmoid'))\n",
    "#             break  # Assuming only one softmax output layer\n",
    "# \n",
    "#     # Replace loss function (assuming 'categorical_crossentropy' was used originally)\n",
    "#     model.compile(optimizer=model.optimizer,  # Preserve the optimizer\n",
    "#                   loss='binary_crossentropy', \n",
    "#                   metrics=['accuracy'])  # Or other suitable metrics\n",
    "# \n",
    "#     return model"
   ],
   "id": "415ea56d0aed8b22",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:40:11.162103Z",
     "start_time": "2024-05-10T10:40:10.715095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WarmupDict = defaultdict(lambda: defaultdict(list))\n",
    "if use_multilabel:   \n",
    "    # run with softmax and CCEloss\n",
    "    for k,v in ModelDict.items():\n",
    "        _model =  v['warmup_model']\n",
    "        _tokenizer = v['tokenizer']\n",
    "        \n",
    "        if pre_trained_embeddings:\n",
    "            X_train = echo_utils.text_to_vectors(TrainTestDict[k]['Xtrain'], max_len) \n",
    "            X_test = echo_utils.text_to_vectors(TrainTestDict[k]['Xtest'], max_len) \n",
    "        else:\n",
    "            X_train = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtrain'])\n",
    "            X_test = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtest'])\n",
    "        \n",
    "            X_train = pad_sequences(X_train, padding = 'post', maxlen=max_len)\n",
    "            X_test = pad_sequences(X_test, padding = 'post', maxlen=max_len)\n",
    "    \n",
    "        _y = TrainTestDict[k]['ytrain']\n",
    "        y_train = multi_hot_encoding(_y, multilabels=None, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "    \n",
    "        _y = TrainTestDict[k]['ytest']\n",
    "        y_test = multi_hot_encoding(_y, multilabels=None, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "    \n",
    "        if UseClassWeights:\n",
    "            PosSum = 1/np.sum(y_train, axis=0)\n",
    "            ClassWeights = dict(enumerate(PosSum/np.min(PosSum)))\n",
    "        else:\n",
    "            ClassWeights = None\n",
    "        \n",
    "        result_list = []\n",
    "        history_list = []\n",
    "        confusion_list = []\n",
    "        for epoch_num in range(1, warmup_steps+1):\n",
    "            _history = _model.fit(X_train,\n",
    "                                 y_train,                         \n",
    "                                 epochs = 1,\n",
    "                                 verbose = False,\n",
    "                                 validation_split = 0,\n",
    "                                 class_weight=ClassWeights,\n",
    "                                 batch_size = batch_size)\n",
    "        \n",
    "            y_pred = _model.predict(X_test, verbose=False)\n",
    "            _res = multi_label_metrics(labels=y_test, probs=y_pred)\n",
    "            \n",
    "            result_list.append(_res)\n",
    "            history_list.append({'loss': _history.history['loss'][0], \n",
    "                                 'accuracy': _history.history['binary_accuracy'][0]})\n",
    "            confusion_list.append(confusion_matrix(np.argmax(y_test, axis=1),\n",
    "                                                   np.argmax(np.round(y_pred),axis=1),  normalize='true'))\n",
    "            \n",
    "            \n",
    "            print(f\"Epoch {epoch_num}, F1 Macro={_res['f1_macro']}\")\n",
    "            \n",
    "        WarmupDict[k]['history'] = pd.DataFrame(history_list)\n",
    "        WarmupDict[k]['result'] = pd.DataFrame(result_list)\n",
    "        WarmupDict[k]['confusion'] = confusion_list\n",
    "        \n",
    "        ModelDict[k]['model'] = make_multilabel(_model, learningrate=warmup_lr)\n",
    "         \n",
    "    "
   ],
   "id": "9549dae54c0f97f4",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:40:11.596241Z",
     "start_time": "2024-05-10T10:40:11.164589Z"
    }
   },
   "cell_type": "code",
   "source": "id2label",
   "id": "e98939a8a987ded8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Normal', 1: 'Mild', 2: 'Moderate', 3: 'No label', 4: 'Severe'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "26a30b4c2d9bfaf3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T10:40:12.061139Z",
     "start_time": "2024-05-10T10:40:11.598593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if use_multilabel:\n",
    "    _confusion_matrix = multilabel_confusion_matrix\n",
    "else:\n",
    "    _confusion_matrix = confusion_matrix"
   ],
   "id": "dc9ef4a32da8a35a",
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "a15851917a011494",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-10T10:40:12.064134Z"
    }
   },
   "source": [
    "ResultDict = defaultdict(lambda: defaultdict(list))\n",
    "for k,v in ModelDict.items():\n",
    "    _model =  v['model']\n",
    "    _tokenizer = v['tokenizer']\n",
    "    \n",
    "    if pre_trained_embeddings:\n",
    "        X_train = echo_utils.text_to_vectors(TrainTestDict[k]['Xtrain'], max_len) \n",
    "        X_test = echo_utils.text_to_vectors(TrainTestDict[k]['Xtest'], max_len) \n",
    "    else:\n",
    "        X_train = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtrain'])\n",
    "        X_test = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtest'])\n",
    "    \n",
    "        X_train = pad_sequences(X_train, padding = 'post', maxlen=max_len)\n",
    "        X_test = pad_sequences(X_test, padding = 'post', maxlen=max_len)\n",
    "\n",
    "    _y = TrainTestDict[k]['ytrain']\n",
    "    y_train = multi_hot_encoding(_y, multilabels=_multilabels, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "\n",
    "    _y = TrainTestDict[k]['ytest']\n",
    "    y_test = multi_hot_encoding(_y, multilabels=_multilabels, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "\n",
    "    if UseClassWeights:\n",
    "        PosSum = 1/np.sum(y_train, axis=0)\n",
    "        ClassWeights = dict(enumerate(PosSum/np.min(PosSum)))\n",
    "    else:\n",
    "        ClassWeights = None\n",
    "    \n",
    "    result_list = []\n",
    "    history_list = []\n",
    "    confusion_list = []\n",
    "    for epoch_num in range(1, num_epochs+1):\n",
    "        _history = _model.fit(X_train,\n",
    "                             y_train,                         \n",
    "                             epochs = 1,\n",
    "                             verbose = False,\n",
    "                             validation_split = 0,\n",
    "                             class_weight=ClassWeights,\n",
    "                             batch_size = batch_size)\n",
    "    \n",
    "        y_pred = _model.predict(X_test, verbose=False)\n",
    "        _res = multi_label_metrics(labels=y_test, probs=y_pred)\n",
    "        \n",
    "        result_list.append(_res)\n",
    "        history_list.append({'loss': _history.history['loss'][0], \n",
    "                             'accuracy': _history.history['binary_accuracy'][0]})\n",
    "        confusion_list.append(_confusion_matrix(np.argmax(y_test, axis=1),\n",
    "                                               np.argmax(np.round(y_pred),axis=1)))\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch_num}, F1 Macro={_res['f1_macro']}\")\n",
    "        \n",
    "    ResultDict[k]['history'] = pd.DataFrame(history_list)\n",
    "    ResultDict[k]['result'] = pd.DataFrame(result_list)\n",
    "    ResultDict[k]['confusion'] = confusion_list\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, F1 Macro=0.3192884449096619\n",
      "Epoch 2, F1 Macro=0.7078458171992844\n",
      "Epoch 3, F1 Macro=0.7696109712761521\n",
      "Epoch 4, F1 Macro=0.8242771968362895\n",
      "Epoch 5, F1 Macro=0.8458912337321772\n",
      "Epoch 6, F1 Macro=0.8692363993117567\n",
      "Epoch 7, F1 Macro=0.8775144490368147\n",
      "Epoch 8, F1 Macro=0.8681410524935999\n",
      "Epoch 9, F1 Macro=0.8636214567474745\n",
      "Epoch 10, F1 Macro=0.8728628295576965\n",
      "Epoch 11, F1 Macro=0.8749515704686999\n",
      "Epoch 12, F1 Macro=0.8831678795849458\n",
      "Epoch 13, F1 Macro=0.8761029975507846\n",
      "Epoch 14, F1 Macro=0.8804087035045607\n",
      "Epoch 15, F1 Macro=0.8776954054822277\n",
      "Epoch 16, F1 Macro=0.8775196529619746\n",
      "Epoch 17, F1 Macro=0.8760611079495352\n",
      "Epoch 18, F1 Macro=0.8683249232119351\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "confusion_list[-1]",
   "id": "369d94430d64ea0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "caace1698b3b2400",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "heatmap_df = \\\n",
    "    pd.DataFrame(\n",
    "    confusion_list[-1],\n",
    "        columns=label2id.keys(),\n",
    "        index=label2id.keys()\n",
    "    )\n",
    "sns.heatmap(heatmap_df.round(4), annot=True, cmap='Blues', fmt='g')\n",
    "    # df.style.background_gradient(cmap='Blues')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b7cad5ab1538d3",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "MultiLabelStr = 'multilabel' if use_multilabel else 'categorical'\n",
    "ResultDict[k]['result'].to_csv(f\"../../G_Output/2_Data/results_{MultiLabelStr}{Class}_{ModelType}_{Splitting}.csv\", sep=\";\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e627eadb8f623ba",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a77e2c88cef599d3"
  },
  {
   "cell_type": "code",
   "id": "1bfb9a2f9db3a0d4",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "plt.plot(ResultDict[k]['result']['f1_macro'], label='F1 Macro')\n",
    "plt.plot(ResultDict[k]['result']['prec_macro'], label='Precision Macro')\n",
    "plt.plot(ResultDict[k]['result']['recall_macro'], label='Recall Macro')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ead311ab73fa4d25",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "plt.plot(ResultDict[k]['history'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c27d70a8f816b3d9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "ResultDict[k]['result']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de0495325550a5c7",
   "metadata": {},
   "source": [
    "## Upload to huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2452b646be1643b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nlp_310)",
   "language": "python",
   "name": "python3_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
