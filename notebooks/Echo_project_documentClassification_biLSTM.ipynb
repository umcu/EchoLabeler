{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:12.449044Z",
     "start_time": "2024-05-13T21:39:10.684553Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import benedict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a24a9f0e29e94",
   "metadata": {},
   "source": [
    "For multilabel the BCE loss collapses to a positive label for all classes. A better way to start is to use categorical cross-entropy initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd8f28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:24.445380Z",
     "start_time": "2024-05-13T21:39:12.452547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers, models, utils as keras_utils\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import echo_models, echo_utils\n",
    "\n",
    "import spacy\n",
    "import re \n",
    "import dotenv\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "\n",
    "from typing import Callable, Tuple, Dict, List, Literal, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1461462f259a5bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:24.755024Z",
     "start_time": "2024-05-13T21:39:24.448224Z"
    }
   },
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(\"../.env\")\n",
    "EmbeddingPath = os.environ['WORD_EMBEDDINGS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efa158612b8bcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:25.096034Z",
     "start_time": "2024-05-13T21:39:24.759531Z"
    }
   },
   "outputs": [],
   "source": [
    "#  settings\n",
    "embedding_dim = 300\n",
    "max_len = 200\n",
    "num_words_in_vocab = 5_000\n",
    "num_epochs = 20\n",
    "warmup_steps = 40 # only relevant for multilabel problems\n",
    "warmup_lr = 2e-3 # only relevant for multilabel problems\n",
    "\n",
    "num_folds = 10\n",
    "# multilabel: lv_dil, pe, rv_dil\n",
    "# categorical: aortic_regurgitation, lv_syst_func, rv_syst_func, aortic_stenosis, diastolic_dysfunction, mitral_regurgitation, tricuspid_regurgitation,wma\n",
    "\n",
    "Class = 'lv_syst_func' \n",
    "batch_size = 128\n",
    "ModelType = 'bilstm' # bigru, bilstm, cnn, hstacked_dcnn, textcnn\n",
    "ProcessorType = \"cpu\" if len(tf.config.experimental.list_physical_devices(\"GPU\"))==0 else \"gpu\"\n",
    "Splitting = 'from_file' # CV or from_file\n",
    "UseClassWeights = False\n",
    "LR = 0.0025\n",
    "dilation=1\n",
    "num_layers = 32\n",
    "dropout=0.2\n",
    "lemmatize = True\n",
    "lowercase = False\n",
    "pre_trained_embeddings = True\n",
    "deabbreviate = False\n",
    "typo_removal = False # TODO: remove typos\n",
    "filter_reports = True\n",
    "use_multilabel = False\n",
    "EmbeddingSource = 'cardio_wv'\n",
    "\n",
    "FLAG_TERMS = ['uitslag zie medische status', 'zie status', 'zie verslag status', 'slecht echovenster', 'echo overwegen', 'ge echo',\n",
    "              'geen echovenster', 'geen beoordeelbaar echo', 'geen beoordeelbare echo', 'verslag op ic']\n",
    "SAVE_TERMS = ['goed', 'geen', 'normaal', 'normale']\n",
    "MULTILABELS = {'Mild': ['Mild', 'Present'], \n",
    "               'Severe': ['Severe', 'Present'],\n",
    "               'Moderate': ['Moderate', 'Present'],\n",
    "               'Normal': ['Normal'],\n",
    "               'No label': ['No label'],\n",
    "               'Present': ['Present'],\n",
    "               }\n",
    "\n",
    "reduce_labels = False\n",
    "REDUCED_LABELMAP = {\n",
    "    'Present': 'Present',\n",
    "    'No label': 'No label',\n",
    "    'Normal': 'Normal',\n",
    "    'Moderate': 'Present',\n",
    "    'Severe': 'Present',\n",
    "    'Mild': 'Present'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b120d5960b8d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:33.278754Z",
     "start_time": "2024-05-13T21:39:25.099035Z"
    }
   },
   "outputs": [],
   "source": [
    "if deabbreviate | typo_removal | filter_reports:\n",
    "    import deabber, echo_utils\n",
    "    ABBREVIATIONS = benedict.benedict(\"../assets/abbreviations.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef30961852feec0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:33.728235Z",
     "start_time": "2024-05-13T21:39:33.281740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aortic_regurgitation.jsonl',\n",
       " 'aortic_stenosis.jsonl',\n",
       " 'diastolic_dysfunction.jsonl',\n",
       " 'lv_dil.jsonl',\n",
       " 'lv_syst_func.jsonl',\n",
       " 'merged_labels.jsonl',\n",
       " 'mitral_regurgitation.jsonl',\n",
       " 'old',\n",
       " 'pe.jsonl',\n",
       " 'rv_dil.jsonl',\n",
       " 'rv_syst_func.jsonl',\n",
       " 'tricuspid_regurgitation.jsonl',\n",
       " 'wma.jsonl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData')\n",
    "os.listdir(\"./echo_doc_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a824afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:34.177734Z",
     "start_time": "2024-05-13T21:39:33.730230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(ProcessorType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353550069f4de85f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:37.587777Z",
     "start_time": "2024-05-13T21:39:34.179738Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "if lemmatize:\n",
    "    nlp = spacy.load(\"nl_core_news_lg\", disable = ['parser','ner'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e4a0adc5cce0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:38.037287Z",
     "start_time": "2024-05-13T21:39:37.590278Z"
    }
   },
   "outputs": [],
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(probs, labels, threshold=0.5):\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    \n",
    "    y_true = labels\n",
    "    #y_true = tf.keras.backend.eval(y_true)\n",
    "    #y_pred = tf.keras.backend.eval(y_pred)\n",
    "    \n",
    "    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    prec_macro = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    prec_weighted = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    prec_micro = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    recall_macro = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_micro = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    try:\n",
    "        roc_auc_weighted = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "        roc_auc_macro = roc_auc_score(y_true, probs, average = 'macro')\n",
    "        roc_auc_micro = roc_auc_score(y_true, probs, average = 'micro')\n",
    "    except ValueError:\n",
    "        roc_auc_weighted = None\n",
    "        roc_auc_macro = None\n",
    "        roc_auc_micro = None\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_macro': f1_macro,\n",
    "               'f1_weighted': f1_weighted,\n",
    "               'prec_macro': prec_macro,\n",
    "               'prec_weighted': prec_weighted,\n",
    "               'recall_macro': recall_macro,\n",
    "               'recall_weighted': recall_weighted,\n",
    "               'roc_auc_macro': roc_auc_macro,\n",
    "               'roc_auc_weighted': roc_auc_weighted,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd0b0ba39bca6be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:38.517807Z",
     "start_time": "2024-05-13T21:39:38.042290Z"
    }
   },
   "outputs": [],
   "source": [
    "# inference pipe\n",
    "def tokenize_and_pad(x, tokenizer, maxlen = 256):\n",
    "    toks = tokenizer.texts_to_sequences(x)\n",
    "    toks_padded = pad_sequences(toks, padding = 'post', maxlen = maxlen)\n",
    "    return toks_padded\n",
    "\n",
    "def evaluate_model(clf, labels):\n",
    "    probas = clf.predict(X_test)\n",
    "    cf_matrix = confusion_matrix(np.argmax(labels, axis=1),\n",
    "                                 np.argmax(probas, axis=1))\n",
    "    return cf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3331f26fd0c509f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:39.044794Z",
     "start_time": "2024-05-13T21:39:38.519791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stratified cross-validation\n",
    "def fold_indices(targets: pd.Series=None, stratified: bool=True, seed: int=42, numfolds: int=10)->Tuple[List,List]:\n",
    "    if stratified:\n",
    "        splitter = StratifiedKFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = targets\n",
    "    else:\n",
    "        splitter = KFold(n_splits=numfolds, shuffle=True, random_state=seed)\n",
    "        _Targets = None\n",
    "\n",
    "    train_indcs, test_indcs = [], []\n",
    "    for train_index, test_index in splitter.split(X=targets, y=_Targets):\n",
    "        train_indcs.append(train_index)\n",
    "        test_indcs.append(test_index)\n",
    "\n",
    "    return zip(train_indcs, test_indcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2ce3cf91202770e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:39.602801Z",
     "start_time": "2024-05-13T21:39:39.047296Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomMetrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super(CustomMetrics, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_predict = self.model.predict(self.validation_data[0])\n",
    "        val_targ = self.validation_data[1]\n",
    "\n",
    "        _val_metrics = multi_label_metrics(val_targ, val_predict)\n",
    "        for name, value in _val_metrics.items():\n",
    "            logs['val_' + name] = value\n",
    "\n",
    "        print(\" — val_f1_macro: %f — val_accuracy: %f\" % (logs['val_f1_macro'], logs['val_accuracy']))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8d3cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:40.579315Z",
     "start_time": "2024-05-13T21:39:40.056809Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model_pipe(x: list, model: Sequential, maxlen: int=256, tokenizer=None):\n",
    "    x_tok = tokenize_and_pad(x, tokenizer=tokenizer, maxlen=maxlen)\n",
    "    return model.predict(x_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4606a13c3ffd81a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:41.013321Z",
     "start_time": "2024-05-13T21:39:40.582328Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # labels should be a list or a 1D tensor\n",
    "    return keras_utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "def _mlabel_tuple_creator(x: List[int],\n",
    "                          multilabels:Dict[int,List[int]])\\\n",
    "                          ->List[Tuple[int,...]]:\n",
    "                              \n",
    "    res = [(_sc for _sc in multilabels[sc]) for sc in x]\n",
    "    return res\n",
    "\n",
    "def multi_hot_encoding(x: List[int], \n",
    "                       multilabels: Union[Dict[int,List[int]], None]=None,\n",
    "                       num_classes: int=None)\\\n",
    "                           ->np.array:    \n",
    "    if multilabels is None:\n",
    "        return one_hot_encode(x, num_classes=num_classes)\n",
    "    else:\n",
    "        return MultiLabelBinarizer(classes=range(num_classes))\\\n",
    "                    .fit_transform(_mlabel_tuple_creator(x,multilabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d471e0819f26480",
   "metadata": {},
   "source": [
    " # Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d0028d86cdb445f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:41.540327Z",
     "start_time": "2024-05-13T21:39:41.015820Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_documents = pd.read_json(f\"./echo_doc_labels/{Class}.jsonl\", lines=True)\n",
    "label_col = 'label' if Class!='merged_labels' else 'labels'\n",
    "target_df = pd.DataFrame.from_records(labeled_documents[label_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7581d300fbdc685b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:42.020836Z",
     "start_time": "2024-05-13T21:39:41.542830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91a201c2411f947b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:42.687340Z",
     "start_time": "2024-05-13T21:39:42.024333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAASdElEQVR4nO3df6xkZX3H8fe6V0n9UVeZZsPd3Qasq40lEZUAkdZQV61QymKjX7EGlnXL1gR/FRMFa4qx/QNblW7ShnRxqbsJCl9/hU1KVIoa26RQftTEKjVdcZFdl7teWdCEWtx1+sc8F2bXe9m5c+bOHO7zfiU395xnnpnzvWfmzmfOc37Mim63iySpTs+YdAGSpMkxBCSpYoaAJFXMEJCkihkCklSxqUkXcBweuiRJw1kxSKe2hwA/+tGPBu7b6XSYnZ1dwmqasb7htbk2sL4m2lwbtLu+hWqbnp4e+DEcDpKkihkCklQxQ0CSKmYISFLFDAFJqpghIEkVMwQkqWKGgCRVzBCQpIq1/ozhNjly2QUD9Vt5/e4lrkSSRsMtAUmqmCEgSRUzBCSpYoaAJFXMEJCkihkCklQxQ0CSKmYISFLFDAFJqpghIEkVMwQkqWLHvXZQRNwAnA8czMxTS9sLgZuBk4G9QGTmoYhYAWwDzgMeAy7NzHvLfTYBHy4P+9eZuXO0f4okabEG2RL4NPDGY9quBG7PzPXA7WUe4FxgffnZClwHT4TG1cCZwBnA1RHxgqbFS5KaOW4IZOY3gYePad4IzH2S3wlc2Ne+KzO7mXkHsCoiTgL+ALgtMx/OzEPAbfxqsEiSxmzYS0mvzswDZfohYHWZXgM82NdvX2lbqP1XRMRWelsRZCadTmfgoqamphbVf7FmBuy3UA1LXV9Tba6vzbWB9TXR5tqg3fWNorbG3yeQmd2I6DZ9nL7H2w5sL7Pd2dnZge/b6XRYTP+lslANbalvIW2ur821gfU10ebaoN31LVTb9PT0wI8x7NFBM2WYh/L7YGnfD6zr67e2tC3ULkmaoGFDYDewqUxvAm7pa78kIlZExFnAo2XY6CvAGyLiBWWH8BtKmyRpggY5RPSzwDlAJyL20TvK5xogI2IL8AAQpfut9A4P3UPvENHNAJn5cET8FXBX6ffRzDx2Z7MkacyOGwKZ+bYFbtowT98ucPkCj3MDcMOiqpMkLSnPGJakihkCklQxQ0CSKmYISFLFDAFJqpghIEkVMwQkqWKGgCRVzBCQpIoZApJUMUNAkipmCEhSxQwBSaqYISBJFTMEJKlihoAkVcwQkKSKGQKSVDFDQJIqZghIUsUMAUmqmCEgSRUzBCSpYoaAJFXMEJCkihkCklQxQ0CSKmYISFLFDAFJqthUkztHxJ8Dfwp0gW8Dm4GTgJuAE4F7gIsz8/GIOAHYBbwK+Anw1szc22T5kqRmht4SiIg1wHuA0zPzVGAlcBHwMeDazHwxcAjYUu6yBThU2q8t/SRJE9R0OGgK+LWImAKeDRwAXgt8vty+E7iwTG8s85TbN0TEiobLlyQ1MPRwUGbuj4iPAz8E/hf4Kr3hn0cy83Dptg9YU6bXAA+W+x6OiEfpDRnN9j9uRGwFtpZ+dDqdwf+YqalF9V+smQH7LVTDUtfXVJvra3NtYH1NtLk2aHd9o6ht6BCIiBfQ+3R/CvAI8DngjY2qATJzO7C9zHZnZ2efqvtROp0Oi+m/VGbe9OqB+q28fvcSV7I4bVl/82lzbWB9TbS5Nmh3fQvVNj09PfBjNBkOeh3wg8z8cWb+AvgicDawqgwPAawF9pfp/cA6gHL78+ntIJYkTUiTo4N+CJwVEc+mNxy0Abgb+DrwZnpHCG0Cbin9d5f5fy+3fy0zuw2WL0lqaOgtgcy8k94O3nvpHR76DHrDOB8EroiIPfTG/HeUu+wATiztVwBXNqhbkjQCjc4TyMyrgauPab4fOGOevj8H3tJkeZKk0fKMYUmqmCEgSRUzBCSpYoaAJFXMEJCkihkCklQxQ0CSKmYISFLFDAFJqpghIEkVMwQkqWKNrh20XBy57IJJlyBJE+GWgCRVzBCQpIoZApJUMUNAkipmCEhSxQwBSaqYISBJFTMEJKlihoAkVcwQkKSKGQKSVDFDQJIqZghIUsUMAUmqmCEgSRUzBCSpYoaAJFXMEJCkijX6esmIWAV8CjgV6ALvAL4H3AycDOwFIjMPRcQKYBtwHvAYcGlm3ttk+ZKkZppuCWwDvpyZvw28HLgPuBK4PTPXA7eXeYBzgfXlZytwXcNlS5IaGjoEIuL5wGuAHQCZ+XhmPgJsBHaWbjuBC8v0RmBXZnYz8w5gVUScNOzyJUnNNRkOOgX4MfBPEfFy4B7gvcDqzDxQ+jwErC7Ta4AH++6/r7Qd6GsjIrbS21IgM+l0OgMXNDU1taj+c2YWfY/RGKbWpTTs+huHNtcG1tdEm2uDdtc3itqahMAU8Erg3Zl5Z0Rs48mhHwAysxsR3cU8aGZuB7aX2e7s7OzA9+10Oiym/6TNvOnVA/Vbef3uJa6kp83rr821gfU10ebaoN31LVTb9PT0wI/RZJ/APmBfZt5Z5j9PLxRm5oZ5yu+D5fb9wLq++68tbZKkCRk6BDLzIeDBiHhpadoAfBfYDWwqbZuAW8r0buCSiFgREWcBj/YNG0mSJqDRIaLAu4EbI+JZwP3AZnrBkhGxBXgAiNL3VnqHh+6hd4jo5obLliQ11CgEMvNbwOnz3LRhnr5d4PImy5MkjZZnDEtSxQwBSaqYISBJFTMEJKlihoAkVcwQkKSKGQKSVDFDQJIqZghIUsUMAUmqmCEgSRVregE5jcGRyy4YqN+4vndA0vLhloAkVcwQkKSKGQKSVDFDQJIqZghIUsUMAUmqmCEgSRUzBCSpYoaAJFXMEJCkihkCklQxQ0CSKmYISFLFDAFJqpghIEkVMwQkqWKGgCRVrPE3i0XESuBuYH9mnh8RpwA3AScC9wAXZ+bjEXECsAt4FfAT4K2Zubfp8iVJwxvFlsB7gfv65j8GXJuZLwYOAVtK+xbgUGm/tvSTJE1QoxCIiLXAHwKfKvMrgNcCny9ddgIXlumNZZ5y+4bSX5I0IU2Hg/4O+ADwvDJ/IvBIZh4u8/uANWV6DfAgQGYejohHS//Z/geMiK3A1tKPTqczcDFTU1OL6j9nZtH3aKdh/vZ+w66/cWhzbWB9TbS5Nmh3faOobegQiIjzgYOZeU9EnNOoij6ZuR3YXma7s7OzT9X9KJ1Oh8X0X26a/u1tXn9trg2sr4k21wbtrm+h2qanpwd+jCbDQWcDF0TEXno7gl8LbANWRcRcuKwF9pfp/cA6gHL78+ntIJYkTcjQIZCZV2Xm2sw8GbgI+Fpmvh34OvDm0m0TcEuZ3l3mKbd/LTO7wy5fktTcUpwn8EHgiojYQ2/Mf0dp3wGcWNqvAK5cgmVLkhah8XkCAJn5DeAbZfp+4Ix5+vwceMsolidJGg3PGJakihkCklQxQ0CSKmYISFLFDAFJqpghIEkVMwQkqWIjOU9A7XDksgsG7rvy+t1LWImkpwu3BCSpYoaAJFXMEJCkihkCklQxQ0CSKmYISFLFDAFJqpghIEkVMwQkqWKGgCRVzBCQpIoZApJUMUNAkipmCEhSxQwBSaqY3ydQqfm+e2Bmnn5+74C0vLklIEkVMwQkqWKGgCRVzBCQpIoZApJUsaGPDoqIdcAuYDXQBbZn5raIeCFwM3AysBeIzDwUESuAbcB5wGPApZl5b7PyJUlNNNkSOAy8PzNfBpwFXB4RLwOuBG7PzPXA7WUe4FxgffnZClzXYNmSpBEYOgQy88DcJ/nM/BlwH7AG2AjsLN12AheW6Y3ArszsZuYdwKqIOGnY5UuSmhvJyWIRcTLwCuBOYHVmHig3PURvuAh6AfFg3932lbYDfW1ExFZ6WwpkJp1OZ+A6pqamFtV/znwnSalnmPW5FIZ9bsfF+obX5tqg3fWNorbGIRARzwW+ALwvM38aEU/clpndiOgu5vEyczuwvcx2Z2dnB75vp9NhMf11fG1Zn21/bq1veG2uDdpd30K1TU9PD/wYjY4Oiohn0guAGzPzi6V5Zm6Yp/w+WNr3A+v67r62tEmSJqTJ0UErgB3AfZn5yb6bdgObgGvK71v62t8VETcBZwKP9g0bSZImoMlw0NnAxcC3I+Jbpe1D9N78MyK2AA8Ac+NDt9I7PHQPvUNENzdYtiRpBIYOgcz8N2DFAjdvmKd/F7h82OUNY74rZUqSnuQZw5JUMUNAkipmCEhSxQwBSaqYXy+ppzToznW/hlJ6enJLQJIqZghIUsUMAUmqmCEgSRUzBCSpYoaAJFXMEJCkinmegEbC8wmkpye3BCSpYoaAJFXMEJCkirlPQGPlvgOpXdwSkKSKGQKSVDFDQJIqZghIUsUMAUmqmCEgSRXzEFG10rGHks4s0M9DSaVm3BKQpIoZApJUMYeD9LTmGchSM24JSFLFDAFJqpjDQarCoMNGi+EQk5aDsYdARLwR2AasBD6VmdeMuwZpFPqDZaFDWGHwsHD/hiZhrCEQESuBfwBeD+wD7oqI3Zn53XHWIdVg0JBaDANo+Rn3lsAZwJ7MvB8gIm4CNgKGgJatUQ9FLcXQ1qiXvZitn1EF1GIYZk8adwisAR7sm98HnNnfISK2AlsBMpPp6elFLeCo/v9895BlShqLp8n/6GLfh8apaW2tOzooM7dn5umZeTqwYjE/EXHPYu8zzh/rW561Wd/yra3t9R2ntoGMOwT2A+v65teWNknSBIx7OOguYH1EnELvzf8i4E/GXIMkqRjrlkBmHgbeBXwFuK/XlN8Z4SK2j/CxloL1Da/NtYH1NdHm2qDd9TWubUW32x1FIZKkp6HW7RiWJI2PISBJFVs21w5q0+UoImIdsAtYDXSB7Zm5LSI+AlwG/Lh0/VBm3jqhGvcCPwOOAIcz8/SIeCFwM3AysBeIzDw0gdpeWuqY8yLgL4FVTGj9RcQNwPnAwcw8tbTNu74iYgW91+J5wGPApZl575hr+1vgj4DHge8DmzPzkYg4md7+uO+Vu9+Rme9cqtqeor6PsMBzGRFXAVvovTbfk5lfGXNtNwMvLV1WAY9k5mkTWncLvZeM7LW3LEKghZejOAy8PzPvjYjnAfdExG3ltmsz8+MTqutYv5+Zs33zVwK3Z+Y1EXFlmf/guIvKzO8Bp8ETz+1+4EvAZia3/j4N/D29f8g5C62vc4H15edM4DqOOSlyDLXdBlyVmYcj4mPAVTz5XH4/M09bwnoGqQ/meS4j4mX0jhr8HWAa+JeIeElmHhlXbZn51r56PgE82td/3OtuofeSSxnRa2+5DAc9cTmKzHwcmLscxURk5oG59M3Mn9H79LBmUvUswkZgZ5neCVw4uVKesIHeP94DkywiM78JPHxM80LrayOwKzO7mXkHsCoiThpnbZn51XI0HsAd9M7JmYgF1t1CNgI3Zeb/ZeYPgD30/r/HXlv5VB3AZ5dq+cfzFO8lI3vtLYstAQa4HMWklE3IVwB3AmcD74qIS4C76SX82Idbii7w1YjoAv+YmduB1Zl5oNz+EL1N0Em7iKP/Cduy/mDh9TXf63ENcIDJeAdHD6+dEhH/CfwU+HBm/utkypr3uVxDL7TmzK27Sfg9YCYz/6evbWLr7pj3kpG99pbLlkArRcRzgS8A78vMn9LbNPstekMdB4BPTK46fjczX0lv8/HyiHhN/42Z2aUXFBMTEc8CLgA+V5ratP6O0ob1NZ+I+At6Qwo3lqYDwG9m5iuAK4DPRMSvT6C01j6Xfd7G0R9AJrbu5nkveULT195yCYHWXY4iIp5J70m7MTO/CJCZM5l5JDN/CVzPEm7mHk9m7i+/D9Ibbz8DmJnbdCy/D06qvuJc4N7MnIF2rb9iofXVitdjRFxKb6fn28sbBWWY5Sdl+h56O41fMu7anuK5bMu6mwL+mL4tqEmtu/neSxjha2+5hMATl6Monx4vAiZ2rdgylrgDuC8zP9nX3j829ybgv8ZdW6njOWUnExHxHOANpZbdwKbSbRNwyyTq63PUJ7G2rL8+C62v3cAlEbEiIs4CHu3bdB+LcrTcB4ALMvOxvvbfKDvbiYgX0duBeP84ayvLXui53A1cFBEnlMvLrAf+Y9z1Aa8D/jsz9801TGLdLfRewghfe8tin0A5AmLuchQrgRtGfDmKxTobuBj4dkR8q7R9CHhbRJxGb9NtL/BnkyiO3vjhlyICeq+Bz2TmlyPiLiAjYgvwAL2dYhNRwun1HL2O/mZS6y8iPgucA3QiYh9wNXAN86+vW+kdoreH3mF6mydQ21XACcBt5XmeO5zxNcBHI+IXwC+Bd2bmoDttR1nfOfM9l5n5nYhIet8xchi4fAmPDJq3tszcwa/ui4IJrDsWfi8Z2WvPy0ZIUsWWy3CQJGkIhoAkVcwQkKSKGQKSVDFDQJIqZghIUsUMAUmq2P8Dmz9Jg0qdkekAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_tokens = labeled_documents.text.apply(lambda x: len(x.split(\" \")))\n",
    "plt.hist(num_tokens, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a00fc68e98cfdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:44.392364Z",
     "start_time": "2024-05-13T21:39:42.689859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 17131\n"
     ]
    }
   ],
   "source": [
    "_set = set()\n",
    "for __set in labeled_documents.text.apply(lambda x: set(x.split(\" \"))).values:\n",
    "    _set = _set.union(__set)\n",
    "print(f\"Number of unique tokens: {len(_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bd53d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:45.043376Z",
     "start_time": "2024-05-13T21:39:44.398363Z"
    }
   },
   "outputs": [],
   "source": [
    "# load train/test hashes\n",
    "#hash_docs_link = pd.read_json(\"./echo_span_labels/reduced_labels/merged_labels.jsonl\", lines=True)[['text', '_input_hash']]\n",
    "train_ids = pd.read_csv('./train_echoid.csv', sep=',').input_hash.unique()\n",
    "test_ids = pd.read_csv('./test_echoid.csv', sep=',').input_hash.unique()\n",
    "\n",
    "labeled_documents['_hash'] = labeled_documents.text.str.strip().apply(lambda x: hash(x))\n",
    "#hash_docs_link['_hash'] = hash_docs_link.text.str.strip().apply(lambda x: hash(x))\n",
    "\n",
    "#labeled_documents = labeled_documents.merge(hash_docs_link[['_input_hash', '_hash']], \n",
    "#                                            on='_hash', how='inner')\n",
    "\n",
    "labeled_documents = labeled_documents.drop_duplicates(subset=['_hash']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6632439635dd361f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:45.492878Z",
     "start_time": "2024-05-13T21:39:45.045890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Expand with label columns\n",
    "if Class == 'merged_labels':\n",
    "    Target_maps = {\n",
    "        _Class: {Label:i for i,Label in enumerate(target_df[Class].unique())}\n",
    "        for _Class in target_df.columns\n",
    "    }\n",
    "else:\n",
    "    Target_maps = {\n",
    "        Class: {Label: i for i,Label in enumerate(labeled_documents['label'].unique())} \n",
    "    }\n",
    "    \n",
    "if Class == 'merged_labels':\n",
    "    DF = labeled_documents[['text', '_input_hash']].join(target_df[Class])\n",
    "else:\n",
    "    DF = labeled_documents[['text', '_input_hash', 'label']]\n",
    "\n",
    "DF.columns = ['sentence', '_input_hash', 'labels']\n",
    "\n",
    "label2id = Target_maps[Class]\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "if use_multilabel:\n",
    "    _multilabels = {label2id.get(k): [label2id.get(l) for l in v if label2id.get(l) is not None]\n",
    "                    for k,v in MULTILABELS.items() if label2id.get(k) is not None}\n",
    "else:\n",
    "    _multilabels = None\n",
    "\n",
    "DF= DF.assign(label=DF['labels'].map(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fa41c34d45cc27e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:46.004427Z",
     "start_time": "2024-05-13T21:39:45.496380Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_multilabel:\n",
    "    if all([len(v)==1 for v in _multilabels.values()]):\n",
    "        use_multilabel = False\n",
    "        _multilabels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134425514247d5e6",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cd611ac6ba3027e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:39:46.469391Z",
     "start_time": "2024-05-13T21:39:46.006883Z"
    }
   },
   "outputs": [],
   "source": [
    "DF = DF.assign(sentence=DF.sentence.str.replace(r'[\\r\\n]', '', regex=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef0e8d7af868e41f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:40:00.342061Z",
     "start_time": "2024-05-13T21:39:46.471895Z"
    }
   },
   "outputs": [],
   "source": [
    "if lemmatize:\n",
    "    docs = nlp.pipe(DF.sentence.values)\n",
    "    new_texts = [\" \".join([token.lemma_ for token in doc]) for doc in docs] \n",
    "    DF = DF.assign(sentence = new_texts)\n",
    "\n",
    "if lowercase:\n",
    "    DF = DF.assign(sentence = DF.sentence.str.lower())\n",
    "    \n",
    "if filter_reports:\n",
    "    DF = DF.assign(sentence = echo_utils.report_filter(DF.sentence, \n",
    "                                            flag_terms=FLAG_TERMS, \n",
    "                                            save_terms=SAVE_TERMS)[0])\n",
    "    DF = DF.loc[DF.sentence.notna()]\n",
    "\n",
    "if deabbreviate:\n",
    "    DeAbber = deabber.deabber(model_type='sbert', abbreviations=ABBREVIATIONS['nl']['echocardiogram'], min_sim=0.5, top_k=10)\n",
    "    DF = DF.assign(sentence=DeAbber.deabb(DF.sentence.values, TokenRadius=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c68f12e14c3d84",
   "metadata": {},
   "source": [
    "# Make folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83203825517c00c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:40:00.931069Z",
     "start_time": "2024-05-13T21:40:00.344065Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepping\n",
    "TrainTestDict = defaultdict(dict)\n",
    "\n",
    "if Splitting == 'from_file':\n",
    "    TrainTestDict[0]['Xtrain'] = DF.loc[DF._input_hash.isin(train_ids)].sentence\n",
    "    TrainTestDict[0]['Xtest'] = DF.loc[DF._input_hash.isin(test_ids)].sentence\n",
    "    \n",
    "    TrainTestDict[0]['ytrain'] = DF.loc[DF._input_hash.isin(train_ids)].label\n",
    "    TrainTestDict[0]['ytest'] = DF.loc[DF._input_hash.isin(test_ids)].label\n",
    "    \n",
    "elif Splitting == 'CV':\n",
    "    for k,(train_index, test_index) in enumerate(fold_indices(targets=DF['label'], stratified=True)):\n",
    "        TrainTestDict[k]['Xtrain'] = DF.iloc[train_index].sentence\n",
    "        TrainTestDict[k]['Xtest'] = DF.iloc[test_index].sentence\n",
    "        \n",
    "        TrainTestDict[k]['ytrain'] = DF.iloc[train_index].label\n",
    "        TrainTestDict[k]['ytest'] = DF.iloc[test_index].label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4b47e71c1ca13",
   "metadata": {},
   "source": [
    "## Initiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a28bb238849448ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:40:01.535578Z",
     "start_time": "2024-05-13T21:40:00.933075Z"
    }
   },
   "outputs": [],
   "source": [
    "if pre_trained_embeddings:\n",
    "    TexToVec = echo_utils.TextToVectors(maxlen=max_len, \n",
    "                                        source=EmbeddingSource, \n",
    "                                        embedding_path=EmbeddingPath,\n",
    "                                        use_progress_bar=False)\n",
    "    pre_emb_dim = TexToVec.emb_dim\n",
    "else:\n",
    "    pre_emb_dim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "664871e9df2dde6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:40:03.008094Z",
     "start_time": "2024-05-13T21:40:01.538077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 200, 100)\n",
      "(None, 200, 300)\n",
      "(None, 200, 400)\n",
      "(None, 200, 64)\n"
     ]
    }
   ],
   "source": [
    "ModelDict = defaultdict(dict)\n",
    "for k,v in TrainTestDict.items():\n",
    "    LSTM_Tokenizer = KerasTokenizer(num_words=num_words_in_vocab)\n",
    "    if pre_trained_embeddings:\n",
    "        vocab_size = num_words_in_vocab\n",
    "    else:        \n",
    "        LSTM_Tokenizer.fit_on_texts(v['Xtrain'])\n",
    "        vocab_size = len(LSTM_Tokenizer.word_index)+1\n",
    "\n",
    "    num_classes = np.unique(v['ytrain']).shape[0]\n",
    "    \n",
    "    ModelDict[k]['tokenizer'] = LSTM_Tokenizer    \n",
    "    if use_multilabel:\n",
    "        ModelDict[k]['warmup_model'] = echo_models.CurrentModel(modelselection=ModelType, \n",
    "                                     embeddingdim=embedding_dim,\n",
    "                                     pre_embeddingdim = pre_emb_dim,\n",
    "                                     maxlen=max_len,\n",
    "                                     vocabsize=vocab_size,\n",
    "                                     numclasses=num_classes,\n",
    "                                     learningrate=LR,\n",
    "                                     dilation=dilation,\n",
    "                                     num_layers=num_layers,\n",
    "                                     multilabel=False,\n",
    "                                     pre_trained_vectors=pre_trained_embeddings,\n",
    "                                     dropout=dropout\n",
    "                                     )\n",
    "    else:\n",
    "        ModelDict[k]['model'] = echo_models.CurrentModel(modelselection=ModelType, \n",
    "                                     embeddingdim=embedding_dim,                                     \n",
    "                                     pre_embeddingdim = pre_emb_dim,\n",
    "                                     maxlen=max_len,\n",
    "                                     vocabsize=vocab_size,\n",
    "                                     numclasses=num_classes,\n",
    "                                     learningrate=LR,\n",
    "                                     dilation=dilation,\n",
    "                                     num_layers=num_layers,\n",
    "                                     multilabel=use_multilabel,\n",
    "                                     pre_trained_vectors=pre_trained_embeddings,\n",
    "                                     dropout=dropout\n",
    "                                    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bc82e1f567223",
   "metadata": {},
   "source": [
    "## Warmup models\n",
    "\n",
    "If we are running a multilabel classification with BCE loss we might get stuck in a local minimum where recall is 1 for all\n",
    "classes. To avoid this we pre-run the model with categorical cross-entropy loss, with a softmax in the final layer. To be able to apply\n",
    "the CCEloss we want 1 label per sample, we force by taking the minority class in each multilabel instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "415ea56d0aed8b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:40:03.643602Z",
     "start_time": "2024-05-13T21:40:03.015595Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_multilabel(Model: Sequential, learningrate: float=1e-4) -> Sequential:\n",
    "    \"\"\"\n",
    "    Make a multilabel classification model, given a pre-trained Sequential model.\n",
    "    It replaces the last Dense layer with 'softmax' activation to a 'sigmoid' activation,\n",
    "    making it suitable for multilabel classification.\n",
    "\n",
    "    :param Model: A pre-trained Keras Sequential model.\n",
    "    :return: A modified Sequential model ready for multilabel classification.\n",
    "    \"\"\"\n",
    "    assert isinstance(Model, Sequential), \"Model must be a Keras Sequential model.\"\n",
    "\n",
    "    # Create a new Sequential model\n",
    "    new_model = Sequential()\n",
    "\n",
    "    # Copy all layers except the last one from the original model to the new model\n",
    "    for layer in Model.layers[:-1]:\n",
    "        new_model.add(layer)\n",
    "\n",
    "    # Assume the last layer is a Dense layer and get the number of units from it\n",
    "    num_classes = Model.layers[-1].output_shape[-1]\n",
    "\n",
    "    # Add a new Dense layer with sigmoid activation suited for multilabel classification\n",
    "    new_model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    # Important: this line copies the weights of all layers except the last one from the original model\n",
    "    for layer, new_layer in zip(Model.layers[:-1], new_model.layers[:-1]):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "    new_model.compile(optimizer = optimizers.Adam(learning_rate=learningrate),\n",
    "                      loss = 'binary_crossentropy',\n",
    "                      metrics = ['binary_accuracy'])\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "# def make_multilabel(model: Sequential):\n",
    "#     '''\n",
    "#     Make a multilabel classification model, given a pre-trained Sequential model.\n",
    "#     Actions:\n",
    "#      1. Replace layers.Dense(numclasses, activation = 'softmax') with\n",
    "#         layers.Dense(numclasses, activation = 'sigmoid')\n",
    "#      2. Replace the loss function 'categorical_crossentropy' with 'binary_crossentropy'.\n",
    "# \n",
    "#     :param model: A pre-trained Keras Sequential model.\n",
    "#     :return: Sequential model with pretrained weights, adapted for multi-label classification.\n",
    "#     '''\n",
    "# \n",
    "#     assert isinstance(model, Sequential), \"Multilabel model must be Sequential\"\n",
    "# \n",
    "#     # Modify the output layer\n",
    "#     for layer in model.layers:\n",
    "#         if isinstance(layer, Dense) and layer.activation == tf.keras.activations.softmax:\n",
    "#             num_classes = layer.units  # Preserve number of classes\n",
    "#             model.pop()  # Remove old output layer\n",
    "#             model.add(Dense(num_classes, activation='sigmoid'))\n",
    "#             break  # Assuming only one softmax output layer\n",
    "# \n",
    "#     # Replace loss function (assuming 'categorical_crossentropy' was used originally)\n",
    "#     model.compile(optimizer=model.optimizer,  # Preserve the optimizer\n",
    "#                   loss='binary_crossentropy', \n",
    "#                   metrics=['accuracy'])  # Or other suitable metrics\n",
    "# \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9549dae54c0f97f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:40:04.232609Z",
     "start_time": "2024-05-13T21:40:03.646103Z"
    }
   },
   "outputs": [],
   "source": [
    "WarmupDict = defaultdict(lambda: defaultdict(list))\n",
    "if use_multilabel:   \n",
    "    # run with softmax and CCEloss\n",
    "    for k,v in ModelDict.items():\n",
    "        _model =  v['warmup_model']\n",
    "        _tokenizer = v['tokenizer']\n",
    "        \n",
    "        if pre_trained_embeddings:\n",
    "            X_train = TexToVec.text_to_vectors(TrainTestDict[k]['Xtrain']) \n",
    "            X_test = TexToVec.text_to_vectors(TrainTestDict[k]['Xtest']) \n",
    "        else:\n",
    "            X_train = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtrain'])\n",
    "            X_test = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtest'])\n",
    "        \n",
    "            X_train = pad_sequences(X_train, padding = 'post', maxlen=max_len)\n",
    "            X_test = pad_sequences(X_test, padding = 'post', maxlen=max_len)\n",
    "    \n",
    "        _y = TrainTestDict[k]['ytrain']\n",
    "        y_train = multi_hot_encoding(_y, multilabels=None, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "    \n",
    "        _y = TrainTestDict[k]['ytest']\n",
    "        y_test = multi_hot_encoding(_y, multilabels=None, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "    \n",
    "        if UseClassWeights:\n",
    "            PosSum = 1/np.sum(y_train, axis=0)\n",
    "            ClassWeights = dict(enumerate(PosSum/np.min(PosSum)))\n",
    "        else:\n",
    "            ClassWeights = None\n",
    "        \n",
    "        result_list = []\n",
    "        history_list = []\n",
    "        confusion_list = []\n",
    "        for epoch_num in range(1, warmup_steps+1):\n",
    "            _history = _model.fit(X_train,\n",
    "                                 y_train,                         \n",
    "                                 epochs = 1,\n",
    "                                 verbose = False,\n",
    "                                 validation_split = 0,\n",
    "                                 class_weight=ClassWeights,\n",
    "                                 batch_size = batch_size)\n",
    "        \n",
    "            y_pred = _model.predict(X_test, verbose=False)\n",
    "            _res = multi_label_metrics(labels=y_test, probs=y_pred)\n",
    "            \n",
    "            result_list.append(_res)\n",
    "            history_list.append({'loss': _history.history['loss'][0], \n",
    "                                 'accuracy': _history.history['binary_accuracy'][0]})\n",
    "            confusion_list.append(confusion_matrix(np.argmax(y_test, axis=1),\n",
    "                                                   np.argmax(np.round(y_pred),axis=1),  normalize='true'))\n",
    "            \n",
    "            \n",
    "            print(f\"Epoch {epoch_num}, F1 Macro={_res['f1_macro']}\")\n",
    "            \n",
    "        WarmupDict[k]['history'] = pd.DataFrame(history_list)\n",
    "        WarmupDict[k]['result'] = pd.DataFrame(result_list)\n",
    "        WarmupDict[k]['confusion'] = confusion_list\n",
    "        \n",
    "        ModelDict[k]['model'] = make_multilabel(_model, learningrate=warmup_lr)\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e98939a8a987ded8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:40:04.837118Z",
     "start_time": "2024-05-13T21:40:04.238111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Normal', 1: 'Mild', 2: 'No label', 3: 'Moderate', 4: 'Severe'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a30b4c2d9bfaf3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc9ef4a32da8a35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:40:05.457126Z",
     "start_time": "2024-05-13T21:40:04.841118Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_multilabel:\n",
    "    _confusion_matrix = multilabel_confusion_matrix\n",
    "else:\n",
    "    _confusion_matrix = confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a15851917a011494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:25:55.617084Z",
     "start_time": "2024-05-13T21:40:05.462626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, F1 Macro=0.14664057403783431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, F1 Macro=0.20432773109243701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, F1 Macro=0.34753946561884586\n"
     ]
    }
   ],
   "source": [
    "ResultDict = defaultdict(lambda: defaultdict(list))\n",
    "for k,v in ModelDict.items():\n",
    "    _model =  v['model']\n",
    "    _tokenizer = v['tokenizer']\n",
    "    \n",
    "    if pre_trained_embeddings:\n",
    "        X_train_pre = TexToVec.text_to_vectors(TrainTestDict[k]['Xtrain']) \n",
    "        X_test_pre = TexToVec.text_to_vectors(TrainTestDict[k]['Xtest']) \n",
    "        \n",
    "        X_train = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtrain'])\n",
    "        X_test = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtest'])\n",
    "    \n",
    "        X_train = pad_sequences(X_train, padding = 'post', maxlen=max_len)\n",
    "        X_test = pad_sequences(X_test, padding = 'post', maxlen=max_len)\n",
    "        \n",
    "        X_train = [X_train, X_train_pre]\n",
    "        X_test = [X_test, X_test_pre]\n",
    "    else:\n",
    "        X_train = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtrain'])\n",
    "        X_test = _tokenizer.texts_to_sequences(TrainTestDict[k]['Xtest'])\n",
    "    \n",
    "        X_train = pad_sequences(X_train, padding = 'post', maxlen=max_len)\n",
    "        X_test = pad_sequences(X_test, padding = 'post', maxlen=max_len)\n",
    "\n",
    "    _y = TrainTestDict[k]['ytrain']\n",
    "    y_train = multi_hot_encoding(_y, multilabels=_multilabels, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "\n",
    "    _y = TrainTestDict[k]['ytest']\n",
    "    y_test = multi_hot_encoding(_y, multilabels=_multilabels, num_classes=num_labels) # keras_utils.to_categorical(_y)\n",
    "\n",
    "    if UseClassWeights:\n",
    "        PosSum = 1/np.sum(y_train, axis=0)\n",
    "        ClassWeights = dict(enumerate(PosSum/np.min(PosSum)))\n",
    "    else:\n",
    "        ClassWeights = None\n",
    "    \n",
    "    result_list = []\n",
    "    history_list = []\n",
    "    confusion_list = []\n",
    "    for epoch_num in range(1, num_epochs+1):\n",
    "        _history = _model.fit(X_train,\n",
    "                             y_train,                         \n",
    "                             epochs = 1,\n",
    "                             verbose = False,\n",
    "                             validation_split = 0,\n",
    "                             class_weight=ClassWeights,\n",
    "                             batch_size = batch_size)\n",
    "    \n",
    "        y_pred = _model.predict(X_test, verbose=False)\n",
    "        _res = multi_label_metrics(labels=y_test, probs=y_pred)\n",
    "        \n",
    "        result_list.append(_res)\n",
    "        history_list.append({'loss': _history.history['loss'][0], \n",
    "                             'accuracy': _history.history['binary_accuracy'][0]})\n",
    "        confusion_list.append(_confusion_matrix(np.argmax(y_test, axis=1),\n",
    "                                               np.argmax(np.round(y_pred),axis=1)))\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch_num}, F1 Macro={_res['f1_macro']}\")\n",
    "        \n",
    "    ResultDict[k]['history'] = pd.DataFrame(history_list)\n",
    "    ResultDict[k]['result'] = pd.DataFrame(result_list)\n",
    "    ResultDict[k]['confusion'] = confusion_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d94430d64ea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:25:56.066592Z",
     "start_time": "2024-05-13T22:25:55.619081Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caace1698b3b2400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:25:56.655595Z",
     "start_time": "2024-05-13T22:25:56.069086Z"
    }
   },
   "outputs": [],
   "source": [
    "heatmap_df = \\\n",
    "    pd.DataFrame(\n",
    "    confusion_list[-1],\n",
    "        columns=label2id.keys(),\n",
    "        index=label2id.keys()\n",
    "    )\n",
    "sns.heatmap(heatmap_df.round(4), annot=True, cmap='Blues', fmt='g')\n",
    "    # df.style.background_gradient(cmap='Blues')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19bce9e1f896ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:25:57.089585Z",
     "start_time": "2024-05-13T22:25:56.658631Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Class: {Class}, Model: {ModelType}, Splitting: {Splitting}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cad5ab1538d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:25:57.771610Z",
     "start_time": "2024-05-13T22:25:57.092094Z"
    }
   },
   "outputs": [],
   "source": [
    "MultiLabelStr = 'multilabel' if use_multilabel else 'categorical'\n",
    "PretrainStr = 'pretrainedEmbs' if pre_trained_embeddings else \"\"\n",
    "ResultDict[k]['result'].to_csv(f\"../../G_Output/2_Data/results_{MultiLabelStr}_{Class}_{ModelType}_{PretrainStr}_{Splitting}.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e627eadb8f623ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a77e2c88cef599d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb9a2f9db3a0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:25:58.329639Z",
     "start_time": "2024-05-13T22:25:57.774619Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ResultDict[k]['result']['f1_macro'], label='F1 Macro')\n",
    "plt.plot(ResultDict[k]['result']['prec_macro'], label='Precision Macro')\n",
    "plt.plot(ResultDict[k]['result']['recall_macro'], label='Recall Macro')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead311ab73fa4d25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:25:58.903134Z",
     "start_time": "2024-05-13T22:25:58.331601Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ResultDict[k]['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d70a8f816b3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:25:59.352641Z",
     "start_time": "2024-05-13T22:25:58.905104Z"
    }
   },
   "outputs": [],
   "source": [
    "ResultDict[k]['result'].sort_values(by='f1_macro', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f536753e31dcdb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:26:02.049637Z",
     "start_time": "2024-05-13T22:25:59.354609Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(_model, to_file='./model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0495325550a5c7",
   "metadata": {},
   "source": [
    "## Upload to huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd788ebc123c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:26:02.056138Z",
     "start_time": "2024-05-13T22:26:02.056138Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2452b646be1643b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T22:26:02.061138Z",
     "start_time": "2024-05-13T22:26:02.060139Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nlp_310)",
   "language": "python",
   "name": "python3_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
