{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VENVS\\Envs\\nlp_310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import spacy\n",
    "from clinlp import Term\n",
    "import json\n",
    "import benedict\n",
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_regex(regex_list: list=None)-> str:\n",
    "    try:\n",
    "        combo_list = []\n",
    "        for regex_ in regex_list:\n",
    "            # SKIP THIS FOR NOW\n",
    "            _regex = regex_['TEXT']\n",
    "            if 'NOT_IN' in _regex.keys():\n",
    "                return None\n",
    "            else:\n",
    "                combo_list.append(f\"({_regex.get('REGEX', _regex.get('LOWER', ''))})\")          \n",
    "        return \" \".join(combo_list)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def collect_regex(regexes: list=None)-> dict:\n",
    "    res = defaultdict(list)    \n",
    "    for regex_str in regexes:\n",
    "        regex_dict = eval(regex_str)\n",
    "        distilled = distill_regex(regex_dict['pattern'])\n",
    "        if distilled is not None:\n",
    "            res[regex_dict['label']].append(distilled)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this notebook\n",
    "\n",
    "Is to establish the basic scripts to collect relevant **phrases** for classification\n",
    "and relevant **context-vectors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionary with terms of interest\n",
    "TermsOfInterest = benedict.benedict().from_yaml('../assets/token_list_new.yml')\n",
    "RegexOfInterest_dict = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/regex/mi_v8.txt', 'r') as fr:\n",
    "    regex_lines = fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'mitral_valve_native_regurgitation_severe': ['(([Kk]ritische?|[Ee]rnstige?|[Bb]elangrijke)) (^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$)',\n",
       "              '(([Kk]ritische?|[Ee]rnstige?|[Bb]elangrijke)) (^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?)',\n",
       "              '(^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$) ((kritische?|ernstige?|belangrijke))',\n",
       "              '(^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$) (^[Gg]r(aad)?(\\\\.)?$) (^([Ii][Ii][Ii]-|3-)?([Ii][Vv]|4)(?=\\\\/)?)',\n",
       "              '(^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?) (^[Gg]r(aad)?(\\\\.)?$) (^([Ii][Ii][Ii]-|3-)?([Ii][Vv]|4)(?=\\\\/)?)',\n",
       "              '(^[Gg]r(aad)?(\\\\.)?$) (^([Ii][Ii][Ii]-|3-)?([Ii][Vv]|4)(?=\\\\/)?) (^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$)',\n",
       "              '(^[Gg]r(aad)?(\\\\.)?$) (^([Ii][Ii][Ii]-|3-)?([Ii][Vv]|4)(?=\\\\/)?) (^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?)'],\n",
       "             'mitral_valve_native_regurgitation_moderate': ['([Mm]atige?) (^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$)',\n",
       "              '([Mm]atige?) (^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?)',\n",
       "              '(^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$) (matige?)',\n",
       "              '(^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$) (^[Gg]r(aad)?(\\\\.)?$) (^([Ii][Ii]-|2-)?([Ii][Ii][Ii]|3)(?=\\\\/)?)',\n",
       "              '(^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?) (^[Gg]r(aad)?(\\\\.)?$) (^([Ii][Ii]-|2-)?([Ii][Ii][Ii]|3)(?=\\\\/)?)',\n",
       "              '(^[Gg]r(aad)?(\\\\.)?$) (^([Ii][Ii]-|2-)?([Ii][Ii][Ii]|3)(?=\\\\/)?) (^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$)',\n",
       "              '(^[Gg]r(aad)?(\\\\.)?$) (^([Ii][Ii]-|2-)?([Ii][Ii][Ii]|3)(?=\\\\/)?) (^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?)'],\n",
       "             'mitral_valve_native_regurgitation_mild': ['(([Mm]ilde?|[Gg]eringe?|[Ll]ichte?)) (^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$)',\n",
       "              '(([Mm]ilde?|[Gg]eringe?|[Ll]ichte?)) (^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?)',\n",
       "              '(^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$) ((milde?|geringe?|lichte?))',\n",
       "              '(^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$) (^[Gg]r(aad)?(\\\\.)?$) (^([Ii]-|1-)?([Ii][Ii]|2)(\\\\.)?(?=\\\\/)?)',\n",
       "              '(^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?) (^[Gg]r(aad)?(\\\\.)?$) (^([Ii]-|1-)?([Ii][Ii]|2)(\\\\.)?(?=\\\\/)?)',\n",
       "              '(^[Gg]r(aad)?(\\\\.)?$) (^([Ii]-|1-)?([Ii][Ii]|2)(\\\\.)?(?=\\\\/)?) (^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$)',\n",
       "              '(^[Gg]r(aad)?(\\\\.)?$) (^([Ii]-|1-)?([Ii][Ii]|2)(\\\\.)?(?=\\\\/)?) (^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?)'],\n",
       "             'mitral_valve_native_regurgitation_trace': ['([Ss]poor(tje)?) (^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$)',\n",
       "              '([Ss]poor(tje)?) (^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?)',\n",
       "              '(^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$) (spoor(tje)?)',\n",
       "              '(^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$) (^[Gg]r(aad)?(\\\\.)?$) (^([Ii]{1}(?![iIvV])|1))',\n",
       "              '(^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?) (^[Gg]r(aad)?(\\\\.)?$) (^([Ii]{1}(?![iIvV])|1))',\n",
       "              '(^[Gg]r(aad)?(\\\\.)?$) (^([Ii]{1}(?![iIvV])|1)) (^[Mm](itr|itralis)?(klep|\\\\.|-)?[Ii](nsuff|nsuffici[eÃ«]ntie)?(\\\\W)?$)',\n",
       "              '(^[Gg]r(aad)?(\\\\.)?$) (^([Ii]{1}(?![iIvV])|1)) (^[Mm](itralis)?(klep|[Vv])?(\\\\W)?$) ([Ii]nsuff(ici[Ã«e]ntie)?)'],\n",
       "             'mitral_valve_native_regurgitation_not_present': ['(([Nn]ormale|[Gg]oede)) ((?<![Kk]unst)klepfuncties?)',\n",
       "              '([Kk]lepfuncties?) ((normaal|goed))']})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_regex(regex_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueTargetTerms = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We would like to expand this!\n",
    "\n",
    "Given that ```clinlp``` is able to handle syntactically similar terms we only want to expand this with \"semantically\" similar terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an echo report corpus\n",
    "\n",
    "echoReports = pd.read_parquet('../../../../data/Digin/echo.parquet') \n",
    "\n",
    "echoReports = echoReports.assign(TEXT=\n",
    "                                 echoReports.Brief_txt.str.decode('latin-1') + \" \" + echoReports.Conclusions_ECHO.str.decode('latin-1')\n",
    "                                 )\n",
    "\n",
    "echo_path = 'T://lab_research/RES-Folder-UPOD/Echo_label/E_ResearchData/2_ResearchData'\n",
    "labeled_texts = pd.read_json(os.path.join(echo_path, 'outdb_140423.jsonl'), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clinical NLP embeddings, hear we are limited to fourgrams\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
    "StaticEmbedding = KeyedVectors.load('../../../../language_modeling/Embeddings/CARDIO/without_tokenizer/fasttext/cardio_cbow.wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative is the use of a sentence transformer model \n",
    "from sentence_transformers import SentenceTransformer\n",
    "# jegormeister/robbert-v2-dutch-base-mqa-finetuned, textgain/allnli-GroNLP-bert-base-dutch-cased\n",
    "sent_model = SentenceTransformer('NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers')\n",
    "\n",
    "# or a clinical BERT model such as MedRoBERTa.nl\n",
    "from scipy.spatial.distance import cosine as cosine_similarity\n",
    "def phrase_similarity(phrase1, phrase2, model):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two phrases\n",
    "    \"\"\"\n",
    "    phrase1 = model.encode(phrase1)\n",
    "    phrase2 = model.encode(phrase2)\n",
    "    return 1 - cosine_similarity(phrase1, phrase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to inspect a nested dictionary with lists, we can do this with a recursive function\n",
    "def find_new_terms(query_list, min_sim=0.95, topn=50, CHECKLIST=set([])):\n",
    "    new_terms = []\n",
    "    for query in query_list:\n",
    "        new_terms.extend([query])\n",
    "        new_terms.extend(set([t.replace(\"BREAK\", \" \")\\\n",
    "                                           .replace(\"_\", \" \")\\\n",
    "                                           .strip(\".\")\\\n",
    "                                           .strip(\",\")\\\n",
    "                                           .strip(\":\")\\\n",
    "                                           .lower() for t,s in StaticEmbedding.most_similar(query, topn=topn) \n",
    "                                           if (s>min_sim) & (t not in CHECKLIST)])\n",
    "        )\n",
    "    return list(set(new_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import MutableMapping\n",
    "\n",
    "def flatten(dictionary, parent_key='', separator='|'):\n",
    "    items = []\n",
    "    for key, value in dictionary.items():\n",
    "        new_key = parent_key + separator + key if parent_key else key\n",
    "        if isinstance(value, MutableMapping):\n",
    "            items.extend(flatten(value, new_key, separator=separator).items())\n",
    "        else:\n",
    "            items.append((new_key, value))\n",
    "    return dict(items)\n",
    "\n",
    "ToI_flat = flatten(ToI)\n",
    "ToI_flat = {tuple(k.split(\"|\")):v for k,v in ToI_flat.items()}\n",
    "\n",
    "L = [l for l in ToI_flat.values() if isinstance(l, list)]\n",
    "ALL_TERMS = set([_l for l in L for _l in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [01:24<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "ToI_flat_expanded = defaultdict(list)\n",
    "empty_keys = []\n",
    "for k,v in tqdm(ToI_flat.items()):\n",
    "    if isinstance(v, list):\n",
    "        new_terms = find_new_terms(v, min_sim=0.95, topn=50, CHECKLIST=ALL_TERMS)\n",
    "    else:\n",
    "        empty_keys.append(k)\n",
    "        continue\n",
    "    ToI_flat_expanded[k] = new_terms\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand back to nested dictionary\n",
    "ToI_expanded = defaultdict(dict)\n",
    "\n",
    "key_level_dict = defaultdict(set)\n",
    "for k,v in ToI_flat_expanded.items():\n",
    "    for level, _k in enumerate(k):\n",
    "        key_level_dict[level].add(_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_keys_to_nested_dict(d):\n",
    "    \"\"\"\n",
    "    Converts a dictionary with tuple keys to a nested dictionary.\n",
    "\n",
    "    :param d: dict, the original dictionary with tuple keys.\n",
    "    :return: dict, the resulting nested dictionary.\n",
    "    \"\"\"\n",
    "    nested_dict = {}\n",
    "    for keys, value in d.items():\n",
    "        temp_d = nested_dict  # Start from the top-level dictionary\n",
    "        for key in keys[:-1]:  # Until the second last key, create/get nested dicts\n",
    "            temp_d = temp_d.setdefault(key, {})\n",
    "        temp_d[keys[-1]] = value  # Set the value for the deepest key\n",
    "    return nested_dict\n",
    "\n",
    "ToI_expanded = tuple_keys_to_nested_dict(ToI_flat_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand with de-abbreviated variants\n",
    "import deabber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to find the negatives!\n",
    "\n",
    "What do we mean?? **Easy**, let's take\n",
    "\n",
    "```normale echo```, then a negative would be ```geen normale echo``` .\n",
    "\n",
    "So, how do we find this in bulk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to construct the ```clinlp``` object!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make concepts dictionary with the terms of interest\n",
    "clinlp_concepts = defaultdict(list)\n",
    "for k, v in ToI_expanded.items():\n",
    "    for term in v:\n",
    "        clinlp_concepts[term].append(Term(k, \n",
    "                                   proximity=1,\n",
    "                                   fuzzy=3,\n",
    "                                   fuzzy_min_len=7))\n",
    "# pseudo=True means it is a negative example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to make our classifier using Sklearn base classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nlp_310)",
   "language": "python",
   "name": "python3_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
